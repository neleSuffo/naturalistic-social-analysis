@InProceedings{NN:SimonyanTwoStream,
  title = {Two-Stream Convolutional Networks for Action Recognition in Videos},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Karen Simonyan and Andrew Zisserman},
  date = {2014},
  volume = {1},
  eprint = {1406.2199},
  eprinttype = {arXiv},
  pages = {568--576},
  issn = {10495258},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  issue = {January},
  file = {/Users/nelesuffo/Zotero/storage/XV856CAV/nnsimonyantwostream.pdf},
}
@Unpublished{wangTemporalSegmentNetworks2017,
  title = {Temporal {{Segment Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Limin Wang and Yuanjun Xiong and Zhe Wang and Yu Qiao and Dahua Lin and Xiaoou Tang and Luc {Van Gool}},
  date = {2017},
  eprint = {1705.02953},
  eprinttype = {arXiv},
  pages = {1--14},
  url = {http://arxiv.org/abs/1705.02953},
  abstract = {Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0\%), UCF101 (94.9\%), THUMOS14 (80.1\%), and ActivityNet v1.2 (89.6\%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0\%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.},
  file = {/Users/nelesuffo/Zotero/storage/W2L8ZPXK/wang2017temporal.pdf},
}
@InProceedings{NN:SimonyanTwoStream,
  title = {Two-Stream Convolutional Networks for Action Recognition in Videos},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Karen Simonyan and Andrew Zisserman},
  date = {2014},
  volume = {1},
  eprint = {1406.2199},
  eprinttype = {arXiv},
  pages = {568--576},
  issn = {10495258},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  issue = {January},
  file = {/Users/nelesuffo/Zotero/storage/XV856CAV/nnsimonyantwostream.pdf},
}
@Unpublished{xiongTemporalSegmentNetworks2018,
  title = {Temporal {{Segment Networks}} : {{Towards Good Practices}} for {{Deep Action Recognition}}},
  author = {Yuanjun Xiong},
  date = {2018},
  volume = {51},
  eprint = {1608.00859},
  eprinttype = {arXiv},
  pages = {1--9},
  doi = {10.1007/978-3-319-46484-8},
  abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet archi-tectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment net-work (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It com-bines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 (69.4\%) and UCF101 (94.2\%). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.},
  isbn = {9783319464848},
  keywords = {Action Recognition,ConvNets,Good Practices,Temporal Segment Networks},
  file = {/Users/nelesuffo/Zotero/storage/Q7F7MQS8/xiong2018temporal.pdf},
}
