@article{aghaeiWhomInteractDetecting2016,
  title = {With {{Whom Do I Interact}}? {{Detecting Social Interactions}} in {{Egocentric Photo-streams}}},
  shorttitle = {With {{Whom Do I Interact}}?},
  author = {Aghaei, Maedeh and Dimiccoli, Mariella and Radeva, Petia},
  date = {2016},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1605.04129},
  url = {https://arxiv.org/abs/1605.04129},
  urldate = {2024-05-16},
  abstract = {Given a user wearing a low frame rate wearable camera during a day, this work aims to automatically detect the moments when the user gets engaged into a social interaction solely by reviewing the automatically captured photos by the worn camera. The proposed method, inspired by the sociological concept of F-formation, exploits distance and orientation of the appearing individuals -with respect to the user- in the scene from a bird-view perspective. As a result, the interaction pattern over the sequence can be understood as a two-dimensional time series that corresponds to the temporal evolution of the distance and orientation features over time. A Long-Short Term Memory-based Recurrent Neural Network is then trained to classify each time series. Experimental evaluation over a dataset of 30.000 images has shown promising results on the proposed method for social interaction detection in egocentric photo-streams.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/C9FVGBHP/Aghaei et al. - 2016 - With Whom Do I Interact Detecting Social Interact.pdf;/Users/nelesuffo/Zotero/storage/U5S9R9CW/1605.04129v2.pdf}
}

@article{arabaciMultimodalEgocentricActivity2021,
  title = {Multi-Modal Egocentric Activity Recognition Using Multi-Kernel Learning},
  author = {Arabacı, Mehmet Ali and Özkan, Fatih and Surer, Elif and Jančovič, Peter and Temizel, Alptekin},
  date = {2021-05},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {80},
  number = {11},
  pages = {16299--16328},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08789-7},
  url = {https://link.springer.com/10.1007/s11042-020-08789-7},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/P2YJ73YD/Arabacı et al. - 2021 - Multi-modal egocentric activity recognition using .pdf}
}

@article{borjonViewTheirOwn2018,
  title = {A {{View}} of {{Their Own}}: {{Capturing}} the {{Egocentric View}} of {{Infants}} and {{Toddlers}} with {{Head-Mounted Cameras}}},
  shorttitle = {A {{View}} of {{Their Own}}},
  author = {Borjon, Jeremy I. and Schroer, Sara E. and Bambach, Sven and Slone, Lauren K. and Abney, Drew H. and Crandall, David J. and Smith, Linda B.},
  date = {2018-10-05},
  journaltitle = {Journal of Visualized Experiments},
  shortjournal = {JoVE},
  number = {140},
  pages = {58445},
  issn = {1940-087X},
  doi = {10.3791/58445-v},
  url = {https://app.jove.com/v/58445},
  urldate = {2024-12-20},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/JLAL6IZ2/Borjon et al. - 2018 - A View of Their Own Capturing the Egocentric View.pdf}
}

@software{brostromBoxMOTCollectionSOTA2023,
  title = {{{BoxMOT}}: {{A}} Collection of {{SOTA}} Real-Time, Multi-Object Trackers for Object Detectors},
  shorttitle = {{{BoxMOT}}},
  author = {Broström, Mikel},
  date = {2023-06-27},
  doi = {10.5281/ZENODO.7452873},
  url = {https://zenodo.org/record/7452873},
  urldate = {2024-10-23},
  abstract = {This repo contains a collections of state-of-the-art multi-object trackers. Supported ones at the moment are: DeepOCSORT , BoTSORT , StrongSORT, OCSORT and ByteTrack. DeepOCSORT, BoTSORT and StrongSORT are based on motion + appearance description; OCSORT and ByteTrack are based on motion only. For the methods using appearance description, lightweight state-of-the-art ReID models (LightMBN, OSNet and more) are downloaded automatically as well. We provide examples on how to use this package together with popular object detection models. Right now Yolov8, Yolo-NAS and YOLOX are available.},
  organization = {Zenodo},
  version = {10.0.15}
}

@online{duStrongSORTMakeDeepSORT2022,
  title = {{{StrongSORT}}: {{Make DeepSORT Great Again}}},
  shorttitle = {{{StrongSORT}}},
  author = {Du, Yunhao and Zhao, Zhicheng and Song, Yang and Zhao, Yanyun and Su, Fei and Gong, Tao and Meng, Hongying},
  date = {2022},
  doi = {10.48550/ARXIV.2202.13514},
  url = {https://arxiv.org/abs/2202.13514},
  urldate = {2024-10-23},
  abstract = {Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/YGKFB2NL/Du et al. - 2022 - StrongSORT Make DeepSORT Great Again.pdf}
}

@incollection{kapidisObjectDetectionBasedLocation2020,
  title = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}: {{A Systematic Analysis}}},
  shorttitle = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}},
  booktitle = {Smart {{Assisted Living}}},
  author = {Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas P. J. J. and Veltkamp, Remco C.},
  editor = {Chen, Feng and García-Betances, Rebeca I. and Chen, Liming and Cabrera-Umpiérrez, María Fernanda and Nugent, Chris},
  date = {2020},
  pages = {119--145},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-25590-9_6},
  url = {http://link.springer.com/10.1007/978-3-030-25590-9_6},
  urldate = {2024-10-23},
  isbn = {978-3-030-25589-3 978-3-030-25590-9},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/XTFMTBU9/Kapidis et al. - 2020 - Object Detection-Based Location and Activity Class.pdf}
}

@online{lavechinOpensourceVoiceType2020,
  title = {An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings},
  author = {Lavechin, Marvin and Bousbib, Ruben and Bredin, Hervé and Dupoux, Emmanuel and Cristia, Alejandrina},
  date = {2020},
  doi = {10.48550/ARXIV.2005.12656},
  url = {https://arxiv.org/abs/2005.12656},
  urldate = {2024-11-14},
  abstract = {Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages. Our model can be used as input for downstream tasks such as estimating the number of words produced by adult speakers, or the number of linguistic units produced by children. Our architecture combines SincNet filters with a stack of recurrent layers and outperforms by a large margin the state-of-the-art system, the Language ENvironment Analysis (LENA) that has been used in numerous child language studies.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Audio and Speech Processing (eess.AS),FOS: Electrical engineering electronic engineering information engineering,I.2.7}
}

@online{linBMNBoundaryMatchingNetwork2019,
  title = {{{BMN}}: {{Boundary-Matching Network}} for {{Temporal Action Proposal Generation}}},
  shorttitle = {{{BMN}}},
  author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
  date = {2019},
  doi = {10.48550/ARXIV.1907.09702},
  url = {https://arxiv.org/abs/1907.09702},
  urldate = {2024-11-13},
  abstract = {Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{longBabyViewDatasetHighresolution2024,
  title = {The {{BabyView}} Dataset: {{High-resolution}} Egocentric Videos of Infants' and Young Children's Everyday Experiences},
  shorttitle = {The {{BabyView}} Dataset},
  author = {Long, Bria and Xiang, Violet and Stojanov, Stefan and Sparks, Robert Z. and Yin, Zi and Keene, Grace E. and Tan, Alvin W. M. and Feng, Steven Y. and Zhuang, Chengxu and Marchman, Virginia A. and Yamins, Daniel L. K. and Frank, Michael C.},
  date = {2024-06-14},
  eprint = {2406.10447},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.10447},
  url = {http://arxiv.org/abs/2406.10447},
  urldate = {2024-12-20},
  abstract = {Human children far exceed modern machine learning algorithms in their sample efficiency, achieving high performance in key domains with much less data than current models. This ''data gap'' is a key challenge both for building intelligent artificial systems and for understanding human development. Egocentric video capturing children's experience -- their ''training data'' -- is a key ingredient for comparison of humans and models and for the development of algorithmic innovations to bridge this gap. Yet there are few such datasets available, and extant data are low-resolution, have limited metadata, and importantly, represent only a small set of children's experiences. Here, we provide the first release of the largest developmental egocentric video dataset to date -- the BabyView dataset -- recorded using a high-resolution camera with a large vertical field-of-view and gyroscope/accelerometer data. This 493 hour dataset includes egocentric videos from children spanning 6 months - 5 years of age in both longitudinal, at-home contexts and in a preschool environment. We provide gold-standard annotations for the evaluation of speech transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains. We train self-supervised language and vision models and evaluate their transfer to out-of-distribution tasks including syntactic structure learning, object recognition, depth estimation, and image segmentation. Although performance in each scales with dataset size, overall performance is relatively lower than when models are trained on curated datasets, especially in the visual domain. Our dataset stands as an open challenge for robust, humanlike AI systems: how can such systems achieve human-levels of success on the same scale and distribution of training data as humans?},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/RGPHCL8P/Long et al. - 2024 - The BabyView dataset High-resolution egocentric v.pdf;/Users/nelesuffo/Zotero/storage/RN3TYVJP/2406.html}
}

@software{mmaction2contributorsOpenMMLabsNextGeneration2020,
  title = {{{OpenMMLab}}'s {{Next Generation Video Understanding Toolbox}} and {{Benchmark}}},
  author = {MMAction2 Contributors},
  date = {2020},
  url = {urlhttps://github.com/open-mmlab/mmaction2}
}

@article{nunez-marcosEgocentricVisionbasedAction2022,
  title = {Egocentric {{Vision-based Action Recognition}}: {{A}} Survey},
  shorttitle = {Egocentric {{Vision-based Action Recognition}}},
  author = {Núñez-Marcos, Adrián and Azkune, Gorka and Arganda-Carreras, Ignacio},
  date = {2022-02},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {472},
  pages = {175--197},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.11.081},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221017586},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/I5LRFE37/Núñez-Marcos et al. - 2022 - Egocentric Vision-based Action Recognition A surv.pdf}
}

@article{rogoffImportanceUnderstandingChildrens2018,
  title = {The Importance of Understanding Children’s Lived Experience},
  author = {Rogoff, Barbara and Dahl, Audun and Callanan, Maureen},
  date = {2018-12},
  journaltitle = {Developmental Review},
  shortjournal = {Developmental Review},
  volume = {50},
  pages = {5--15},
  issn = {02732297},
  doi = {10.1016/j.dr.2018.05.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0273229718300236},
  urldate = {2024-12-23},
  langid = {english}
}

@article{royPredictingBirthSpoken2015,
  title = {Predicting the Birth of a Spoken Word},
  author = {Roy, Brandon C. and Frank, Michael C. and DeCamp, Philip and Miller, Matthew and Roy, Deb},
  date = {2015-10-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {112},
  number = {41},
  pages = {12663--12668},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1419773112},
  url = {https://pnas.org/doi/full/10.1073/pnas.1419773112},
  urldate = {2024-12-20},
  abstract = {Significance             The emergence of productive language is a critical milestone in a child’s life. Laboratory studies have identified many individual factors that contribute to word learning, and larger scale studies show correlations between aspects of the home environment and language outcomes. To date, no study has compared across many factors involved in word learning. We introduce a new ultradense set of recordings that capture a single child’s daily experience during the emergence of language. We show that words used in distinctive spatial, temporal, and linguistic contexts are produced earlier, suggesting they are easier to learn. These findings support the importance of multimodal context in word learning for one child and provide new methods for quantifying the quality of children’s language input.           ,              Children learn words through an accumulation of interactions grounded in context. Although many factors in the learning environment have been shown to contribute to word learning in individual studies, no empirical synthesis connects across factors. We introduce a new ultradense corpus of audio and video recordings of a single child’s life that allows us to measure the child’s experience of each word in his vocabulary. This corpus provides the first direct comparison, to our knowledge, between different predictors of the child’s production of individual words. We develop a series of new measures of the distinctiveness of the spatial, temporal, and linguistic contexts in which a word appears, and show that these measures are stronger predictors of learning than frequency of use and that, unlike frequency, they play a consistent role across different syntactic categories. Our findings provide a concrete instantiation of classic ideas about the role of coherent activities in word learning and demonstrate the value of multimodal data in understanding children’s language acquisition.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/NKMNP2P4/Roy et al. - 2015 - Predicting the birth of a spoken word.pdf}
}

@article{saberCurriculumLearningInfant2023,
  title = {Curriculum Learning with Infant Egocentric Videos},
  author = {Saber, Sheybani and Hansaria,, Himanshu and Wood, Justin N. and Smith, Linda B. and Tiganj, Zoran},
  date = {2023},
  journaltitle = {NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages = {54199--54212},
  abstract = {Infants possess a remarkable ability to rapidly learn and process visual inputs. As an infant's mobility increases, so does the variety and dynamics of their visual inputs. Is this change in the properties of the visual inputs beneficial or even critical for the proper development of the visual system? To address this question, we used video recordings from infants wearing head-mounted cameras to train a variety of self-supervised learning models. Critically, we separated the infant data by age group and evaluated the importance of training with a curriculum aligned with developmental order. We found that initiating learning with the data from the youngest age group provided the strongest learning signal and led to the best learning outcomes in terms of downstream task performance. We then showed that the benefits of the data from the youngest age group are due to the slowness and simplicity of the visual experience. The results provide strong empirical evidence for the importance of the properties of the early infant experience and developmental progression in training. More broadly, our approach and findings take a noteworthy step towards reverse engineering the learning mechanisms in newborn brains using image-computable models from artificial intelligence.}
}

@online{simonyanTwoStreamConvolutionalNetworks2014,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  doi = {10.48550/ARXIV.1406.2199},
  url = {https://arxiv.org/abs/1406.2199},
  urldate = {2024-11-14},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{smithContributionsHeadMountedCameras2015,
  title = {Contributions of {{Head-Mounted Cameras}} to {{Studying}} the {{Visual Environments}} of {{Infants}} and {{Young Children}}},
  author = {Smith, Linda B. and Yu, Chen and Yoshida, Hanako and Fausey, Caitlin M.},
  date = {2015-05-27},
  journaltitle = {Journal of Cognition and Development},
  shortjournal = {Journal of Cognition and Development},
  volume = {16},
  number = {3},
  pages = {407--419},
  issn = {1524-8372, 1532-7647},
  doi = {10.1080/15248372.2014.933430},
  url = {http://www.tandfonline.com/doi/full/10.1080/15248372.2014.933430},
  urldate = {2024-12-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/44QZX3Y5/Smith et al. - 2015 - Contributions of Head-Mounted Cameras to Studying .pdf}
}

@article{sullivanSAYCamLargeLongitudinal2021,
  title = {{{SAYCam}}: {{A Large}}, {{Longitudinal Audiovisual Dataset Recorded From}} the {{Infant}}’s {{Perspective}}},
  shorttitle = {{{SAYCam}}},
  author = {Sullivan, Jessica and Mei, Michelle and Perfors, Andrew and Wojcik, Erica and Frank, Michael C.},
  date = {2021-05-26},
  journaltitle = {Open Mind},
  volume = {5},
  pages = {20--29},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00039},
  url = {https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset},
  urldate = {2024-12-20},
  abstract = {Abstract             We introduce a new resource: the SAYCam corpus. Infants aged 6–32 months wore a head-mounted camera for approximately 2 hr per week, over the course of approximately two-and-a-half years. The result is a large, naturalistic, longitudinal dataset of infant- and child-perspective videos. Over 200,000 words of naturalistic speech have already been transcribed. Similarly, the dataset is searchable using a number of criteria (e.g., age of participant, location, setting, objects present). The resulting dataset will be of broad use to psychologists, linguists, and computer scientists.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/A5ANFVQR/Sullivan et al. - 2021 - SAYCam A Large, Longitudinal Audiovisual Dataset .pdf}
}

@article{truongCrossviewActionRecognition2024,
  title = {Cross-View Action Recognition Understanding from Exocentric to Egocentric Perspective},
  author = {Truong, Thanh-Dat and Luu, Khoa},
  date = {2024-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  pages = {128731},
  issn = {09252312},
  doi = {10.1016/j.neucom.2024.128731},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231224015029},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/95IM2Q74/Truong and Luu - 2024 - Cross-view action recognition understanding from e.pdf}
}

@online{tsutsuiComputationalModelEarly2020,
  title = {A {{Computational Model}} of {{Early Word Learning}} from the {{Infant}}'s {{Point}} of {{View}}},
  author = {Tsutsui, Satoshi and Chandrasekaran, Arjun and Reza, Md Alimoor and Crandall, David and Yu, Chen},
  date = {2020-06-04},
  eprint = {2006.02802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.02802},
  url = {http://arxiv.org/abs/2006.02802},
  urldate = {2024-12-20},
  abstract = {Human infants have the remarkable ability to learn the associations between object names and visual objects from inherently ambiguous experiences. Researchers in cognitive science and developmental psychology have built formal models that implement in-principle learning algorithms, and then used pre-selected and pre-cleaned datasets to test the abilities of the models to find statistical regularities in the input data. In contrast to previous modeling approaches, the present study used egocentric video and gaze data collected from infant learners during natural toy play with their parents. This allowed us to capture the learning environment from the perspective of the learner's own point of view. We then used a Convolutional Neural Network (CNN) model to process sensory data from the infant's point of view and learn name-object associations from scratch. As the first model that takes raw egocentric video to simulate infant word learning, the present study provides a proof of principle that the problem of early word learning can be solved, using actual visual data perceived by infant learners. Moreover, we conducted simulation experiments to systematically determine how visual, perceptual, and attentional properties of infants' sensory experiences may affect word learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/RQHSGKUR/Tsutsui et al. - 2020 - A Computational Model of Early Word Learning from .pdf;/Users/nelesuffo/Zotero/storage/MQUAE3QR/2006.html}
}

@online{wangFasterPersonReIdentification2020,
  title = {Faster {{Person Re-Identification}}},
  author = {Wang, Guan'an and Gong, Shaogang and Cheng, Jian and Hou, Zengguang},
  date = {2020},
  doi = {10.48550/ARXIV.2008.06826},
  url = {https://arxiv.org/abs/2008.06826},
  urldate = {2024-10-23},
  abstract = {Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a \$F\_β\$ score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8\% more accurate but also 5x faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is \$50\textbackslash times\$ faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/WG9M6GYH/Wang et al. - 2020 - Faster Person Re-Identification.pdf}
}

@online{wangTemporalSegmentNetworks2017,
  title = {Temporal {{Segment Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  date = {2017},
  doi = {10.48550/ARXIV.1705.02953},
  url = {https://arxiv.org/abs/1705.02953},
  urldate = {2024-11-14},
  abstract = {Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0\%), UCF101 (94.9\%), THUMOS14 (80.1\%), and ActivityNet v1.2 (89.6\%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0\%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{yoshidaWhatsViewToddlers2008,
  title = {What's in {{View}} for {{Toddlers}}? {{Using}} a {{Head Camera}} to {{Study Visual Experience}}},
  shorttitle = {What's in {{View}} for {{Toddlers}}?},
  author = {Yoshida, Hanako and Smith, Linda B.},
  date = {2008-05-06},
  journaltitle = {Infancy},
  shortjournal = {Infancy},
  volume = {13},
  number = {3},
  pages = {229--248},
  issn = {1525-0008, 1532-7078},
  doi = {10.1080/15250000802004437},
  url = {https://onlinelibrary.wiley.com/doi/10.1080/15250000802004437},
  urldate = {2024-12-23},
  abstract = {This article reports 2 experiments using a new method to study 18‐to 24‐month‐olds' visual experiences as they interact with objects. Experiment 1 presents evidence on the coupling of head and eye movements and thus the validity of the head camera view of the infant's visual field in the geometry of the task context. Experiment 2 demonstrates the use of this method in the naturalistic context of toy play with a parent. The results point to the embodied nature of toddlers' attentional strategies and to importance of hands and hand actions in their visual experience of objects. The head camera thus appears to be a promising method that, despite some limitations, will yield new insights about the ecology and content of young children's experiences.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/ZVPCJWU6/Yoshida and Smith - 2008 - What's in View for Toddlers Using a Head Camera t.pdf}
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2024},
  url = {https://www.R-project.org/},
}
@Manual{R-BayesFactor,
  title = {BayesFactor: Computation of Bayes Factors for Common Designs},
  author = {Richard D. Morey and Jeffrey N. Rouder},
  year = {2024},
  note = {R package version 0.9.12-4.7},
  url = {https://CRAN.R-project.org/package=BayesFactor},
}
@Article{R-brms_a,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}
@Article{R-brms_b,
  title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
  author = {Paul-Christian Bürkner},
  journal = {The R Journal},
  year = {2018},
  volume = {10},
  number = {1},
  pages = {395--411},
  doi = {10.32614/RJ-2018-017},
  encoding = {UTF-8},
}
@Article{R-brms_c,
  title = {Bayesian Item Response Modeling in {R} with {brms} and {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2021},
  volume = {100},
  number = {5},
  pages = {1--54},
  doi = {10.18637/jss.v100.i05},
  encoding = {UTF-8},
}
@Manual{R-broom,
  title = {broom: Convert Statistical Objects into Tidy Tibbles},
  author = {David Robinson and Alex Hayes and Simon Couch},
  year = {2024},
  note = {R package version 1.0.7},
  url = {https://CRAN.R-project.org/package=broom},
}
@Article{R-coda,
  title = {CODA: Convergence Diagnosis and Output Analysis for MCMC},
  author = {Martyn Plummer and Nicky Best and Kate Cowles and Karen Vines},
  journal = {R News},
  year = {2006},
  volume = {6},
  number = {1},
  pages = {7--11},
  url = {https://journal.r-project.org/archive/},
  pdf = {https://www.r-project.org/doc/Rnews/Rnews_2006-1.pdf},
}
@Manual{R-cowplot,
  title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 1.1.3},
  url = {https://CRAN.R-project.org/package=cowplot},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
  year = {2023},
  note = {R package version 1.1.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-ggplotify,
  title = {ggplotify: Convert Plot to 'grob' or 'ggplot' Object},
  author = {Guangchuang Yu},
  year = {2023},
  note = {R package version 0.1.2},
  url = {https://CRAN.R-project.org/package=ggplotify},
}
@Manual{R-ggpubr,
  title = {ggpubr: 'ggplot2' Based Publication Ready Plots},
  author = {Alboukadel Kassambara},
  year = {2023},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggpubr},
}
@Manual{R-ggridges,
  title = {ggridges: Ridgeline Plots in 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 0.5.6},
  url = {https://CRAN.R-project.org/package=ggridges},
}
@Manual{R-ggthemes,
  title = {ggthemes: Extra Themes, Scales and Geoms for 'ggplot2'},
  author = {Jeffrey B. Arnold},
  year = {2024},
  note = {R package version 5.1.0},
  url = {https://CRAN.R-project.org/package=ggthemes},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Manual{R-kableExtra,
  title = {kableExtra: Construct Complex Table with 'kable' and Pipe Syntax},
  author = {Hao Zhu},
  year = {2024},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=kableExtra},
}
@Article{R-lubridate,
  title = {Dates and Times Made Easy with {lubridate}},
  author = {Garrett Grolemund and Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {3},
  pages = {1--25},
  url = {https://www.jstatsoft.org/v40/i03/},
}
@Manual{R-magick,
  title = {magick: Advanced Graphics and Image-Processing in R},
  author = {Jeroen Ooms},
  year = {2024},
  note = {R package version 2.8.5},
  url = {https://CRAN.R-project.org/package=magick},
}
@Manual{R-Matrix,
  title = {Matrix: Sparse and Dense Matrix Classes and Methods},
  author = {Douglas Bates and Martin Maechler and Mikael Jagan},
  year = {2024},
  note = {R package version 1.7-1},
  url = {https://CRAN.R-project.org/package=Matrix},
}
@Manual{R-papaja,
  title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
  author = {Frederik Aust and Marius Barth},
  year = {2024},
  note = {R package version 0.1.3},
  url = {https://github.com/crsh/papaja},
  doi = {10.32614/CRAN.package.papaja},
}
@Manual{R-patchwork,
  title = {patchwork: The Composer of Plots},
  author = {Thomas Lin Pedersen},
  year = {2024},
  note = {R package version 1.3.0},
  url = {https://CRAN.R-project.org/package=patchwork},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Hadley Wickham and Lionel Henry},
  year = {2023},
  note = {R package version 1.0.2},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Article{R-Rcpp_a,
  title = {{Rcpp}: Seamless {R} and {C++} Integration},
  author = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {8},
  pages = {1--18},
  doi = {10.18637/jss.v040.i08},
}
@Article{R-Rcpp_b,
  title = {{Extending {R} with {C++}: A Brief Introduction to {Rcpp}}},
  author = {Dirk Eddelbuettel and James Joseph Balamuta},
  journal = {The American Statistician},
  year = {2018},
  volume = {72},
  number = {1},
  pages = {28-36},
  doi = {10.1080/00031305.2017.1375990},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Jennifer Bryan},
  year = {2024},
  note = {R package version 2.1.5},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-readxl,
  title = {readxl: Read Excel Files},
  author = {Hadley Wickham and Jennifer Bryan},
  year = {2023},
  note = {R package version 1.4.3},
  url = {https://CRAN.R-project.org/package=readxl},
}
@Article{R-reshape2,
  title = {Reshaping Data with the {reshape} Package},
  author = {Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2007},
  volume = {21},
  number = {12},
  pages = {1--20},
  url = {http://www.jstatsoft.org/v21/i12/},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.5.1},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2023},
  note = {R package version 3.2.1},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Davis Vaughan and Maximilian Girlich},
  year = {2024},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Article{R-tidyverse,
  title = {Welcome to the {tidyverse}},
  author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686},
}
@Manual{R-tinylabels,
  title = {{tinylabels}: Lightweight Variable Labels},
  author = {Marius Barth},
  year = {2023},
  note = {R package version 0.2.4},
  url = {https://cran.r-project.org/package=tinylabels},
}
@Article{R-zoo,
  title = {zoo: S3 Infrastructure for Regular and Irregular Time Series},
  author = {Achim Zeileis and Gabor Grothendieck},
  journal = {Journal of Statistical Software},
  year = {2005},
  volume = {14},
  number = {6},
  pages = {1--27},
  doi = {10.18637/jss.v014.i06},
}
