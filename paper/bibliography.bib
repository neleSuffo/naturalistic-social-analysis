@article{aghaeiWhomInteractDetecting2016,
  title = {With {{Whom Do I Interact}}? {{Detecting Social Interactions}} in {{Egocentric Photo-streams}}},
  shorttitle = {With {{Whom Do I Interact}}?},
  author = {Aghaei, Maedeh and Dimiccoli, Mariella and Radeva, Petia},
  date = {2016},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1605.04129},
  url = {https://arxiv.org/abs/1605.04129},
  urldate = {2024-05-16},
  abstract = {Given a user wearing a low frame rate wearable camera during a day, this work aims to automatically detect the moments when the user gets engaged into a social interaction solely by reviewing the automatically captured photos by the worn camera. The proposed method, inspired by the sociological concept of F-formation, exploits distance and orientation of the appearing individuals -with respect to the user- in the scene from a bird-view perspective. As a result, the interaction pattern over the sequence can be understood as a two-dimensional time series that corresponds to the temporal evolution of the distance and orientation features over time. A Long-Short Term Memory-based Recurrent Neural Network is then trained to classify each time series. Experimental evaluation over a dataset of 30.000 images has shown promising results on the proposed method for social interaction detection in egocentric photo-streams.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/C9FVGBHP/Aghaei et al. - 2016 - With Whom Do I Interact Detecting Social Interact.pdf;/Users/nelesuffo/Zotero/storage/U5S9R9CW/1605.04129v2.pdf}
}

@inproceedings{alaniClassifyingImbalancedMultimodal2020,
  title = {Classifying {{Imbalanced Multi-modal Sensor Data}} for {{Human Activity Recognition}} in a {{Smart Home}} Using {{Deep Learning}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Alani, Ali A. and Cosma, Georgina and Taherkhani, Aboozar},
  date = {2020-07},
  pages = {1--8},
  publisher = {IEEE},
  location = {Glasgow, United Kingdom},
  doi = {10.1109/IJCNN48605.2020.9207697},
  url = {https://ieeexplore.ieee.org/document/9207697/},
  urldate = {2025-01-02},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-72816-926-2},
  file = {/Users/nelesuffo/Zotero/storage/LZ2V5GUC/Alani et al. - 2020 - Classifying Imbalanced Multi-modal Sensor Data for.pdf}
}

@article{arabaciMultimodalEgocentricActivity2021,
  title = {Multi-Modal Egocentric Activity Recognition Using Multi-Kernel Learning},
  author = {Arabacı, Mehmet Ali and Özkan, Fatih and Surer, Elif and Jančovič, Peter and Temizel, Alptekin},
  date = {2021-05},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {80},
  number = {11},
  pages = {16299--16328},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08789-7},
  url = {https://link.springer.com/10.1007/s11042-020-08789-7},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/P2YJ73YD/Arabacı et al. - 2021 - Multi-modal egocentric activity recognition using .pdf}
}

@online{baevskiWav2vec20Framework2020,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020},
  doi = {10.48550/ARXIV.2006.11477},
  url = {https://arxiv.org/abs/2006.11477},
  urldate = {2025-01-12},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Audio and Speech Processing (eess.AS),Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Machine Learning (cs.LG),Sound (cs.SD)}
}

@inproceedings{bambachLendingHandDetecting2015,
  title = {Lending {{A Hand}}: {{Detecting Hands}} and {{Recognizing Activities}} in {{Complex Egocentric Interactions}}},
  shorttitle = {Lending {{A Hand}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Bambach, Sven and Lee, Stefan and Crandall, David J. and Yu, Chen},
  date = {2015-12},
  pages = {1949--1957},
  publisher = {IEEE},
  location = {Santiago, Chile},
  doi = {10.1109/ICCV.2015.226},
  url = {http://ieeexplore.ieee.org/document/7410583/},
  urldate = {2025-01-02},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  file = {/Users/nelesuffo/Zotero/storage/NVTHGNDX/Bambach et al. - 2015 - Lending A Hand Detecting Hands and Recognizing Ac.pdf}
}

@article{bergelsonEverydayLanguageInput2023,
  title = {Everyday Language Input and Production in 1,001 Children from Six Continents},
  author = {Bergelson, Elika and Soderstrom, Melanie and Schwarz, Iris-Corinna and Rowland, Caroline F. and Ramírez-Esparza, Nairán and R. Hamrick, Lisa and Marklund, Ellen and Kalashnikova, Marina and Guez, Ava and Casillas, Marisa and Benetti, Lucia and Alphen, Petra Van and Cristia, Alejandrina},
  date = {2023-12-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {120},
  number = {52},
  pages = {e2300671120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2300671120},
  url = {https://pnas.org/doi/10.1073/pnas.2300671120},
  urldate = {2025-01-11},
  abstract = {Language is a universal human ability, acquired readily by young children, who otherwise struggle with many basics of survival. And yet, language ability is variable across individuals. Naturalistic and experimental observations suggest that children’s linguistic skills vary with factors like socioeconomic status and children’s gender. But which factors really influence children’s day-to-day language use? Here, we leverage speech technology in a big-data approach to report on a unique cross-cultural and diverse data set: {$>$}2,500 d-long, child-centered audio-recordings of 1,001 2- to 48-mo-olds from 12 countries spanning six continents across urban, farmer-forager, and subsistence-farming contexts. As expected, age and language-relevant clinical risks and diagnoses predicted how much speech (and speech-like vocalization) children produced. Critically, so too did adult talk in children’s environments: Children who heard more talk from adults produced more speech. In contrast to previous conclusions based on more limited sampling methods and a different set of language proxies, socioeconomic status (operationalized as maternal education) was not significantly associated with children’s productions over the first 4 y of life, and neither were gender or multilingualism. These findings from large-scale naturalistic data advance our understanding of which factors are robust predictors of variability in the speech behaviors of young learners in a wide range of everyday contexts.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/B4PYKNBG/Bergelson et al. - 2023 - Everyday language input and production in 1,001 ch.pdf}
}

@article{borjonViewTheirOwn2018,
  title = {A {{View}} of {{Their Own}}: {{Capturing}} the {{Egocentric View}} of {{Infants}} and {{Toddlers}} with {{Head-Mounted Cameras}}},
  shorttitle = {A {{View}} of {{Their Own}}},
  author = {Borjon, Jeremy I. and Schroer, Sara E. and Bambach, Sven and Slone, Lauren K. and Abney, Drew H. and Crandall, David J. and Smith, Linda B.},
  date = {2018-10-05},
  journaltitle = {Journal of Visualized Experiments},
  shortjournal = {JoVE},
  number = {140},
  pages = {58445},
  issn = {1940-087X},
  doi = {10.3791/58445-v},
  url = {https://app.jove.com/v/58445},
  urldate = {2024-12-20},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/JLAL6IZ2/Borjon et al. - 2018 - A View of Their Own Capturing the Egocentric View.pdf}
}

@software{brostromBoxMOTCollectionSOTA2023,
  title = {{{BoxMOT}}: {{A}} Collection of {{SOTA}} Real-Time, Multi-Object Trackers for Object Detectors},
  shorttitle = {{{BoxMOT}}},
  author = {Broström, Mikel},
  date = {2023-06-27},
  doi = {10.5281/ZENODO.7452873},
  url = {https://zenodo.org/record/7452873},
  urldate = {2024-10-23},
  abstract = {This repo contains a collections of state-of-the-art multi-object trackers. Supported ones at the moment are: DeepOCSORT , BoTSORT , StrongSORT, OCSORT and ByteTrack. DeepOCSORT, BoTSORT and StrongSORT are based on motion + appearance description; OCSORT and ByteTrack are based on motion only. For the methods using appearance description, lightweight state-of-the-art ReID models (LightMBN, OSNet and more) are downloaded automatically as well. We provide examples on how to use this package together with popular object detection models. Right now Yolov8, Yolo-NAS and YOLOX are available.},
  organization = {Zenodo},
  version = {10.0.15}
}

@online{caoOpenPoseRealtimeMultiPerson2018,
  title = {{{OpenPose}}: {{Realtime Multi-Person 2D Pose Estimation}} Using {{Part Affinity Fields}}},
  shorttitle = {{{OpenPose}}},
  author = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2018},
  doi = {10.48550/ARXIV.1812.08008},
  url = {https://arxiv.org/abs/1812.08008},
  urldate = {2025-01-11},
  abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@book{carpendaleWhatMakesUs2020,
  title = {What {{Makes Us Human}}: {{How Minds Develop}} through {{Social Interactions}}},
  shorttitle = {What {{Makes Us Human}}},
  author = {Carpendale, Jeremy and Lewis, Charlie},
  date = {2020-12-21},
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9781003125105},
  url = {https://www.taylorfrancis.com/books/9781000283983},
  urldate = {2025-01-11},
  isbn = {978-1-00-312510-5},
  langid = {english}
}

@online{carreiraQuoVadisAction2017,
  title = {Quo {{Vadis}}, {{Action Recognition}}? {{A New Model}} and the {{Kinetics Dataset}}},
  shorttitle = {Quo {{Vadis}}, {{Action Recognition}}?},
  author = {Carreira, Joao and Zisserman, Andrew},
  date = {2017},
  doi = {10.48550/ARXIV.1705.07750},
  url = {https://arxiv.org/abs/1705.07750},
  urldate = {2025-01-12},
  abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{chenDyadicAffectParentChild2023,
  title = {Dyadic {{Affect}} in {{Parent-Child Multimodal Interaction}}: {{Introducing}} the {{DAMI-P2C Dataset}} and Its {{Preliminary Analysis}}},
  shorttitle = {Dyadic {{Affect}} in {{Parent-Child Multimodal Interaction}}},
  author = {Chen, Huili and Alghowinem, Sharifa and Jang, Soo Jung and Breazeal, Cynthia and Park, Hae Won},
  date = {2023-10-01},
  journaltitle = {IEEE Transactions on Affective Computing},
  shortjournal = {IEEE Trans. Affective Comput.},
  volume = {14},
  number = {4},
  pages = {3345--3361},
  issn = {1949-3045, 2371-9850},
  doi = {10.1109/TAFFC.2022.3178689},
  url = {https://ieeexplore.ieee.org/document/9784429/},
  urldate = {2025-01-12},
  file = {/Users/nelesuffo/Zotero/storage/XZZVRTPE/Chen et al. - 2023 - Dyadic Affect in Parent-Child Multimodal Interacti.pdf}
}

@article{debarbaroTenLessonsInfants2022,
  title = {Ten {{Lessons About Infants}}’ {{Everyday Experiences}}},
  author = {De Barbaro, Kaya and Fausey, Caitlin M.},
  date = {2022-02},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {31},
  number = {1},
  pages = {28--33},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/09637214211059536},
  url = {https://journals.sagepub.com/doi/10.1177/09637214211059536},
  urldate = {2024-12-23},
  abstract = {Audio recorders, accelerometers, and cameras that infants wear throughout their everyday lives capture the experiences that are available to shape development. Using sensors to capture behaviors in natural settings can reveal patterns within the everyday hubbub that are unknowable using methods that capture shorter, more isolated, or more planned slices of behavior. Here, we review 10 lessons learned from recent endeavors in which researchers neither designed nor participated in infants’ experiences and instead quantified patterns that arose within infants’ own spontaneously arising everyday experiences. The striking heterogeneity of experiences—the fact that there is no meaningfully “representative” hour of a day, instance of a category, interaction context, or infant—inspires next steps in theory and practice that embrace the complex, dynamic, and multiple pathways of human development.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/29MXEGW7/De Barbaro and Fausey - 2022 - Ten Lessons About Infants’ Everyday Experiences.pdf}
}

@article{donnellyLongitudinalRelationshipConversational2021,
  title = {The {{Longitudinal Relationship Between Conversational Turn}}‐{{Taking}} and {{Vocabulary Growth}} in {{Early Language Development}}},
  author = {Donnelly, Seamus and Kidd, Evan},
  date = {2021-03},
  journaltitle = {Child Development},
  shortjournal = {Child Development},
  volume = {92},
  number = {2},
  pages = {609--625},
  issn = {0009-3920, 1467-8624},
  doi = {10.1111/cdev.13511},
  url = {https://srcd.onlinelibrary.wiley.com/doi/10.1111/cdev.13511},
  urldate = {2025-01-11},
  abstract = {Children acquire language embedded within the rich social context of interaction. This paper reports on a longitudinal study investigating the developmental relationship between conversational turn‐taking and vocabulary growth in English‐acquiring children (               N               ~=~122) followed between 9 and 24~months. Daylong audio recordings obtained every 3~months provided several indices of the language environment, including the number of adult words children heard in their environment and their number of conversational turns. Vocabulary was measured independently via parental report. Growth curve analyses revealed a bidirectional relationship between conversational turns and vocabulary growth, controlling for the amount of words in children’s environments. The results are consistent with theoretical approaches that identify social interaction as a core component of early language acquisition.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/KTKFUA2Y/Donnelly and Kidd - 2021 - The Longitudinal Relationship Between Conversation.pdf}
}

@online{duStrongSORTMakeDeepSORT2022,
  title = {{{StrongSORT}}: {{Make DeepSORT Great Again}}},
  shorttitle = {{{StrongSORT}}},
  author = {Du, Yunhao and Zhao, Zhicheng and Song, Yang and Zhao, Yanyun and Su, Fei and Gong, Tao and Meng, Hongying},
  date = {2022},
  doi = {10.48550/ARXIV.2202.13514},
  url = {https://arxiv.org/abs/2202.13514},
  urldate = {2024-10-23},
  abstract = {Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/YGKFB2NL/Du et al. - 2022 - StrongSORT Make DeepSORT Great Again.pdf}
}

@online{fraileUpStoryUppsalaStorytelling2024,
  title = {{{UpStory}}: The {{Uppsala Storytelling}} Dataset},
  shorttitle = {{{UpStory}}},
  author = {Fraile, Marc and Calvo-Barajas, Natalia and Apeiron, Anastasia Sophia and Varni, Giovanna and Lindblad, Joakim and Sladoje, Nataša and Castellano, Ginevra},
  date = {2024},
  doi = {10.48550/ARXIV.2407.04352},
  url = {https://arxiv.org/abs/2407.04352},
  urldate = {2025-01-12},
  abstract = {Friendship and rapport play an important role in the formation of constructive social interactions, and have been widely studied in educational settings due to their impact on student outcomes. Given the growing interest in automating the analysis of such phenomena through Machine Learning (ML), access to annotated interaction datasets is highly valuable. However, no dataset on dyadic child-child interactions explicitly capturing rapport currently exists. Moreover, despite advances in the automatic analysis of human behaviour, no previous work has addressed the prediction of rapport in child-child dyadic interactions in educational settings. We present UpStory -- the Uppsala Storytelling dataset: a novel dataset of naturalistic dyadic interactions between primary school aged children, with an experimental manipulation of rapport. Pairs of children aged 8-10 participate in a task-oriented activity: designing a story together, while being allowed free movement within the play area. We promote balanced collection of different levels of rapport by using a within-subjects design: self-reported friendships are used to pair each child twice, either minimizing or maximizing pair separation in the friendship network. The dataset contains data for 35 pairs, totalling 3h 40m of audio and video recordings. It includes two video sources covering the play area, as well as separate voice recordings for each child. An anonymized version of the dataset is made publicly available, containing per-frame head pose, body pose, and face features; as well as per-pair information, including the level of rapport. Finally, we provide ML baselines for the prediction of rapport.},
  pubstate = {prepublished},
  version = {1},
  keywords = {FOS: Computer and information sciences,Human-Computer Interaction (cs.HC),Machine Learning (cs.LG)}
}

@book{heyesCognitiveGadgetsCultural2018,
  title = {Cognitive Gadgets: The Cultural Evolution of Thinking},
  shorttitle = {Cognitive Gadgets},
  author = {Heyes, Cecilia M.},
  date = {2018},
  publisher = {Harvard University press},
  location = {Cambridge (Mass.)},
  abstract = {How did human minds become so different from those of other animals? What accounts for our capacity to understand the way the physical world works, to think ourselves into the minds of others, to gossip, read, tell stories about the past, and imagine the future? These questions are not new: they have been debated by philosophers, psychologists, anthropologists, evolutionists, and neurobiologists over the course of centuries. One explanation widely accepted today is that humans have special cognitive instincts. Unlike other living animal species, we are born with complicated mechanisms for reasoning about causation, reading the minds of others, copying behaviors, and using language. Cecilia Heyes agrees that adult humans have impressive pieces of cognitive equipment. In her framing, however, these cognitive gadgets are not instincts programmed in the genes but are constructed in the course of childhood through social interaction. Cognitive gadgets are products of cultural evolution, rather than genetic evolution. At birth, the minds of human babies are only subtly different from the minds of newborn chimpanzees. We are friendlier, our attention is drawn to different things, and we have a capacity to learn and remember that outstrips the abilities of newborn chimpanzees. Yet when these subtle differences are exposed to culture-soaked human environments, they have enormous effects. They enable us to upload distinctively human ways of thinking from the social world around us. As Cognitive Gadgets makes clear, from birth our malleable human minds can learn through culture not only what to think but how to think it},
  isbn = {978-0-674-98015-0},
  langid = {english}
}

@incollection{kapidisObjectDetectionBasedLocation2020,
  title = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}: {{A Systematic Analysis}}},
  shorttitle = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}},
  booktitle = {Smart {{Assisted Living}}},
  author = {Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas P. J. J. and Veltkamp, Remco C.},
  editor = {Chen, Feng and García-Betances, Rebeca I. and Chen, Liming and Cabrera-Umpiérrez, María Fernanda and Nugent, Chris},
  date = {2020},
  pages = {119--145},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-25590-9_6},
  url = {http://link.springer.com/10.1007/978-3-030-25590-9_6},
  urldate = {2024-10-23},
  isbn = {978-3-030-25589-3 978-3-030-25590-9},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/XTFMTBU9/Kapidis et al. - 2020 - Object Detection-Based Location and Activity Class.pdf}
}

@online{kazakosLittleHelpMy2021,
  title = {With a {{Little Help}} from My {{Temporal Context}}: {{Multimodal Egocentric Action Recognition}}},
  shorttitle = {With a {{Little Help}} from My {{Temporal Context}}},
  author = {Kazakos, Evangelos and Huh, Jaesung and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  date = {2021},
  doi = {10.48550/ARXIV.2111.01024},
  url = {https://arxiv.org/abs/2111.01024},
  urldate = {2025-01-02},
  abstract = {In egocentric videos, actions occur in quick succession. We capitalise on the action's temporal context and propose a method that learns to attend to surrounding actions in order to improve recognition performance. To incorporate the temporal context, we propose a transformer-based multimodal model that ingests video and audio as input modalities, with an explicit language model providing action sequence context to enhance the predictions. We test our approach on EPIC-KITCHENS and EGTEA datasets reporting state-of-the-art performance. Our ablations showcase the advantage of utilising temporal context as well as incorporating audio input modality and language model to rescore predictions. Code and models at: https://github.com/ekazakos/MTCN.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Audio and Speech Processing (eess.AS),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Sound (cs.SD)}
}

@online{lavechinOpensourceVoiceType2020,
  title = {An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings},
  author = {Lavechin, Marvin and Bousbib, Ruben and Bredin, Hervé and Dupoux, Emmanuel and Cristia, Alejandrina},
  date = {2020},
  doi = {10.48550/ARXIV.2005.12656},
  url = {https://arxiv.org/abs/2005.12656},
  urldate = {2024-11-14},
  abstract = {Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages. Our model can be used as input for downstream tasks such as estimating the number of words produced by adult speakers, or the number of linguistic units produced by children. Our architecture combines SincNet filters with a stack of recurrent layers and outperforms by a large margin the state-of-the-art system, the Language ENvironment Analysis (LENA) that has been used in numerous child language studies.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Audio and Speech Processing (eess.AS),FOS: Electrical engineering electronic engineering information engineering,I.2.7}
}

@online{linBMNBoundaryMatchingNetwork2019,
  title = {{{BMN}}: {{Boundary-Matching Network}} for {{Temporal Action Proposal Generation}}},
  shorttitle = {{{BMN}}},
  author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
  date = {2019},
  doi = {10.48550/ARXIV.1907.09702},
  url = {https://arxiv.org/abs/1907.09702},
  urldate = {2024-11-13},
  abstract = {Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{longBabyViewDatasetHighresolution2024,
  title = {The {{BabyView}} Dataset: {{High-resolution}} Egocentric Videos of Infants' and Young Children's Everyday Experiences},
  shorttitle = {The {{BabyView}} Dataset},
  author = {Long, Bria and Xiang, Violet and Stojanov, Stefan and Sparks, Robert Z. and Yin, Zi and Keene, Grace E. and Tan, Alvin W. M. and Feng, Steven Y. and Zhuang, Chengxu and Marchman, Virginia A. and Yamins, Daniel L. K. and Frank, Michael C.},
  date = {2024-06-14},
  eprint = {2406.10447},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.10447},
  url = {http://arxiv.org/abs/2406.10447},
  urldate = {2024-12-20},
  abstract = {Human children far exceed modern machine learning algorithms in their sample efficiency, achieving high performance in key domains with much less data than current models. This ''data gap'' is a key challenge both for building intelligent artificial systems and for understanding human development. Egocentric video capturing children's experience -- their ''training data'' -- is a key ingredient for comparison of humans and models and for the development of algorithmic innovations to bridge this gap. Yet there are few such datasets available, and extant data are low-resolution, have limited metadata, and importantly, represent only a small set of children's experiences. Here, we provide the first release of the largest developmental egocentric video dataset to date -- the BabyView dataset -- recorded using a high-resolution camera with a large vertical field-of-view and gyroscope/accelerometer data. This 493 hour dataset includes egocentric videos from children spanning 6 months - 5 years of age in both longitudinal, at-home contexts and in a preschool environment. We provide gold-standard annotations for the evaluation of speech transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains. We train self-supervised language and vision models and evaluate their transfer to out-of-distribution tasks including syntactic structure learning, object recognition, depth estimation, and image segmentation. Although performance in each scales with dataset size, overall performance is relatively lower than when models are trained on curated datasets, especially in the visual domain. Our dataset stands as an open challenge for robust, humanlike AI systems: how can such systems achieve human-levels of success on the same scale and distribution of training data as humans?},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/RGPHCL8P/Long et al. - 2024 - The BabyView dataset High-resolution egocentric v.pdf;/Users/nelesuffo/Zotero/storage/RN3TYVJP/2406.html}
}

@online{maNymeriaMassiveCollection2024,
  title = {Nymeria: {{A Massive Collection}} of {{Multimodal Egocentric Daily Motion}} in the {{Wild}}},
  shorttitle = {Nymeria},
  author = {Ma, Lingni and Ye, Yuting and Hong, Fangzhou and Guzov, Vladimir and Jiang, Yifeng and Postyeni, Rowan and Pesqueira, Luis and Gamino, Alexander and Baiyya, Vijay and Kim, Hyo Jin and Bailey, Kevin and Fosas, David Soriano and Liu, C. Karen and Liu, Ziwei and Engel, Jakob and De Nardi, Renzo and Newcombe, Richard},
  date = {2024},
  doi = {10.48550/ARXIV.2406.09905},
  url = {https://arxiv.org/abs/2406.09905},
  urldate = {2025-01-02},
  abstract = {We introduce Nymeria - a large-scale, diverse, richly annotated human motion dataset collected in the wild with multiple multimodal egocentric devices. The dataset comes with a) full-body ground-truth motion; b) multiple multimodal egocentric data from Project Aria devices with videos, eye tracking, IMUs and etc; and c) a third-person perspective by an additional observer. All devices are precisely synchronized and localized in on metric 3D world. We derive hierarchical protocol to add in-context language descriptions of human motion, from fine-grain motion narration, to simplified atomic action and high-level activity summarization. To the best of our knowledge, Nymeria dataset is the world's largest collection of human motion in the wild; first of its kind to provide synchronized and localized multi-device multimodal egocentric data; and the world's largest motion-language dataset. It provides 300 hours of daily activities from 264 participants across 50 locations, total travelling distance over 399Km. The language descriptions contain 301.5K sentences in 8.64M words from a vocabulary size of 6545. To demonstrate the potential of the dataset, we evaluate several SOTA algorithms for egocentric body tracking, motion synthesis, and action recognition. Data and code are open-sourced for research (c.f. https://www.projectaria.com/datasets/nymeria).},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Graphics (cs.GR)}
}

@software{mmaction2contributorsOpenMMLabsNextGeneration2020,
  title = {{{OpenMMLab}}'s {{Next Generation Video Understanding Toolbox}} and {{Benchmark}}},
  author = {MMAction2 Contributors},
  date = {2020},
  url = {urlhttps://github.com/open-mmlab/mmaction2}
}

@article{nunez-marcosEgocentricVisionbasedAction2022,
  title = {Egocentric {{Vision-based Action Recognition}}: {{A}} Survey},
  shorttitle = {Egocentric {{Vision-based Action Recognition}}},
  author = {Núñez-Marcos, Adrián and Azkune, Gorka and Arganda-Carreras, Ignacio},
  date = {2022-02},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {472},
  pages = {175--197},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.11.081},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221017586},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/I5LRFE37/Núñez-Marcos et al. - 2022 - Egocentric Vision-based Action Recognition A surv.pdf}
}

@article{piagetPartCognitiveDevelopment1964,
  title = {Part {{I}}: {{Cognitive}} Development in Children: {{Piaget}} Development and Learning},
  shorttitle = {Part {{I}}},
  author = {Piaget, Jean},
  date = {1964-09},
  journaltitle = {Journal of Research in Science Teaching},
  shortjournal = {J Res Sci Teach},
  volume = {2},
  number = {3},
  pages = {176--186},
  issn = {0022-4308, 1098-2736},
  doi = {10.1002/tea.3660020306},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/tea.3660020306},
  urldate = {2025-01-11},
  langid = {english}
}

@online{redmonYouOnlyLook2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2015},
  doi = {10.48550/ARXIV.1506.02640},
  url = {https://arxiv.org/abs/1506.02640},
  urldate = {2025-01-11},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  pubstate = {prepublished},
  version = {5},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@inproceedings{rehgDecodingChildrensSocial2013,
  title = {Decoding {{Children}}'s {{Social Behavior}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rehg, James M. and Abowd, Gregory D. and Rozga, Agata and Romero, Mario and Clements, Mark A. and Sclaroff, Stan and Essa, Irfan and Ousley, Opal Y. and Li, Yin and Kim, Chanho and Rao, Hrishikesh and Kim, Jonathan C. and Presti, Liliana Lo and Zhang, Jianming and Lantsman, Denis and Bidwell, Jonathan and Ye, Zhefan},
  date = {2013-06},
  pages = {3414--3421},
  publisher = {IEEE},
  location = {Portland, OR, USA},
  doi = {10.1109/CVPR.2013.438},
  url = {http://ieeexplore.ieee.org/document/6619282/},
  urldate = {2025-01-12},
  eventtitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-0-7695-4989-7},
  file = {/Users/nelesuffo/Zotero/storage/WANJ69M6/Rehg et al. - 2013 - Decoding Children's Social Behavior.pdf}
}

@article{rogoffImportanceUnderstandingChildrens2018,
  title = {The Importance of Understanding Children’s Lived Experience},
  author = {Rogoff, Barbara and Dahl, Audun and Callanan, Maureen},
  date = {2018-12},
  journaltitle = {Developmental Review},
  shortjournal = {Developmental Review},
  volume = {50},
  pages = {5--15},
  issn = {02732297},
  doi = {10.1016/j.dr.2018.05.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0273229718300236},
  urldate = {2024-12-23},
  langid = {english}
}

@article{roweDifferencesEarlyGesture2009,
  title = {Differences in {{Early Gesture Explain SES Disparities}} in {{Child Vocabulary Size}} at {{School Entry}}},
  author = {Rowe, Meredith L. and Goldin-Meadow, Susan},
  date = {2009-02-13},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {323},
  number = {5916},
  pages = {951--953},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1167025},
  url = {https://www.science.org/doi/10.1126/science.1167025},
  urldate = {2025-01-11},
  abstract = {Children from low–socioeconomic status (SES) families, on average, arrive at school with smaller vocabularies than children from high-SES families. In an effort to identify precursors to, and possible remedies for, this inequality, we videotaped 50 children from families with a range of different SES interacting with parents at 14 months and assessed their vocabulary skills at 54 months. We found that children from high-SES families frequently used gesture to communicate at 14 months, a relation that was explained by parent gesture use (with speech controlled). In turn, the fact that children from high-SES families have large vocabularies at 54 months was explained by children's gesture use at 14 months. Thus, differences in early gesture help to explain the disparities in vocabulary that children bring with them to school.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/YQ9VHZTY/Rowe and Goldin-Meadow - 2009 - Differences in Early Gesture Explain SES Dispariti.pdf}
}

@article{royPredictingBirthSpoken2015,
  title = {Predicting the Birth of a Spoken Word},
  author = {Roy, Brandon C. and Frank, Michael C. and DeCamp, Philip and Miller, Matthew and Roy, Deb},
  date = {2015-10-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {112},
  number = {41},
  pages = {12663--12668},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1419773112},
  url = {https://pnas.org/doi/full/10.1073/pnas.1419773112},
  urldate = {2024-12-20},
  abstract = {Significance             The emergence of productive language is a critical milestone in a child’s life. Laboratory studies have identified many individual factors that contribute to word learning, and larger scale studies show correlations between aspects of the home environment and language outcomes. To date, no study has compared across many factors involved in word learning. We introduce a new ultradense set of recordings that capture a single child’s daily experience during the emergence of language. We show that words used in distinctive spatial, temporal, and linguistic contexts are produced earlier, suggesting they are easier to learn. These findings support the importance of multimodal context in word learning for one child and provide new methods for quantifying the quality of children’s language input.           ,              Children learn words through an accumulation of interactions grounded in context. Although many factors in the learning environment have been shown to contribute to word learning in individual studies, no empirical synthesis connects across factors. We introduce a new ultradense corpus of audio and video recordings of a single child’s life that allows us to measure the child’s experience of each word in his vocabulary. This corpus provides the first direct comparison, to our knowledge, between different predictors of the child’s production of individual words. We develop a series of new measures of the distinctiveness of the spatial, temporal, and linguistic contexts in which a word appears, and show that these measures are stronger predictors of learning than frequency of use and that, unlike frequency, they play a consistent role across different syntactic categories. Our findings provide a concrete instantiation of classic ideas about the role of coherent activities in word learning and demonstrate the value of multimodal data in understanding children’s language acquisition.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/NKMNP2P4/Roy et al. - 2015 - Predicting the birth of a spoken word.pdf}
}

@article{ruffmanExposureBehavioralRegularities2023,
  title = {Exposure to Behavioral Regularities in Everyday Life Predicts Infants’ Acquisition of Mental State Vocabulary},
  author = {Ruffman, Ted and Chen, Lisa and Lorimer, Ben and Vanier, Sarah and Edgar, Kate and Scarf, Damian and Taumoepeau, Mele},
  date = {2023-07},
  journaltitle = {Developmental Science},
  shortjournal = {Developmental Science},
  volume = {26},
  number = {4},
  pages = {e13343},
  issn = {1363-755X, 1467-7687},
  doi = {10.1111/desc.13343},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/desc.13343},
  urldate = {2025-01-11},
  abstract = {Abstract                                           There are two broad views of children's theory of mind. The mentalist view is that it emerges in infancy and is possibly innate. The minimalist view is that it emerges more gradually in childhood and is heavily dependent on learning. According to minimalism, children initially understand behaviors rather than mental states, and they are assisted in doing so by recognizing repeating patterns in behavior. The regularities in behavior allow them to predict future behaviors, succeed on theory‐of‐mind tasks, acquire mental state words, and eventually, understand the mental states underlying behavior. The present study provided the first clear evidence for the plausibility of this view by fitting head cameras to 54 infants aged 6 to 25 months, and recording their view of the world in their daily lives. At 6 and 12 months, infants viewed an average of 146.5 repeated behaviors per hour, a rate consistent with approximately 560,000 repetitions in their first year, and with repetitions correlating with children's acquisition of mental state words, even after controlling for their general vocabulary and a range of variables indexing social interaction. We also recorded infants’ view of people searching or searching for and retrieving objects. These were 92 times less common and did not correlate with mental state vocabulary. Overall, the findings indicate that repeated behaviors provide a rich source of information for children that would readily allow them to recognize patterns in behavior and help them acquire mental state words, providing the first clear evidence for this claim of minimalism.                                         Research Highlights                                                                        Six‐ to 25‐month‐olds wore head cameras to record home life from infants’ point‐of‐view and help adjudicate between nativist and minimalist views of theory‐of‐mind (ToM).                                                           Nativists say ToM is too early developing to enable learning, whereas minimalists say infants learn to predict behaviors from behavior patterns in environment.                                                           Consistent with minimalism, infants had an incredibly rich exposure (146.5/h, {$>$}560,000 in first year) to repeated behaviors (e.g., drinking from a cup repeatedly).                                                           Consistent with minimalism, more repeated behaviors correlated with infants’ mental state vocabulary, even after controlling for gender, age, searches witnessed and non‐mental state vocabulary.},
  langid = {english}
}

@online{russakovskyImageNetLargeScale2014,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2014},
  doi = {10.48550/ARXIV.1409.0575},
  url = {https://arxiv.org/abs/1409.0575},
  urldate = {2025-01-12},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,I.4.8; I.5.2}
}

@article{saberCurriculumLearningInfant2023,
  title = {Curriculum Learning with Infant Egocentric Videos},
  author = {Saber, Sheybani and Hansaria,, Himanshu and Wood, Justin N. and Smith, Linda B. and Tiganj, Zoran},
  date = {2023},
  journaltitle = {NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages = {54199--54212},
  abstract = {Infants possess a remarkable ability to rapidly learn and process visual inputs. As an infant's mobility increases, so does the variety and dynamics of their visual inputs. Is this change in the properties of the visual inputs beneficial or even critical for the proper development of the visual system? To address this question, we used video recordings from infants wearing head-mounted cameras to train a variety of self-supervised learning models. Critically, we separated the infant data by age group and evaluated the importance of training with a curriculum aligned with developmental order. We found that initiating learning with the data from the youngest age group provided the strongest learning signal and led to the best learning outcomes in terms of downstream task performance. We then showed that the benefits of the data from the youngest age group are due to the slowness and simplicity of the visual experience. The results provide strong empirical evidence for the importance of the properties of the early infant experience and developmental progression in training. More broadly, our approach and findings take a noteworthy step towards reverse engineering the learning mechanisms in newborn brains using image-computable models from artificial intelligence.}
}

@online{simonyanTwoStreamConvolutionalNetworks2014,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  doi = {10.48550/ARXIV.1406.2199},
  url = {https://arxiv.org/abs/1406.2199},
  urldate = {2024-11-14},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{smithContributionsHeadMountedCameras2015,
  title = {Contributions of {{Head-Mounted Cameras}} to {{Studying}} the {{Visual Environments}} of {{Infants}} and {{Young Children}}},
  author = {Smith, Linda B. and Yu, Chen and Yoshida, Hanako and Fausey, Caitlin M.},
  date = {2015-05-27},
  journaltitle = {Journal of Cognition and Development},
  shortjournal = {Journal of Cognition and Development},
  volume = {16},
  number = {3},
  pages = {407--419},
  issn = {1524-8372, 1532-7647},
  doi = {10.1080/15248372.2014.933430},
  url = {http://www.tandfonline.com/doi/full/10.1080/15248372.2014.933430},
  urldate = {2024-12-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/44QZX3Y5/Smith et al. - 2015 - Contributions of Head-Mounted Cameras to Studying .pdf}
}

@article{smithDevelopingInfantCreates2018,
  title = {The {{Developing Infant Creates}} a {{Curriculum}} for {{Statistical Learning}}},
  author = {Smith, Linda B. and Jayaraman, Swapnaa and Clerkin, Elizabeth and Yu, Chen},
  date = {2018-04},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {4},
  pages = {325--336},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.02.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661318300275},
  urldate = {2025-01-11},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/2MNPACN4/Smith et al. - 2018 - The Developing Infant Creates a Curriculum for Sta.pdf}
}

@article{spanglerToddlersEverydayExperiences1989,
  title = {Toddlers' {{Everyday Experiences}} as {{Related}} to {{Preceding Mental}} and {{Emotional Disposition}} and {{Their Relationship}} to {{Subsequent Mental}} and {{Motivational Development}}: {{A Short-Term Longitudinal Study}}},
  shorttitle = {Toddlers' {{Everyday Experiences}} as {{Related}} to {{Preceding Mental}} and {{Emotional Disposition}} and {{Their Relationship}} to {{Subsequent Mental}} and {{Motivational Development}}},
  author = {Spangler, Gottfried},
  date = {1989-09},
  journaltitle = {International Journal of Behavioral Development},
  shortjournal = {International Journal of Behavioral Development},
  volume = {12},
  number = {3},
  pages = {285--303},
  issn = {0165-0254, 1464-0651},
  doi = {10.1177/016502548901200301},
  url = {https://journals.sagepub.com/doi/10.1177/016502548901200301},
  urldate = {2024-12-23},
  abstract = {The aims of the study were (1) to assess the relationship between children's everyday experiences and preceding mental and emotional disposition; (2) to describe the relationships between children's experiences and the children's mental and motivational development; (3) to derive hypotheses about the transactional connections between individual disposition, quality of experience, and mental and motivational development. Mental development and emotional disposition of 24 children were assessed at 12 months. During the second year type and quality of children's everyday experiences were repeatedly observed at home. At 24 months mental development was assessed again. In addition, children's motivation was observed during free play. The results reveal a transactional process for the mental and motivational development during the second year. Type and quality of children's everyday experiences were related to their individual mental and emotional disposition. In addition, level and quality of experiences were associated with subsequent mental and motivational development.},
  langid = {english}
}

@inproceedings{spelmenReviewHandlingImbalanced2018,
  title = {A {{Review}} on {{Handling Imbalanced Data}}},
  booktitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  author = {Spelmen, Vimalraj S and Porkodi, R},
  date = {2018-03},
  pages = {1--11},
  publisher = {IEEE},
  location = {Coimbatore},
  doi = {10.1109/ICCTCT.2018.8551020},
  url = {https://ieeexplore.ieee.org/document/8551020/},
  urldate = {2025-01-02},
  eventtitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  isbn = {978-1-5386-3702-9}
}

@article{sullivanSAYCamLargeLongitudinal2021,
  title = {{{SAYCam}}: {{A Large}}, {{Longitudinal Audiovisual Dataset Recorded From}} the {{Infant}}’s {{Perspective}}},
  shorttitle = {{{SAYCam}}},
  author = {Sullivan, Jessica and Mei, Michelle and Perfors, Andrew and Wojcik, Erica and Frank, Michael C.},
  date = {2021-05-26},
  journaltitle = {Open Mind},
  volume = {5},
  pages = {20--29},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00039},
  url = {https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset},
  urldate = {2024-12-20},
  abstract = {Abstract             We introduce a new resource: the SAYCam corpus. Infants aged 6–32 months wore a head-mounted camera for approximately 2 hr per week, over the course of approximately two-and-a-half years. The result is a large, naturalistic, longitudinal dataset of infant- and child-perspective videos. Over 200,000 words of naturalistic speech have already been transcribed. Similarly, the dataset is searchable using a number of criteria (e.g., age of participant, location, setting, objects present). The resulting dataset will be of broad use to psychologists, linguists, and computer scientists.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/A5ANFVQR/Sullivan et al. - 2021 - SAYCam A Large, Longitudinal Audiovisual Dataset .pdf}
}

@book{tomaselloCulturalOriginsHuman2009,
  title = {Cultural {{Origins}} of {{Human Cognition}}},
  author = {Tomasello, Michael},
  date = {2009},
  publisher = {Harvard University Press},
  location = {Cambridge},
  isbn = {978-0-674-00582-2},
  langid = {english},
  pagetotal = {256}
}

@article{truongCrossviewActionRecognition2024,
  title = {Cross-View Action Recognition Understanding from Exocentric to Egocentric Perspective},
  author = {Truong, Thanh-Dat and Luu, Khoa},
  date = {2024-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  pages = {128731},
  issn = {09252312},
  doi = {10.1016/j.neucom.2024.128731},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231224015029},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/95IM2Q74/Truong and Luu - 2024 - Cross-view action recognition understanding from e.pdf}
}

@online{tsutsuiComputationalModelEarly2020,
  title = {A {{Computational Model}} of {{Early Word Learning}} from the {{Infant}}'s {{Point}} of {{View}}},
  author = {Tsutsui, Satoshi and Chandrasekaran, Arjun and Reza, Md Alimoor and Crandall, David and Yu, Chen},
  date = {2020-06-04},
  eprint = {2006.02802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.02802},
  url = {http://arxiv.org/abs/2006.02802},
  urldate = {2024-12-20},
  abstract = {Human infants have the remarkable ability to learn the associations between object names and visual objects from inherently ambiguous experiences. Researchers in cognitive science and developmental psychology have built formal models that implement in-principle learning algorithms, and then used pre-selected and pre-cleaned datasets to test the abilities of the models to find statistical regularities in the input data. In contrast to previous modeling approaches, the present study used egocentric video and gaze data collected from infant learners during natural toy play with their parents. This allowed us to capture the learning environment from the perspective of the learner's own point of view. We then used a Convolutional Neural Network (CNN) model to process sensory data from the infant's point of view and learn name-object associations from scratch. As the first model that takes raw egocentric video to simulate infant word learning, the present study provides a proof of principle that the problem of early word learning can be solved, using actual visual data perceived by infant learners. Moreover, we conducted simulation experiments to systematically determine how visual, perceptual, and attentional properties of infants' sensory experiences may affect word learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/RQHSGKUR/Tsutsui et al. - 2020 - A Computational Model of Early Word Learning from .pdf;/Users/nelesuffo/Zotero/storage/MQUAE3QR/2006.html}
}

@book{vygotskyMindSocietyDevelopment1978,
  title = {Mind in {{Society}}: {{The Development}} of {{Higher Psychological Processes}}},
  author = {Vygotsky, Lev Semyonovich},
  date = {1978},
  publisher = {Harvard University Press},
  location = {Cambridge, MA}
}

@online{wangFasterPersonReIdentification2020,
  title = {Faster {{Person Re-Identification}}},
  author = {Wang, Guan'an and Gong, Shaogang and Cheng, Jian and Hou, Zengguang},
  date = {2020},
  doi = {10.48550/ARXIV.2008.06826},
  url = {https://arxiv.org/abs/2008.06826},
  urldate = {2024-10-23},
  abstract = {Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a \$F\_β\$ score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8\% more accurate but also 5x faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is \$50\textbackslash times\$ faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/WG9M6GYH/Wang et al. - 2020 - Faster Person Re-Identification.pdf}
}

@online{wangTemporalSegmentNetworks2017,
  title = {Temporal {{Segment Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  date = {2017},
  doi = {10.48550/ARXIV.1705.02953},
  url = {https://arxiv.org/abs/1705.02953},
  urldate = {2024-11-14},
  abstract = {Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0\%), UCF101 (94.9\%), THUMOS14 (80.1\%), and ActivityNet v1.2 (89.6\%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0\%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{xiongCUHKETHZSIAT2016,
  title = {{{CUHK}} \& {{ETHZ}} \& {{SIAT Submission}} to {{ActivityNet Challenge}} 2016},
  author = {Xiong, Yuanjun and Wang, Limin and Wang, Zhe and Zhang, Bowen and Song, Hang and Li, Wei and Lin, Dahua and Qiao, Yu and Van Gool, Luc and Tang, Xiaoou},
  date = {2016},
  doi = {10.48550/ARXIV.1608.00797},
  url = {https://arxiv.org/abs/1608.00797},
  urldate = {2025-01-11},
  abstract = {This paper presents the method that underlies our submission to the untrimmed video classification task of ActivityNet Challenge 2016. We follow the basic pipeline of temporal segment networks and further raise the performance via a number of other techniques. Specifically, we use the latest deep model architecture, e.g., ResNet and Inception V3, and introduce new aggregation schemes (top-k and attention-weighted pooling). Additionally, we incorporate the audio as a complementary channel, extracting relevant information via a CNN applied to the spectrograms. With these techniques, we derive an ensemble of deep models, which, together, attains a high classification accuracy (mAP \$93.23\textbackslash\%\$) on the testing set and secured the first place in the challenge.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{xuContinualEgocentricActivity2023,
  title = {Towards {{Continual Egocentric Activity Recognition}}: {{A Multi-modal Egocentric Activity Dataset}} for {{Continual Learning}}},
  shorttitle = {Towards {{Continual Egocentric Activity Recognition}}},
  author = {Xu, Linfeng and Wu, Qingbo and Pan, Lili and Meng, Fanman and Li, Hongliang and He, Chiyuan and Wang, Hanxin and Cheng, Shaoxu and Dai, Yu},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.10931},
  url = {https://arxiv.org/abs/2301.10931},
  urldate = {2025-01-02},
  abstract = {With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{yoshidaWhatsViewToddlers2008,
  title = {What's in {{View}} for {{Toddlers}}? {{Using}} a {{Head Camera}} to {{Study Visual Experience}}},
  shorttitle = {What's in {{View}} for {{Toddlers}}?},
  author = {Yoshida, Hanako and Smith, Linda B.},
  date = {2008-05-06},
  journaltitle = {Infancy},
  shortjournal = {Infancy},
  volume = {13},
  number = {3},
  pages = {229--248},
  issn = {1525-0008, 1532-7078},
  doi = {10.1080/15250000802004437},
  url = {https://onlinelibrary.wiley.com/doi/10.1080/15250000802004437},
  urldate = {2024-12-23},
  abstract = {This article reports 2 experiments using a new method to study 18‐to 24‐month‐olds' visual experiences as they interact with objects. Experiment 1 presents evidence on the coupling of head and eye movements and thus the validity of the head camera view of the infant's visual field in the geometry of the task context. Experiment 2 demonstrates the use of this method in the naturalistic context of toy play with a parent. The results point to the embodied nature of toddlers' attentional strategies and to importance of hands and hand actions in their visual experience of objects. The head camera thus appears to be a promising method that, despite some limitations, will yield new insights about the ecology and content of young children's experiences.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/ZVPCJWU6/Yoshida and Smith - 2008 - What's in View for Toddlers Using a Head Camera t.pdf}
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2024},
  url = {https://www.R-project.org/},
}
@Manual{R-BayesFactor,
  title = {BayesFactor: Computation of Bayes Factors for Common Designs},
  author = {Richard D. Morey and Jeffrey N. Rouder},
  year = {2024},
  note = {R package version 0.9.12-4.7},
  url = {https://CRAN.R-project.org/package=BayesFactor},
}
@Article{R-brms_a,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}
@Article{R-brms_b,
  title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
  author = {Paul-Christian Bürkner},
  journal = {The R Journal},
  year = {2018},
  volume = {10},
  number = {1},
  pages = {395--411},
  doi = {10.32614/RJ-2018-017},
  encoding = {UTF-8},
}
@Article{R-brms_c,
  title = {Bayesian Item Response Modeling in {R} with {brms} and {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2021},
  volume = {100},
  number = {5},
  pages = {1--54},
  doi = {10.18637/jss.v100.i05},
  encoding = {UTF-8},
}
@Manual{R-broom,
  title = {broom: Convert Statistical Objects into Tidy Tibbles},
  author = {David Robinson and Alex Hayes and Simon Couch},
  year = {2024},
  note = {R package version 1.0.7},
  url = {https://CRAN.R-project.org/package=broom},
}
@Article{R-coda,
  title = {CODA: Convergence Diagnosis and Output Analysis for MCMC},
  author = {Martyn Plummer and Nicky Best and Kate Cowles and Karen Vines},
  journal = {R News},
  year = {2006},
  volume = {6},
  number = {1},
  pages = {7--11},
  url = {https://journal.r-project.org/archive/},
  pdf = {https://www.r-project.org/doc/Rnews/Rnews_2006-1.pdf},
}
@Manual{R-cowplot,
  title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 1.1.3},
  url = {https://CRAN.R-project.org/package=cowplot},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
  year = {2023},
  note = {R package version 1.1.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-ggplotify,
  title = {ggplotify: Convert Plot to 'grob' or 'ggplot' Object},
  author = {Guangchuang Yu},
  year = {2023},
  note = {R package version 0.1.2},
  url = {https://CRAN.R-project.org/package=ggplotify},
}
@Manual{R-ggpubr,
  title = {ggpubr: 'ggplot2' Based Publication Ready Plots},
  author = {Alboukadel Kassambara},
  year = {2023},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggpubr},
}
@Manual{R-ggridges,
  title = {ggridges: Ridgeline Plots in 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 0.5.6},
  url = {https://CRAN.R-project.org/package=ggridges},
}
@Manual{R-ggthemes,
  title = {ggthemes: Extra Themes, Scales and Geoms for 'ggplot2'},
  author = {Jeffrey B. Arnold},
  year = {2024},
  note = {R package version 5.1.0},
  url = {https://CRAN.R-project.org/package=ggthemes},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Manual{R-kableExtra,
  title = {kableExtra: Construct Complex Table with 'kable' and Pipe Syntax},
  author = {Hao Zhu},
  year = {2024},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=kableExtra},
}
@Article{R-lubridate,
  title = {Dates and Times Made Easy with {lubridate}},
  author = {Garrett Grolemund and Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {3},
  pages = {1--25},
  url = {https://www.jstatsoft.org/v40/i03/},
}
@Manual{R-magick,
  title = {magick: Advanced Graphics and Image-Processing in R},
  author = {Jeroen Ooms},
  year = {2024},
  note = {R package version 2.8.5},
  url = {https://CRAN.R-project.org/package=magick},
}
@Manual{R-Matrix,
  title = {Matrix: Sparse and Dense Matrix Classes and Methods},
  author = {Douglas Bates and Martin Maechler and Mikael Jagan},
  year = {2024},
  note = {R package version 1.7-1},
  url = {https://CRAN.R-project.org/package=Matrix},
}
@Manual{R-papaja,
  title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
  author = {Frederik Aust and Marius Barth},
  year = {2024},
  note = {R package version 0.1.3},
  url = {https://github.com/crsh/papaja},
  doi = {10.32614/CRAN.package.papaja},
}
@Manual{R-patchwork,
  title = {patchwork: The Composer of Plots},
  author = {Thomas Lin Pedersen},
  year = {2024},
  note = {R package version 1.3.0},
  url = {https://CRAN.R-project.org/package=patchwork},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Hadley Wickham and Lionel Henry},
  year = {2023},
  note = {R package version 1.0.2},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Article{R-Rcpp_a,
  title = {{Rcpp}: Seamless {R} and {C++} Integration},
  author = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {8},
  pages = {1--18},
  doi = {10.18637/jss.v040.i08},
}
@Article{R-Rcpp_b,
  title = {{Extending {R} with {C++}: A Brief Introduction to {Rcpp}}},
  author = {Dirk Eddelbuettel and James Joseph Balamuta},
  journal = {The American Statistician},
  year = {2018},
  volume = {72},
  number = {1},
  pages = {28-36},
  doi = {10.1080/00031305.2017.1375990},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Jennifer Bryan},
  year = {2024},
  note = {R package version 2.1.5},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-readxl,
  title = {readxl: Read Excel Files},
  author = {Hadley Wickham and Jennifer Bryan},
  year = {2023},
  note = {R package version 1.4.3},
  url = {https://CRAN.R-project.org/package=readxl},
}
@Article{R-reshape2,
  title = {Reshaping Data with the {reshape} Package},
  author = {Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2007},
  volume = {21},
  number = {12},
  pages = {1--20},
  url = {http://www.jstatsoft.org/v21/i12/},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.5.1},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2023},
  note = {R package version 3.2.1},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Davis Vaughan and Maximilian Girlich},
  year = {2024},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Article{R-tidyverse,
  title = {Welcome to the {tidyverse}},
  author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686},
}
@Manual{R-tinylabels,
  title = {{tinylabels}: Lightweight Variable Labels},
  author = {Marius Barth},
  year = {2023},
  note = {R package version 0.2.4},
  url = {https://cran.r-project.org/package=tinylabels},
}
@Article{R-zoo,
  title = {zoo: S3 Infrastructure for Regular and Irregular Time Series},
  author = {Achim Zeileis and Gabor Grothendieck},
  journal = {Journal of Statistical Software},
  year = {2005},
  volume = {14},
  number = {6},
  pages = {1--27},
  doi = {10.18637/jss.v014.i06},
}
