@article{aghaeiWhomInteractDetecting2016,
  title = {With {{Whom Do I Interact}}? {{Detecting Social Interactions}} in {{Egocentric Photo-streams}}},
  shorttitle = {With {{Whom Do I Interact}}?},
  author = {Aghaei, Maedeh and Dimiccoli, Mariella and Radeva, Petia},
  date = {2016},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1605.04129},
  url = {https://arxiv.org/abs/1605.04129},
  urldate = {2024-05-16},
  abstract = {Given a user wearing a low frame rate wearable camera during a day, this work aims to automatically detect the moments when the user gets engaged into a social interaction solely by reviewing the automatically captured photos by the worn camera. The proposed method, inspired by the sociological concept of F-formation, exploits distance and orientation of the appearing individuals -with respect to the user- in the scene from a bird-view perspective. As a result, the interaction pattern over the sequence can be understood as a two-dimensional time series that corresponds to the temporal evolution of the distance and orientation features over time. A Long-Short Term Memory-based Recurrent Neural Network is then trained to classify each time series. Experimental evaluation over a dataset of 30.000 images has shown promising results on the proposed method for social interaction detection in egocentric photo-streams.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/C9FVGBHP/Aghaei et al. - 2016 - With Whom Do I Interact Detecting Social Interact.pdf;/Users/nelesuffo/Zotero/storage/U5S9R9CW/1605.04129v2.pdf}
}

@online{akcakirExploringInterplayIndividual2024,
  title = {Exploring the Interplay of Individual Traits and Interaction Dynamics in Preschool Social Networks},
  author = {Akçakır, Gülşah and Azaiez, Amina and Ceria, Alberto and Eminente, Clara and Ferranti, Guglielmo and Gandhi, Govind and Raj, Aishvarya and Iacopini, Iacopo},
  date = {2024},
  doi = {10.48550/ARXIV.2407.12728},
  url = {https://arxiv.org/abs/2407.12728},
  urldate = {2025-02-18},
  abstract = {Several studies have investigated human interaction using modern tracking techniques for face-to-face encounters across various settings and age groups. However, little attention has been given to understanding how individual characteristics relate to social behavior. This is particularly important in younger age groups due to its potential effects on early childhood development. In this study, conducted during the Complexity 72h Workshop, we analyze human social interactions in a French preschool, where children's face-to-face interactions were monitored using proximity sensors over an academic year. We use metadata from parent surveys and preschool linguistic tests, covering demographic information and home habits, to examine the interplay between individual characteristics and contact patterns. Using a mixture of approaches, from random forest classifiers to network-based metrics at both dyadic and higher-order (group) levels, we identify sex, age, language scores, and number of siblings as the variables displaying the most significant associations with interaction patterns. We explore these variables' relationships to interactions within and outside classrooms and across mixed and single-grade classes. At the group level, we investigate how group affinity affects group persistence. We also find that higher-order network centrality (hypercoreness) is higher among children with siblings, indicating different group embedding despite similar total contact duration. This study aligns with existing literature on early social development and highlights the importance of integrating individual traits into the study of human interactions. Focusing on 2-5-year-olds offers insights into emerging social preferences during critical phases of cognitive development. Future research could use these findings to enhance mechanistic models of complex social systems by incorporating individual traits.},
  pubstate = {prepublished},
  version = {1},
  keywords = {FOS: Computer and information sciences,FOS: Physical sciences,Physics and Society (physics.soc-ph),Social and Information Networks (cs.SI)}
}

@inproceedings{alaniClassifyingImbalancedMultimodal2020,
  title = {Classifying {{Imbalanced Multi-modal Sensor Data}} for {{Human Activity Recognition}} in a {{Smart Home}} Using {{Deep Learning}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Alani, Ali A. and Cosma, Georgina and Taherkhani, Aboozar},
  date = {2020-07},
  pages = {1--8},
  publisher = {IEEE},
  location = {Glasgow, United Kingdom},
  doi = {10.1109/IJCNN48605.2020.9207697},
  url = {https://ieeexplore.ieee.org/document/9207697/},
  urldate = {2025-01-02},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-72816-926-2},
  file = {/Users/nelesuffo/Zotero/storage/LZ2V5GUC/Alani et al. - 2020 - Classifying Imbalanced Multi-modal Sensor Data for.pdf}
}

@article{arabaciMultimodalEgocentricActivity2021,
  title = {Multi-Modal Egocentric Activity Recognition Using Multi-Kernel Learning},
  author = {Arabacı, Mehmet Ali and Özkan, Fatih and Surer, Elif and Jančovič, Peter and Temizel, Alptekin},
  date = {2021-05},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {80},
  number = {11},
  pages = {16299--16328},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08789-7},
  url = {https://link.springer.com/10.1007/s11042-020-08789-7},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/P2YJ73YD/Arabacı et al. - 2021 - Multi-modal egocentric activity recognition using .pdf}
}

@online{baevskiWav2vec20Framework2020,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020},
  doi = {10.48550/ARXIV.2006.11477},
  url = {https://arxiv.org/abs/2006.11477},
  urldate = {2025-01-12},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Audio and Speech Processing (eess.AS),Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Machine Learning (cs.LG),Sound (cs.SD)}
}

@inproceedings{bambachLendingHandDetecting2015,
  title = {Lending {{A Hand}}: {{Detecting Hands}} and {{Recognizing Activities}} in {{Complex Egocentric Interactions}}},
  shorttitle = {Lending {{A Hand}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Bambach, Sven and Lee, Stefan and Crandall, David J. and Yu, Chen},
  date = {2015-12},
  pages = {1949--1957},
  publisher = {IEEE},
  location = {Santiago, Chile},
  doi = {10.1109/ICCV.2015.226},
  url = {http://ieeexplore.ieee.org/document/7410583/},
  urldate = {2025-01-02},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  file = {/Users/nelesuffo/Zotero/storage/NVTHGNDX/Bambach et al. - 2015 - Lending A Hand Detecting Hands and Recognizing Ac.pdf}
}

@article{bangAutomatedClassifierPeriods2023,
  title = {An Automated Classifier for Periods of Sleep and Target-Child-Directed Speech from {{LENA}} Recordings},
  author = {Bang, Janet Yougi and Kachergis, George and Weisleder, Adriana and Marchman, Virginia},
  date = {2023},
  journaltitle = {Language Development Research},
  volume = {3.0},
  number = {1.0},
  publisher = {Carnegie Mellon University Library Publishing Service},
  issn = {2771-7976},
  doi = {10.34842/XMRQ-ER43},
  url = {https://ldr.lps.library.cmu.edu/article/id/659/},
  urldate = {2025-05-21},
  abstract = {Some theories of language development propose that children learn more effectively when exposed to speech that is directed to them (target child directed speech, tCDS) than when exposed to speech that is directed to others (other-directed speech, ODS). During naturalistic daylong recordings, it is useful to identify periods of tCDS and ODS, as well as periods when the child is awake and able to make use of that speech. To do so, researchers typically rely on the laborious work of human listeners who consider numerous features when making judgments. In this paper, we detail our efforts to automate these pro-cesses. We analyzed over 1,000 hours of audio from daylong recordings of 153 English- and Spanish-speaking families in the U.S. with 17- to 28-month-old children that had been previously coded by hu-man listeners for periods of sleep, tCDS, and ODS. We first explored patterns of features that character-ized periods of sleep, tCDS, and ODS. Then, we evaluated two classifiers that were trained using auto-mated measures generated from LENATM, including frequency (AWC, CTC, CVC) and duration (mean-ingful speech, distant speech, TV, noise, silence) measures. Results revealed high sensitivity and speci-ficity in our sleep classifier, and moderate sensitivity and specificity in our tCDS/ODS classifier. Moreo-ver, model-derived predictions replicated previously-published findings showing significant and posi-tive links between tCDS, but not ODS, and children’s later vocabularies (Weisleder \& Fernald, 2013). This work offers promising tools for streamlining work with daylong recordings, facilitating research that aims to better understand how children learn from everyday speech environments. Keywords: child-directed speech, other-directed speech, LENA, daylong recordings, automated classifier},
  langid = {english}
}

@inproceedings{baroudiSpecializingSelfSupervisedSpeech2024,
  title = {Specializing {{Self-Supervised Speech Representations}} for {{Speaker Segmentation}}},
  booktitle = {Interspeech 2024},
  author = {Baroudi, Séverin and Pellegrini, Thomas and Bredin, Hervé},
  date = {2024-09-01},
  pages = {3769--3773},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2024-962},
  url = {https://www.isca-archive.org/interspeech_2024/baroudi24_interspeech.html},
  urldate = {2025-05-12},
  eventtitle = {Interspeech 2024},
  langid = {english}
}

@inproceedings{beelIntroducingMrDLib2011,
  title = {Introducing {{Mr}}. {{DLib}}, a {{Machine-readable Digital Library}}},
  booktitle = {Proceedings of the 11th Annual International {{ACM}}/{{IEEE}} Joint Conference on {{Digital}} Libraries},
  author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Genzmehr, Marcel and Wilde, Erik and Nürnberger, Andreas and Pitman, Jim},
  date = {2011-06-13},
  pages = {463--464},
  publisher = {ACM},
  location = {Ottawa Ontario Canada},
  doi = {10.1145/1998076.1998187},
  url = {https://dl.acm.org/doi/10.1145/1998076.1998187},
  urldate = {2025-02-23},
  eventtitle = {{{JCDL}} '11: {{Joint Conference}} on {{Digital Libraries}}},
  isbn = {978-1-4503-0744-4},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/IBT53RYK/Beel et al. - 2011 - Introducing Mr. DLib, a Machine-readable Digital L.pdf}
}

@article{bergelsonEverydayLanguageInput2023,
  title = {Everyday Language Input and Production in 1,001 Children from Six Continents},
  author = {Bergelson, Elika and Soderstrom, Melanie and Schwarz, Iris-Corinna and Rowland, Caroline F. and Ramírez-Esparza, Nairán and R. Hamrick, Lisa and Marklund, Ellen and Kalashnikova, Marina and Guez, Ava and Casillas, Marisa and Benetti, Lucia and Alphen, Petra Van and Cristia, Alejandrina},
  date = {2023-12-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {120},
  number = {52},
  pages = {e2300671120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2300671120},
  url = {https://pnas.org/doi/10.1073/pnas.2300671120},
  urldate = {2025-01-11},
  abstract = {Language is a universal human ability, acquired readily by young children, who otherwise struggle with many basics of survival. And yet, language ability is variable across individuals. Naturalistic and experimental observations suggest that children’s linguistic skills vary with factors like socioeconomic status and children’s gender. But which factors really influence children’s day-to-day language use? Here, we leverage speech technology in a big-data approach to report on a unique cross-cultural and diverse data set: {$>$}2,500 d-long, child-centered audio-recordings of 1,001 2- to 48-mo-olds from 12 countries spanning six continents across urban, farmer-forager, and subsistence-farming contexts. As expected, age and language-relevant clinical risks and diagnoses predicted how much speech (and speech-like vocalization) children produced. Critically, so too did adult talk in children’s environments: Children who heard more talk from adults produced more speech. In contrast to previous conclusions based on more limited sampling methods and a different set of language proxies, socioeconomic status (operationalized as maternal education) was not significantly associated with children’s productions over the first 4 y of life, and neither were gender or multilingualism. These findings from large-scale naturalistic data advance our understanding of which factors are robust predictors of variability in the speech behaviors of young learners in a wide range of everyday contexts.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/B4PYKNBG/Bergelson et al. - 2023 - Everyday language input and production in 1,001 ch.pdf}
}

@article{bergelsonEverydayLanguageInput2023a,
  title = {Everyday Language Input and Production in 1,001 Children from Six Continents},
  author = {Bergelson, Elika and Soderstrom, Melanie and Schwarz, Iris-Corinna and Rowland, Caroline F. and Ramírez-Esparza, Nairán and R. Hamrick, Lisa and Marklund, Ellen and Kalashnikova, Marina and Guez, Ava and Casillas, Marisa and Benetti, Lucia and Alphen, Petra Van and Cristia, Alejandrina},
  date = {2023-12-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {120},
  number = {52},
  pages = {e2300671120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2300671120},
  url = {https://pnas.org/doi/10.1073/pnas.2300671120},
  urldate = {2025-05-19},
  abstract = {Language is a universal human ability, acquired readily by young children, who otherwise struggle with many basics of survival. And yet, language ability is variable across individuals. Naturalistic and experimental observations suggest that children’s linguistic skills vary with factors like socioeconomic status and children’s gender. But which factors really influence children’s day-to-day language use? Here, we leverage speech technology in a big-data approach to report on a unique cross-cultural and diverse data set: {$>$}2,500 d-long, child-centered audio-recordings of 1,001 2- to 48-mo-olds from 12 countries spanning six continents across urban, farmer-forager, and subsistence-farming contexts. As expected, age and language-relevant clinical risks and diagnoses predicted how much speech (and speech-like vocalization) children produced. Critically, so too did adult talk in children’s environments: Children who heard more talk from adults produced more speech. In contrast to previous conclusions based on more limited sampling methods and a different set of language proxies, socioeconomic status (operationalized as maternal education) was not significantly associated with children’s productions over the first 4 y of life, and neither were gender or multilingualism. These findings from large-scale naturalistic data advance our understanding of which factors are robust predictors of variability in the speech behaviors of young learners in a wide range of everyday contexts.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/AIFK6UY7/Bergelson et al. - 2023 - Everyday language input and production in 1,001 ch.pdf}
}

@article{borjonViewTheirOwn2018,
  title = {A {{View}} of {{Their Own}}: {{Capturing}} the {{Egocentric View}} of {{Infants}} and {{Toddlers}} with {{Head-Mounted Cameras}}},
  shorttitle = {A {{View}} of {{Their Own}}},
  author = {Borjon, Jeremy I. and Schroer, Sara E. and Bambach, Sven and Slone, Lauren K. and Abney, Drew H. and Crandall, David J. and Smith, Linda B.},
  date = {2018-10-05},
  journaltitle = {Journal of Visualized Experiments},
  shortjournal = {JoVE},
  number = {140},
  pages = {58445},
  issn = {1940-087X},
  doi = {10.3791/58445-v},
  url = {https://app.jove.com/v/58445},
  urldate = {2024-12-20},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/JLAL6IZ2/Borjon et al. - 2018 - A View of Their Own Capturing the Egocentric View.pdf}
}

@online{bredinEndtoendSpeakerSegmentation2021,
  title = {End-to-End Speaker Segmentation for Overlap-Aware Resegmentation},
  author = {Bredin, Hervé and Laurent, Antoine},
  date = {2021},
  doi = {10.48550/ARXIV.2104.04045},
  url = {https://arxiv.org/abs/2104.04045},
  urldate = {2025-05-13},
  abstract = {Speaker segmentation consists in partitioning a conversation between one or more speakers into speaker turns. Usually addressed as the late combination of three sub-tasks (voice activity detection, speaker change detection, and overlapped speech detection), we propose to train an end-to-end segmentation model that does it directly. Inspired by the original end-to-end neural speaker diarization approach (EEND), the task is modeled as a multi-label classification problem using permutation-invariant training. The main difference is that our model operates on short audio chunks (5 seconds) but at a much higher temporal resolution (every 16ms). Experiments on multiple speaker diarization datasets conclude that our model can be used with great success on both voice activity detection and overlapped speech detection. Our proposed model can also be used as a post-processing step, to detect and correctly assign overlapped speech regions. Relative diarization error rate improvement over the best considered baseline (VBx) reaches 17\% on AMI, 13\% on DIHARD 3, and 13\% on VoxConverse.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Audio and Speech Processing (eess.AS),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Sound (cs.SD)}
}

@inproceedings{bredinPyannoteaudio21Speaker2023,
  title = {Pyannote.Audio 2.1 Speaker Diarization Pipeline: Principle, Benchmark, and Recipe},
  shorttitle = {Pyannote.Audio 2.1 Speaker Diarization Pipeline},
  booktitle = {{{INTERSPEECH}} 2023},
  author = {Bredin, Hervé},
  date = {2023-08-20},
  pages = {1983--1987},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2023-105},
  url = {https://www.isca-archive.org/interspeech_2023/bredin23_interspeech.html},
  urldate = {2025-05-12},
  eventtitle = {{{INTERSPEECH}} 2023},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/UPY5VFMX/Bredin - 2023 - pyannote.audio 2.1 speaker diarization pipeline p.pdf}
}

@software{brostromBoxMOTCollectionSOTA2023,
  title = {{{BoxMOT}}: {{A}} Collection of {{SOTA}} Real-Time, Multi-Object Trackers for Object Detectors},
  shorttitle = {{{BoxMOT}}},
  author = {Broström, Mikel},
  date = {2023-06-27},
  doi = {10.5281/ZENODO.7452873},
  url = {https://zenodo.org/record/7452873},
  urldate = {2024-10-23},
  abstract = {This repo contains a collections of state-of-the-art multi-object trackers. Supported ones at the moment are: DeepOCSORT , BoTSORT , StrongSORT, OCSORT and ByteTrack. DeepOCSORT, BoTSORT and StrongSORT are based on motion + appearance description; OCSORT and ByteTrack are based on motion only. For the methods using appearance description, lightweight state-of-the-art ReID models (LightMBN, OSNet and more) are downloaded automatically as well. We provide examples on how to use this package together with popular object detection models. Right now Yolov8, Yolo-NAS and YOLOX are available.},
  organization = {Zenodo},
  version = {10.0.15}
}

@online{caoOpenPoseRealtimeMultiPerson2018,
  title = {{{OpenPose}}: {{Realtime Multi-Person 2D Pose Estimation}} Using {{Part Affinity Fields}}},
  shorttitle = {{{OpenPose}}},
  author = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2018},
  doi = {10.48550/ARXIV.1812.08008},
  url = {https://arxiv.org/abs/1812.08008},
  urldate = {2025-01-11},
  abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@book{carpendaleWhatMakesUs2020,
  title = {What {{Makes Us Human}}: {{How Minds Develop}} through {{Social Interactions}}},
  shorttitle = {What {{Makes Us Human}}},
  author = {Carpendale, Jeremy and Lewis, Charlie},
  date = {2020-12-21},
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9781003125105},
  url = {https://www.taylorfrancis.com/books/9781000283983},
  urldate = {2025-01-11},
  isbn = {978-1-00-312510-5},
  langid = {english}
}

@online{carreiraQuoVadisAction2017,
  title = {Quo {{Vadis}}, {{Action Recognition}}? {{A New Model}} and the {{Kinetics Dataset}}},
  shorttitle = {Quo {{Vadis}}, {{Action Recognition}}?},
  author = {Carreira, Joao and Zisserman, Andrew},
  date = {2017},
  doi = {10.48550/ARXIV.1705.07750},
  url = {https://arxiv.org/abs/1705.07750},
  urldate = {2025-01-12},
  abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{chenDyadicAffectParentChild2023,
  title = {Dyadic {{Affect}} in {{Parent-Child Multimodal Interaction}}: {{Introducing}} the {{DAMI-P2C Dataset}} and Its {{Preliminary Analysis}}},
  shorttitle = {Dyadic {{Affect}} in {{Parent-Child Multimodal Interaction}}},
  author = {Chen, Huili and Alghowinem, Sharifa and Jang, Soo Jung and Breazeal, Cynthia and Park, Hae Won},
  date = {2023-10-01},
  journaltitle = {IEEE Transactions on Affective Computing},
  shortjournal = {IEEE Trans. Affective Comput.},
  volume = {14},
  number = {4},
  pages = {3345--3361},
  issn = {1949-3045, 2371-9850},
  doi = {10.1109/TAFFC.2022.3178689},
  url = {https://ieeexplore.ieee.org/document/9784429/},
  urldate = {2025-01-12},
  file = {/Users/nelesuffo/Zotero/storage/XZZVRTPE/Chen et al. - 2023 - Dyadic Affect in Parent-Child Multimodal Interacti.pdf}
}

@online{chengAppearancebasedGazeEstimation2021,
  title = {Appearance-Based {{Gaze Estimation With Deep Learning}}: {{A Review}} and {{Benchmark}}},
  shorttitle = {Appearance-Based {{Gaze Estimation With Deep Learning}}},
  author = {Cheng, Yihua and Wang, Haofei and Bao, Yiwei and Lu, Feng},
  date = {2021},
  doi = {10.48550/ARXIV.2104.12668},
  url = {https://arxiv.org/abs/2104.12668},
  urldate = {2025-03-04},
  abstract = {Human gaze provides valuable information on human focus and intentions, making it a crucial area of research. Recently, deep learning has revolutionized appearance-based gaze estimation. However, due to the unique features of gaze estimation research, such as the unfair comparison between 2D gaze positions and 3D gaze vectors and the different pre-processing and post-processing methods, there is a lack of a definitive guideline for developing deep learning-based gaze estimation algorithms. In this paper, we present a systematic review of the appearance-based gaze estimation methods using deep learning. Firstly, we survey the existing gaze estimation algorithms along the typical gaze estimation pipeline: deep feature extraction, deep learning model design, personal calibration and platforms. Secondly, to fairly compare the performance of different approaches, we summarize the data pre-processing and post-processing methods, including face/eye detection, data rectification, 2D/3D gaze conversion and gaze origin conversion. Finally, we set up a comprehensive benchmark for deep learning-based gaze estimation. We characterize all the public datasets and provide the source code of typical gaze estimation algorithms. This paper serves not only as a reference to develop deep learning-based gaze estimation methods, but also a guideline for future gaze estimation research. The project web page can be found at https://phi-ai.buaa.edu.cn/Gazehub.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@software{coddYOLOv11nfacedetection2024,
  title = {{{YOLOv11n-face-detection}}},
  author = {Codd, Adam},
  date = {2024},
  url = {https://huggingface.co/AdamCodd/YOLOv11n-face-detection},
  abstract = {A lightweight face detection model based on YOLO architecture (YOLOv11 nano), trained for 225 epochs on the WIDERFACE dataset.}
}

@article{daiLongitudinalDataCollection2022,
  title = {Longitudinal Data Collection to Follow Social Network and Language Development Dynamics at Preschool},
  author = {Dai, Sicheng and Bouchet, Hélène and Karsai, Márton and Chevrot, Jean-Pierre and Fleury, Eric and Nardy, Aurélie},
  date = {2022-12-22},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {9},
  number = {1},
  pages = {777},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01756-x},
  url = {https://www.nature.com/articles/s41597-022-01756-x},
  urldate = {2025-02-18},
  abstract = {Abstract             DyLNet is a large-scale longitudinal social experiment designed to observe the relations between child socialisation and oral language learning at preschool. During three years, a complete preschool in France was followed to record proximity interactions of about 200 children and adults every 5\,seconds using autonomous Radio Frequency Identification Wireless Proximity Sensors. Data was collected monthly with one week-long deployments. In parallel, survey campaigns were carried out to record the socio-demographic and language background of children and their families, and to monitor the linguistic skills of the pupils at regular intervals. From data we inferred real social interactions and distinguished inter- and intra-class interactions in different settings. We share ten weeks of cleaned, pre-processed and reconstructed interaction data recorded over a complete school year, together with two sets of survey data providing details about the pupils’ socio-demographic profile and language development level at the beginning and end of this period. Our dataset may stimulate researchers from several fields to study the simultaneous development of language and social interactions of children.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/KDVM7TZQ/Dai et al. - 2022 - Longitudinal data collection to follow social netw.pdf}
}

@article{debarbaroTenLessonsInfants2022,
  title = {Ten {{Lessons About Infants}}’ {{Everyday Experiences}}},
  author = {De Barbaro, Kaya and Fausey, Caitlin M.},
  date = {2022-02},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {31},
  number = {1},
  pages = {28--33},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/09637214211059536},
  url = {https://journals.sagepub.com/doi/10.1177/09637214211059536},
  urldate = {2024-12-23},
  abstract = {Audio recorders, accelerometers, and cameras that infants wear throughout their everyday lives capture the experiences that are available to shape development. Using sensors to capture behaviors in natural settings can reveal patterns within the everyday hubbub that are unknowable using methods that capture shorter, more isolated, or more planned slices of behavior. Here, we review 10 lessons learned from recent endeavors in which researchers neither designed nor participated in infants’ experiences and instead quantified patterns that arose within infants’ own spontaneously arising everyday experiences. The striking heterogeneity of experiences—the fact that there is no meaningfully “representative” hour of a day, instance of a category, interaction context, or infant—inspires next steps in theory and practice that embrace the complex, dynamic, and multiple pathways of human development.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/29MXEGW7/De Barbaro and Fausey - 2022 - Ten Lessons About Infants’ Everyday Experiences.pdf}
}

@article{donnellyLongitudinalRelationshipConversational2021,
  title = {The {{Longitudinal Relationship Between Conversational Turn}}‐{{Taking}} and {{Vocabulary Growth}} in {{Early Language Development}}},
  author = {Donnelly, Seamus and Kidd, Evan},
  date = {2021-03},
  journaltitle = {Child Development},
  shortjournal = {Child Development},
  volume = {92},
  number = {2},
  pages = {609--625},
  issn = {0009-3920, 1467-8624},
  doi = {10.1111/cdev.13511},
  url = {https://srcd.onlinelibrary.wiley.com/doi/10.1111/cdev.13511},
  urldate = {2025-01-11},
  abstract = {Children acquire language embedded within the rich social context of interaction. This paper reports on a longitudinal study investigating the developmental relationship between conversational turn‐taking and vocabulary growth in English‐acquiring children (               N               ~=~122) followed between 9 and 24~months. Daylong audio recordings obtained every 3~months provided several indices of the language environment, including the number of adult words children heard in their environment and their number of conversational turns. Vocabulary was measured independently via parental report. Growth curve analyses revealed a bidirectional relationship between conversational turns and vocabulary growth, controlling for the amount of words in children’s environments. The results are consistent with theoretical approaches that identify social interaction as a core component of early language acquisition.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/KTKFUA2Y/Donnelly and Kidd - 2021 - The Longitudinal Relationship Between Conversation.pdf}
}

@online{duStrongSORTMakeDeepSORT2022,
  title = {{{StrongSORT}}: {{Make DeepSORT Great Again}}},
  shorttitle = {{{StrongSORT}}},
  author = {Du, Yunhao and Zhao, Zhicheng and Song, Yang and Zhao, Yanyun and Su, Fei and Gong, Tao and Meng, Hongying},
  date = {2022},
  doi = {10.48550/ARXIV.2202.13514},
  url = {https://arxiv.org/abs/2202.13514},
  urldate = {2024-10-23},
  abstract = {Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/YGKFB2NL/Du et al. - 2022 - StrongSORT Make DeepSORT Great Again.pdf}
}

@article{ferjanramirezParentCoachingIncreases2020,
  title = {Parent Coaching Increases Conversational Turns and Advances Infant Language Development},
  author = {Ferjan Ramírez, Naja and Lytle, Sarah Roseberry and Kuhl, Patricia K.},
  date = {2020-02-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {117},
  number = {7},
  pages = {3484--3491},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1921653117},
  url = {https://pnas.org/doi/full/10.1073/pnas.1921653117},
  urldate = {2025-05-19},
  abstract = {Parental language input is one of the best predictors of children’s language achievement. Parentese, a near-universal speaking style distinguished by higher pitch, slower tempo, and exaggerated intonation, has been documented in speech directed toward young children in many countries. Previous research shows that the use of parentese and parent–child turn-taking are both associated with advances in children’s language learning. We conducted a randomized controlled trial to determine whether a parent coaching intervention delivered when the infants are 6, 10, and 14 mo of age can enhance parental language input and whether this, in turn, changes the trajectory of child language development between 6 and 18 mo of age. Families of typically developing 6-mo-old infants (               n               = 71) were randomly assigned to intervention and control groups. Naturalistic first-person audio recordings of the infants’ home language environment and vocalizations were recorded when the infants were 6, 10, 14, and 18 mo of age. After the 6-, 10-, and 14-mo recordings, intervention, but not control parents attended individual coaching appointments to receive linguistic feedback, listen to language input in their own recordings, and discuss age-appropriate activities that promote language growth. Intervention significantly enhanced parental use of parentese and parent–child turn-taking between 6 and 18 mo. Increases in both variables were significantly correlated with children’s language growth during the same period, and children’s language outcomes at 18 mo. Using parentese, a socially and linguistically enhanced speaking style, improves children’s social language turn-taking and language skills. Research-based interventions targeting social aspects of parent–child interactions can enhance language outcomes.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/W3JH734G/Ferjan Ramírez et al. - 2020 - Parent coaching increases conversational turns and.pdf}
}

@report{fordLENALanguageEnvironment2008,
  title = {The {{LENA Language Environment Analysis System}}: {{Audio Specifications}} of the {{DLP-0121}}},
  author = {Ford, Michael and Baer, Charles T. and Xu, Dongxin and Yapanel, Umit and Gray, Sharmi},
  date = {2008-09},
  url = {https://www.lena.org/wp-content/uploads/2016/07/LTR-03-2_Audio_Specifications.pdf},
  abstract = {Xu}
}

@online{fraileUpStoryUppsalaStorytelling2024,
  title = {{{UpStory}}: The {{Uppsala Storytelling}} Dataset},
  shorttitle = {{{UpStory}}},
  author = {Fraile, Marc and Calvo-Barajas, Natalia and Apeiron, Anastasia Sophia and Varni, Giovanna and Lindblad, Joakim and Sladoje, Nataša and Castellano, Ginevra},
  date = {2024},
  doi = {10.48550/ARXIV.2407.04352},
  url = {https://arxiv.org/abs/2407.04352},
  urldate = {2025-01-12},
  abstract = {Friendship and rapport play an important role in the formation of constructive social interactions, and have been widely studied in educational settings due to their impact on student outcomes. Given the growing interest in automating the analysis of such phenomena through Machine Learning (ML), access to annotated interaction datasets is highly valuable. However, no dataset on dyadic child-child interactions explicitly capturing rapport currently exists. Moreover, despite advances in the automatic analysis of human behaviour, no previous work has addressed the prediction of rapport in child-child dyadic interactions in educational settings. We present UpStory -- the Uppsala Storytelling dataset: a novel dataset of naturalistic dyadic interactions between primary school aged children, with an experimental manipulation of rapport. Pairs of children aged 8-10 participate in a task-oriented activity: designing a story together, while being allowed free movement within the play area. We promote balanced collection of different levels of rapport by using a within-subjects design: self-reported friendships are used to pair each child twice, either minimizing or maximizing pair separation in the friendship network. The dataset contains data for 35 pairs, totalling 3h 40m of audio and video recordings. It includes two video sources covering the play area, as well as separate voice recordings for each child. An anonymized version of the dataset is made publicly available, containing per-frame head pose, body pose, and face features; as well as per-pair information, including the level of rapport. Finally, we provide ML baselines for the prediction of rapport.},
  pubstate = {prepublished},
  version = {1},
  keywords = {FOS: Computer and information sciences,Human-Computer Interaction (cs.HC),Machine Learning (cs.LG)}
}

@article{ginsburgImportancePlayPromoting2007,
  title = {The {{Importance}} of {{Play}} in {{Promoting Healthy Child Development}} and {{Maintaining Strong Parent-Child Bonds}}},
  author = {Ginsburg, Kenneth R. and {and the Committee on Communications} and {and the Committee on Psychosocial Aspects of Child and Family Health}},
  date = {2007-01-01},
  journaltitle = {Pediatrics},
  volume = {119},
  number = {1},
  pages = {182--191},
  issn = {0031-4005, 1098-4275},
  doi = {10.1542/peds.2006-2697},
  url = {https://publications.aap.org/pediatrics/article/119/1/182/70699/The-Importance-of-Play-in-Promoting-Healthy-Child},
  urldate = {2025-05-20},
  abstract = {Play is essential to development because it contributes to the cognitive, physical, social, and emotional well-being of children and youth. Play also offers an ideal opportunity for parents to engage fully with their children. Despite the benefits derived from play for both children and parents, time for free play has been markedly reduced for some children. This report addresses a variety of factors that have reduced play, including a hurried lifestyle, changes in family structure, and increased attention to academics and enrichment activities at the expense of recess or free child-centered play. This report offers guidelines on how pediatricians can advocate for children by helping families, school systems, and communities consider how best to ensure that play is protected as they seek the balance in children’s lives to create the optimal developmental milieu.},
  langid = {english}
}

@article{guoLiteGazeNeuralArchitecture2023,
  title = {{{LiteGaze}}: {{Neural}} Architecture Search for Efficient Gaze Estimation},
  shorttitle = {{{LiteGaze}}},
  author = {Guo, Xinwei and Wu, Yong and Miao, Jingjing and Chen, Yang},
  editor = {Gomes, Rahul},
  date = {2023-05-01},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {18},
  number = {5},
  pages = {e0284814},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0284814},
  url = {https://dx.plos.org/10.1371/journal.pone.0284814},
  urldate = {2025-02-23},
  abstract = {Gaze estimation plays a critical role in human-centered vision applications such as human–computer interaction and virtual reality. Although significant progress has been made in automatic gaze estimation by deep convolutional neural networks, it is still difficult to directly deploy deep learning based gaze estimation models across different edge devices, due to the high computational cost and various resource constraints. This work proposes LiteGaze, a deep learning framework to learn architectures for efficient gaze estimation via neural architecture search (NAS). Inspired by the once-for-all model (Cai et al., 2020), this work decouples the model training and architecture search into two different stages. In particular, a supernet is trained to support diverse architectural settings. Then specialized sub-networks are selected from the obtained supernet, given different efficiency constraints. Extensive experiments are performed on two gaze estimation datasets and demonstrate the superiority of the proposed method over previous works, advancing the real-time gaze estimation on edge devices.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/4UH5J2DB/Guo et al. - 2023 - LiteGaze Neural architecture search for efficient.pdf}
}

@article{hasaninSeverelyImbalancedBig2019,
  title = {Severely Imbalanced {{Big Data}} Challenges: Investigating Data Sampling Approaches},
  shorttitle = {Severely Imbalanced {{Big Data}} Challenges},
  author = {Hasanin, Tawfiq and Khoshgoftaar, Taghi M. and Leevy, Joffrey L. and Bauder, Richard A.},
  date = {2019-12},
  journaltitle = {Journal of Big Data},
  shortjournal = {J Big Data},
  volume = {6},
  number = {1},
  pages = {107},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0274-4},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0274-4},
  urldate = {2025-02-02},
  abstract = {Abstract                            Severe class imbalance between majority and minority classes in Big Data can bias the predictive performance of               Machine Learning               algorithms toward the majority (negative) class. Where the minority (positive) class holds greater value than the majority (negative) class and the occurrence of false negatives incurs a greater penalty than false positives, the bias may lead to adverse consequences. Our paper incorporates two case studies, each utilizing three learners, six sampling approaches, two performance metrics, and five sampled distribution ratios, to uniquely investigate the effect of severe class imbalance on Big Data analytics. The learners (               Gradient-Boosted Trees, Logistic Regression, Random Forest               ) were implemented within the Apache Spark framework. The first case study is based on a Medicare fraud detection dataset. The second case study, unlike the first, includes training data from one source (SlowlorisBig Dataset) and test data from a separate source (POST dataset). Results from the Medicare case study are not conclusive regarding the best sampling approach using               Area Under the Receiver Operating Characteristic Curve               and               Geometric Mean               performance metrics. However, it should be noted that the               Random Undersampling               approach performs adequately in the first case study. For the SlowlorisBig case study, Random Undersampling convincingly outperforms the other five sampling approaches (               Random Oversampling, Synthetic Minority Over-sampling TEchnique, SMOTE-borderline1 , SMOTE-borderline2 , ADAptive SYNthetic               ) when measuring performance with               Area Under the Receiver Operating Characteristic Curve               and               Geometric Mean               metrics. Based on its classification performance in both case studies,               Random Undersampling               is the best choice as it results in models with a significantly smaller number of samples, thus reducing computational burden and training time.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/3FT3AHBL/Hasanin et al. - 2019 - Severely imbalanced Big Data challenges investiga.pdf}
}

@inproceedings{heilbronActivityNetLargescaleVideo2015,
  title = {{{ActivityNet}}: {{A}} Large-Scale Video Benchmark for Human Activity Understanding},
  shorttitle = {{{ActivityNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
  date = {2015-06},
  pages = {961--970},
  publisher = {IEEE},
  location = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298698},
  url = {http://ieeexplore.ieee.org/document/7298698/},
  urldate = {2025-05-19},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0}
}

@article{hernandez-herediaProximitySensorMeasuring2024,
  title = {Proximity {{Sensor}} for {{Measuring Social Interaction}} in a {{School Environment}}},
  author = {Hernández-Heredia, Tania Karina and Reyes-Manzano, Cesar Fabián and Flores-Hernández, Diego Alonso and Ramos-Fernández, Gabriel and Guzmán-Vargas, Lev},
  date = {2024-07-25},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {24},
  number = {15},
  pages = {4822},
  issn = {1424-8220},
  doi = {10.3390/s24154822},
  url = {https://www.mdpi.com/1424-8220/24/15/4822},
  urldate = {2025-02-23},
  abstract = {Social interactions are characterized by being very diverse and changing over time. Understanding this diversity and dynamics, as well as their emerging patterns, is of great interest from social, health, and educational perspectives. The development of new devices has been made possible in recent years by advances in applied technology. This paper presents the design and development of a novel device composed of several sensors. Specifically, we propose a proximity sensor integrated by three devices: a Bluetooth sensor, a global positioning system (GPS) unit and an accelerometer. By means of this sensor it is possible to detect the presence of neighboring sensors in various configurations and operating conditions. Profiles based on the Received Signal Strength Indicator (RSSI) exhibit behavior consistent with that reported by empirical relationships. The present sensor is functional in detecting the proximity of other sensors and is thus useful for the identification of interactions between people in relevant contexts such as schools.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/K8DU5PUS/Hernández-Heredia et al. - 2024 - Proximity Sensor for Measuring Social Interaction .pdf}
}

@book{heyesCognitiveGadgetsCultural2018,
  title = {Cognitive Gadgets: The Cultural Evolution of Thinking},
  shorttitle = {Cognitive Gadgets},
  author = {Heyes, Cecilia M.},
  date = {2018},
  publisher = {Harvard University press},
  location = {Cambridge (Mass.)},
  abstract = {How did human minds become so different from those of other animals? What accounts for our capacity to understand the way the physical world works, to think ourselves into the minds of others, to gossip, read, tell stories about the past, and imagine the future? These questions are not new: they have been debated by philosophers, psychologists, anthropologists, evolutionists, and neurobiologists over the course of centuries. One explanation widely accepted today is that humans have special cognitive instincts. Unlike other living animal species, we are born with complicated mechanisms for reasoning about causation, reading the minds of others, copying behaviors, and using language. Cecilia Heyes agrees that adult humans have impressive pieces of cognitive equipment. In her framing, however, these cognitive gadgets are not instincts programmed in the genes but are constructed in the course of childhood through social interaction. Cognitive gadgets are products of cultural evolution, rather than genetic evolution. At birth, the minds of human babies are only subtly different from the minds of newborn chimpanzees. We are friendlier, our attention is drawn to different things, and we have a capacity to learn and remember that outstrips the abilities of newborn chimpanzees. Yet when these subtle differences are exposed to culture-soaked human environments, they have enormous effects. They enable us to upload distinctively human ways of thinking from the social world around us. As Cognitive Gadgets makes clear, from birth our malleable human minds can learn through culture not only what to think but how to think it},
  isbn = {978-0-674-98015-0},
  langid = {english}
}

@article{hofferthHowAmericanChildren2001,
  title = {How {{American Children Spend Their Time}}},
  author = {Hofferth, Sandra L. and Sandberg, John F.},
  date = {2001-05},
  journaltitle = {Journal of Marriage and Family},
  shortjournal = {J of Marriage and Family},
  volume = {63},
  number = {2},
  pages = {295--308},
  issn = {0022-2445, 1741-3737},
  doi = {10.1111/j.1741-3737.2001.00295.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1741-3737.2001.00295.x},
  urldate = {2025-05-20},
  abstract = {The purpose of this article is to examine how American children under age 13 spend their time, sources of variation in time use, and associations with achievement and behavior. Data come from the 1997 Child Development Supplement to the Panel Study of Income Dynamics. The results suggest that parents' characteristics and decisions regarding marriage, family size, and employment affect the time children spend in educational, structured, and family activities, which may affect their school achievement. Learning activities such as reading for pleasure are associated with higher achievement, as is structured time spent playing sports and in social activities. Family time spent at meals and time spent sleeping are linked to fewer behavior problems, as measured by the child's score on the Behavior Problems Index. The results support common language and myth about the optimal use of time for child development.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/WITNBY6E/Hofferth and Sandberg - 2001 - How American Children Spend Their Time.pdf}
}

@article{hornAutomatedDatadrivenApproach2024,
  title = {An Automated, Data‐driven Approach to Children's Social Dynamics in Space and Time},
  author = {Horn, Lisa and Karsai, Márton and Markova, Gabriela},
  date = {2024-03},
  journaltitle = {Child Development Perspectives},
  shortjournal = {Child Dev Perspectives},
  volume = {18},
  number = {1},
  pages = {36--43},
  issn = {1750-8592, 1750-8606},
  doi = {10.1111/cdep.12495},
  url = {https://srcd.onlinelibrary.wiley.com/doi/10.1111/cdep.12495},
  urldate = {2025-01-27},
  abstract = {Abstract             Most children first enter social groups of peers in preschool. In this context, children use movement as a social tool, resulting in distinctive proximity patterns in space and synchrony with others over time. However, the social implications of children's movements with peers in space and time are difficult to determine due to the difficulty of acquiring reliable data during natural interactions. In this article, we review research demonstrating that proximity and synchrony are important indicators of affiliation among preschoolers and highlight challenges in this line of research. We then argue for the advantages of using wearable sensor technology and machine learning analytics to quantify social movement. This technological and analytical advancement provides an unprecedented view of complex social interactions among preschoolers in natural settings, and can help integrate young children's movements with others in space and time into a coherent interaction framework.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/6V8PTM5H/Horn et al. - 2024 - An automated, data‐driven approach to children's s.pdf}
}

@article{janssenTrackingRealtimeProximity2024,
  title = {Tracking Real-Time Proximity in Daily Life: {{A}} New Tool to Examine Social Interactions},
  shorttitle = {Tracking Real-Time Proximity in Daily Life},
  author = {Janssen, Loes H. C. and Verkuil, Bart and Nedderhoff, Andre and Van Houtum, Lisanne A. E. M. and Wever, Mirjam C. M. and Elzinga, Bernet M.},
  date = {2024-04-29},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {56},
  number = {7},
  pages = {7482--7497},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02432-1},
  url = {https://link.springer.com/10.3758/s13428-024-02432-1},
  urldate = {2025-02-23},
  abstract = {Abstract                            Social interactions, spending time together, and relationships are important for individuals’ well-being, with people feeling happier when they spend more time with others. So far, most information about the frequency and duration of spending time together is based on self-report questionnaires. Although recent technological innovations have stimulated the development of objective approaches for measuring physical proximity in humans in everyday life, these methods still have substantial limitations. Here we present a novel method, using Bluetooth low-energy beacons and a smartphone application, to measure the frequency and duration of dyads being in close proximity in daily life. This method can also be used to link the frequency and duration of proximity to the quality of interactions, by using proximity-triggered questionnaires. We examined the use of this novel method by exploring proximity patterns of family interactions among 233 participants (77 Dutch families, with 77 adolescents [               M               age               = 15.9] and 145 parents [               M               age               = 48.9]) for 14 consecutive days. Overall, proximity-based analyses indicated that adolescents were more often and longer in proximity to mothers than to fathers, with large differences between families in frequency and duration. Proximity-triggered evaluations of the interactions and parenting behavior were generally positive for both fathers and mothers. This innovative method is a promising tool that can be broadly used in other social contexts to yield new and more detailed insights into social proximity in daily life.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/NBR9ZY7L/Janssen et al. - 2024 - Tracking real-time proximity in daily life A new .pdf}
}

@software{jocherUltralyticsYOLO112024,
  title = {Ultralytics {{YOLO11}}},
  author = {Jocher, Glenn and Qiu, Jing},
  date = {2024},
  url = {https://github.com/ultralytics/ultralytics},
  isbn = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  version = {11.0.0}
}

@inreference{jocherUltralyticsYolo11Performance2024,
  title = {Ultralytics {{Yolo11 Performance Metrics}}},
  author = {Jocher, Glenn},
  date = {2024-01-10},
  url = {https://docs.ultralytics.com/guides/yolo-performance-metrics/?utm_source=chatgpt.com},
  urldate = {2025-01-02},
  abstract = {Performance metrics are key tools to evaluate the accuracy and efficiency of object detection models. They shed light on how effectively a model can identify and localize objects within images. Additionally, they help in understanding the model's handling of false positives and false negatives. These insights are crucial for evaluating and enhancing the model's performance. In this guide, we will explore various performance metrics associated with YOLO11, their significance, and how to interpret them.}
}

@software{jocherUltralyticsYOLO2023,
  title = {Ultralytics {{YOLO}}},
  author = {Jocher, Glenn and Jing, Qiu and Chaurasia, Ayush},
  date = {2023-01-10},
  url = {https://github.com/ultralytics/ultralytics},
  abstract = {Ultralytics YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLO11 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.},
  version = {8.0.0}
}

@incollection{kapidisObjectDetectionBasedLocation2020,
  title = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}: {{A Systematic Analysis}}},
  shorttitle = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}},
  booktitle = {Smart {{Assisted Living}}},
  author = {Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas P. J. J. and Veltkamp, Remco C.},
  editor = {Chen, Feng and García-Betances, Rebeca I. and Chen, Liming and Cabrera-Umpiérrez, María Fernanda and Nugent, Chris},
  date = {2020},
  pages = {119--145},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-25590-9_6},
  url = {http://link.springer.com/10.1007/978-3-030-25590-9_6},
  urldate = {2024-10-23},
  isbn = {978-3-030-25589-3 978-3-030-25590-9},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/XTFMTBU9/Kapidis et al. - 2020 - Object Detection-Based Location and Activity Class.pdf}
}

@article{kaurSystematicReviewImbalanced2020,
  title = {A {{Systematic Review}} on {{Imbalanced Data Challenges}} in {{Machine Learning}}: {{Applications}} and {{Solutions}}},
  shorttitle = {A {{Systematic Review}} on {{Imbalanced Data Challenges}} in {{Machine Learning}}},
  author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
  date = {2020-07-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  number = {4},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3343440},
  url = {https://dl.acm.org/doi/10.1145/3343440},
  urldate = {2025-02-02},
  abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
  langid = {english}
}

@online{kazakosLittleHelpMy2021,
  title = {With a {{Little Help}} from My {{Temporal Context}}: {{Multimodal Egocentric Action Recognition}}},
  shorttitle = {With a {{Little Help}} from My {{Temporal Context}}},
  author = {Kazakos, Evangelos and Huh, Jaesung and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  date = {2021},
  doi = {10.48550/ARXIV.2111.01024},
  url = {https://arxiv.org/abs/2111.01024},
  urldate = {2025-01-02},
  abstract = {In egocentric videos, actions occur in quick succession. We capitalise on the action's temporal context and propose a method that learns to attend to surrounding actions in order to improve recognition performance. To incorporate the temporal context, we propose a transformer-based multimodal model that ingests video and audio as input modalities, with an explicit language model providing action sequence context to enhance the predictions. We test our approach on EPIC-KITCHENS and EGTEA datasets reporting state-of-the-art performance. Our ablations showcase the advantage of utilising temporal context as well as incorporating audio input modality and language model to rescore predictions. Code and models at: https://github.com/ekazakos/MTCN.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Audio and Speech Processing (eess.AS),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Sound (cs.SD)}
}

@online{kellnhoferGaze360PhysicallyUnconstrained2019,
  title = {Gaze360: {{Physically Unconstrained Gaze Estimation}} in the {{Wild}}},
  shorttitle = {Gaze360},
  author = {Kellnhofer, Petr and Recasens, Adria and Stent, Simon and Matusik, Wojciech and Torralba, Antonio},
  date = {2019},
  doi = {10.48550/ARXIV.1910.10088},
  url = {https://arxiv.org/abs/1910.10088},
  urldate = {2025-03-04},
  abstract = {Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models are available at http://gaze360.csail.mit.edu .},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{khanamYOLOv11OverviewKey2024,
  title = {{{YOLOv11}}: {{An Overview}} of the {{Key Architectural Enhancements}}},
  shorttitle = {{{YOLOv11}}},
  author = {Khanam, Rahima and Hussain, Muhammad},
  date = {2024-10-23},
  eprint = {2410.17725},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.17725},
  url = {http://arxiv.org/abs/2410.17725},
  urldate = {2025-02-02},
  abstract = {This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/4CUNEDTY/Khanam and Hussain - 2024 - YOLOv11 An Overview of the Key Architectural Enha.pdf;/Users/nelesuffo/Zotero/storage/LC9ZJ84K/2410.html}
}

@online{lavechinOpensourceVoiceType2020,
  title = {An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings},
  author = {Lavechin, Marvin and Bousbib, Ruben and Bredin, Hervé and Dupoux, Emmanuel and Cristia, Alejandrina},
  date = {2020},
  doi = {10.48550/ARXIV.2005.12656},
  url = {https://arxiv.org/abs/2005.12656},
  urldate = {2024-11-14},
  abstract = {Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages. Our model can be used as input for downstream tasks such as estimating the number of words produced by adult speakers, or the number of linguistic units produced by children. Our architecture combines SincNet filters with a stack of recurrent layers and outperforms by a large margin the state-of-the-art system, the Language ENvironment Analysis (LENA) that has been used in numerous child language studies.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Audio and Speech Processing (eess.AS),FOS: Electrical engineering electronic engineering information engineering,I.2.7}
}

@software{lavechinVoiceTypeClassifier2020,
  title = {Voice {{Type Classifier}}},
  author = {Lavechin, Marvin},
  date = {2020},
  url = {https://github.com/MarvinLvn/voice-type-classifier}
}

@article{lemaignanPInSoRoDatasetSupporting2018,
  title = {The {{PInSoRo}} Dataset: {{Supporting}} the Data-Driven Study of Child-Child and Child-Robot Social Dynamics},
  shorttitle = {The {{PInSoRo}} Dataset},
  author = {Lemaignan, Séverin and Edmunds, Charlotte E. R. and Senft, Emmanuel and Belpaeme, Tony},
  editor = {Goodman, Michael L.},
  date = {2018-10-19},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {13},
  number = {10},
  pages = {e0205999},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0205999},
  url = {https://dx.plos.org/10.1371/journal.pone.0205999},
  urldate = {2025-02-18},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/4JUIJJTX/Lemaignan et al. - 2018 - The PInSoRo dataset Supporting the data-driven st.pdf}
}

@online{liGeneralizedFocalLoss2020,
  title = {Generalized {{Focal Loss}}: {{Learning Qualified}} and {{Distributed Bounding Boxes}} for {{Dense Object Detection}}},
  shorttitle = {Generalized {{Focal Loss}}},
  author = {Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
  date = {2020-06-08},
  eprint = {2006.04388},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.04388},
  url = {http://arxiv.org/abs/2006.04388},
  urldate = {2025-02-03},
  abstract = {One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\textbackslash\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\textbackslash\%) and ATSS (43.6\textbackslash\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\textbackslash\%, at 10 FPS on a single 2080Ti GPU. Code and models are available at https://github.com/implus/GFocal.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/AX2H4SD2/Li et al. - 2020 - Generalized Focal Loss Learning Qualified and Dis.pdf;/Users/nelesuffo/Zotero/storage/LFHZ6MAA/2006.html}
}

@online{linBMNBoundaryMatchingNetwork2019,
  title = {{{BMN}}: {{Boundary-Matching Network}} for {{Temporal Action Proposal Generation}}},
  shorttitle = {{{BMN}}},
  author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
  date = {2019},
  doi = {10.48550/ARXIV.1907.09702},
  url = {https://arxiv.org/abs/1907.09702},
  urldate = {2024-11-13},
  abstract = {Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2014},
  doi = {10.48550/ARXIV.1405.0312},
  url = {https://arxiv.org/abs/1405.0312},
  urldate = {2025-03-09},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{longBabyViewDatasetHighresolution2024,
  title = {The {{BabyView}} Dataset: {{High-resolution}} Egocentric Videos of Infants' and Young Children's Everyday Experiences},
  shorttitle = {The {{BabyView}} Dataset},
  author = {Long, Bria and Xiang, Violet and Stojanov, Stefan and Sparks, Robert Z. and Yin, Zi and Keene, Grace E. and Tan, Alvin W. M. and Feng, Steven Y. and Zhuang, Chengxu and Marchman, Virginia A. and Yamins, Daniel L. K. and Frank, Michael C.},
  date = {2024-06-14},
  eprint = {2406.10447},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.10447},
  url = {http://arxiv.org/abs/2406.10447},
  urldate = {2024-12-20},
  abstract = {Human children far exceed modern machine learning algorithms in their sample efficiency, achieving high performance in key domains with much less data than current models. This ''data gap'' is a key challenge both for building intelligent artificial systems and for understanding human development. Egocentric video capturing children's experience -- their ''training data'' -- is a key ingredient for comparison of humans and models and for the development of algorithmic innovations to bridge this gap. Yet there are few such datasets available, and extant data are low-resolution, have limited metadata, and importantly, represent only a small set of children's experiences. Here, we provide the first release of the largest developmental egocentric video dataset to date -- the BabyView dataset -- recorded using a high-resolution camera with a large vertical field-of-view and gyroscope/accelerometer data. This 493 hour dataset includes egocentric videos from children spanning 6 months - 5 years of age in both longitudinal, at-home contexts and in a preschool environment. We provide gold-standard annotations for the evaluation of speech transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains. We train self-supervised language and vision models and evaluate their transfer to out-of-distribution tasks including syntactic structure learning, object recognition, depth estimation, and image segmentation. Although performance in each scales with dataset size, overall performance is relatively lower than when models are trained on curated datasets, especially in the visual domain. Our dataset stands as an open challenge for robust, humanlike AI systems: how can such systems achieve human-levels of success on the same scale and distribution of training data as humans?},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/RGPHCL8P/Long et al. - 2024 - The BabyView dataset High-resolution egocentric v.pdf;/Users/nelesuffo/Zotero/storage/RN3TYVJP/2406.html}
}

@online{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017-05-03},
  eprint = {1608.03983},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.03983},
  url = {http://arxiv.org/abs/1608.03983},
  urldate = {2025-02-03},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/nelesuffo/Zotero/storage/NVG5CPMI/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf;/Users/nelesuffo/Zotero/storage/YWBKR2PJ/1608.html}
}

@online{maNymeriaMassiveCollection2024,
  title = {Nymeria: {{A Massive Collection}} of {{Multimodal Egocentric Daily Motion}} in the {{Wild}}},
  shorttitle = {Nymeria},
  author = {Ma, Lingni and Ye, Yuting and Hong, Fangzhou and Guzov, Vladimir and Jiang, Yifeng and Postyeni, Rowan and Pesqueira, Luis and Gamino, Alexander and Baiyya, Vijay and Kim, Hyo Jin and Bailey, Kevin and Fosas, David Soriano and Liu, C. Karen and Liu, Ziwei and Engel, Jakob and De Nardi, Renzo and Newcombe, Richard},
  date = {2024},
  doi = {10.48550/ARXIV.2406.09905},
  url = {https://arxiv.org/abs/2406.09905},
  urldate = {2025-01-02},
  abstract = {We introduce Nymeria - a large-scale, diverse, richly annotated human motion dataset collected in the wild with multiple multimodal egocentric devices. The dataset comes with a) full-body ground-truth motion; b) multiple multimodal egocentric data from Project Aria devices with videos, eye tracking, IMUs and etc; and c) a third-person perspective by an additional observer. All devices are precisely synchronized and localized in on metric 3D world. We derive hierarchical protocol to add in-context language descriptions of human motion, from fine-grain motion narration, to simplified atomic action and high-level activity summarization. To the best of our knowledge, Nymeria dataset is the world's largest collection of human motion in the wild; first of its kind to provide synchronized and localized multi-device multimodal egocentric data; and the world's largest motion-language dataset. It provides 300 hours of daily activities from 264 participants across 50 locations, total travelling distance over 399Km. The language descriptions contain 301.5K sentences in 8.64M words from a vocabulary size of 6545. To demonstrate the potential of the dataset, we evaluate several SOTA algorithms for egocentric body tracking, motion synthesis, and action recognition. Data and code are open-sourced for research (c.f. https://www.projectaria.com/datasets/nymeria).},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Graphics (cs.GR)}
}

@software{mmaction2contributorsOpenMMLabsNextGeneration2020,
  title = {{{OpenMMLab}}'s {{Next Generation Video Understanding Toolbox}} and {{Benchmark}}},
  author = {MMAction2 Contributors},
  date = {2020},
  url = {urlhttps://github.com/open-mmlab/mmaction2}
}

@article{nunez-marcosEgocentricVisionbasedAction2022,
  title = {Egocentric {{Vision-based Action Recognition}}: {{A}} Survey},
  shorttitle = {Egocentric {{Vision-based Action Recognition}}},
  author = {Núñez-Marcos, Adrián and Azkune, Gorka and Arganda-Carreras, Ignacio},
  date = {2022-02},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {472},
  pages = {175--197},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.11.081},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221017586},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/I5LRFE37/Núñez-Marcos et al. - 2022 - Egocentric Vision-based Action Recognition A surv.pdf}
}

@article{onnelaUsingSociometersQuantify2014,
  title = {Using Sociometers to Quantify Social Interaction Patterns},
  author = {Onnela, Jukka-Pekka and Waber, Benjamin N. and Pentland, Alex and Schnorf, Sebastian and Lazer, David},
  date = {2014-07-15},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {4},
  number = {1},
  pages = {5604},
  issn = {2045-2322},
  doi = {10.1038/srep05604},
  url = {https://www.nature.com/articles/srep05604},
  urldate = {2025-02-18},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/KLMM9TXE/Onnela et al. - 2014 - Using sociometers to quantify social interaction p.pdf}
}

@article{piagetPartCognitiveDevelopment1964,
  title = {Part {{I}}: {{Cognitive}} Development in Children: {{Piaget}} Development and Learning},
  shorttitle = {Part {{I}}},
  author = {Piaget, Jean},
  date = {1964-09},
  journaltitle = {Journal of Research in Science Teaching},
  shortjournal = {J Res Sci Teach},
  volume = {2},
  number = {3},
  pages = {176--186},
  issn = {0022-4308, 1098-2736},
  doi = {10.1002/tea.3660020306},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/tea.3660020306},
  urldate = {2025-01-11},
  langid = {english}
}

@inproceedings{plaquetPowersetMulticlassCross2023,
  title = {Powerset Multi-Class Cross Entropy Loss for Neural Speaker Diarization},
  booktitle = {{{INTERSPEECH}} 2023},
  author = {Plaquet, Alexis and Bredin, Hervé},
  date = {2023-08-20},
  pages = {3222--3226},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2023-205},
  url = {https://www.isca-archive.org/interspeech_2023/plaquet23_interspeech.html},
  urldate = {2025-05-12},
  eventtitle = {{{INTERSPEECH}} 2023},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/QXGC4Y9H/Plaquet and Bredin - 2023 - Powerset multi-class cross entropy loss for neural.pdf}
}

@online{redmonYouOnlyLook2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2015},
  doi = {10.48550/ARXIV.1506.02640},
  url = {https://arxiv.org/abs/1506.02640},
  urldate = {2025-01-11},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  pubstate = {prepublished},
  version = {5},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@inproceedings{rehgDecodingChildrensSocial2013,
  title = {Decoding {{Children}}'s {{Social Behavior}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rehg, James M. and Abowd, Gregory D. and Rozga, Agata and Romero, Mario and Clements, Mark A. and Sclaroff, Stan and Essa, Irfan and Ousley, Opal Y. and Li, Yin and Kim, Chanho and Rao, Hrishikesh and Kim, Jonathan C. and Presti, Liliana Lo and Zhang, Jianming and Lantsman, Denis and Bidwell, Jonathan and Ye, Zhefan},
  date = {2013-06},
  pages = {3414--3421},
  publisher = {IEEE},
  location = {Portland, OR, USA},
  doi = {10.1109/CVPR.2013.438},
  url = {http://ieeexplore.ieee.org/document/6619282/},
  urldate = {2025-01-12},
  eventtitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-0-7695-4989-7},
  file = {/Users/nelesuffo/Zotero/storage/WANJ69M6/Rehg et al. - 2013 - Decoding Children's Social Behavior.pdf}
}

@article{rogoffImportanceUnderstandingChildrens2018,
  title = {The Importance of Understanding Children’s Lived Experience},
  author = {Rogoff, Barbara and Dahl, Audun and Callanan, Maureen},
  date = {2018-12},
  journaltitle = {Developmental Review},
  shortjournal = {Developmental Review},
  volume = {50},
  pages = {5--15},
  issn = {02732297},
  doi = {10.1016/j.dr.2018.05.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0273229718300236},
  urldate = {2024-12-23},
  langid = {english}
}

@article{rossanoHow24yearold2022,
  title = {How 2- and 4-Year-Old Children Coordinate Social Interactions with Peers},
  author = {Rossano, Federico and Terwilliger, Jack and Bangerter, Adrian and Genty, Emilie and Heesen, Raphaela and Zuberbühler, Klaus},
  date = {2022-09-12},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Phil. Trans. R. Soc. B},
  volume = {377},
  number = {1859},
  pages = {20210100},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2021.0100},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0100},
  urldate = {2025-02-18},
  abstract = {The Interaction Engine Hypothesis postulates that humans have a unique ability and motivation for social interaction. A crucial juncture in the ontogeny of the interaction engine could be around 2–4 years of age, but observational studies of children in natural contexts are limited. These data appear critical also for comparison with non-human primates. Here, we report on focal observations on 31 children aged 2- and 4-years old in four preschools (10 h per child). Children interact with a wide range of partners, many infrequently, but with one or two close friends. Four-year olds engage in cooperative social interactions more often than 2-year olds and fight less than 2-year olds. Conversations and playing with objects are the most frequent social interaction types in both age groups. Children engage in social interactions with peers frequently (on average 13 distinct social interactions per hour) and briefly (28 s on average) and shorter than those of great apes in comparable studies. Their social interactions feature entry and exit phases about two-thirds of the time, less frequently than great apes. The results support the Interaction Engine Hypothesis, as young children manifest a remarkable motivation and ability for fast-paced interactions with multiple partners.             This article is part of the theme issue ‘Revisiting the human ‘interaction engine’: comparative approaches to social action coordination’.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/AQ3DIDFG/Rossano et al. - 2022 - How 2- and 4-year-old children coordinate social i.pdf}
}

@article{roweDifferencesEarlyGesture2009,
  title = {Differences in {{Early Gesture Explain SES Disparities}} in {{Child Vocabulary Size}} at {{School Entry}}},
  author = {Rowe, Meredith L. and Goldin-Meadow, Susan},
  date = {2009-02-13},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {323},
  number = {5916},
  pages = {951--953},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1167025},
  url = {https://www.science.org/doi/10.1126/science.1167025},
  urldate = {2025-01-11},
  abstract = {Children from low–socioeconomic status (SES) families, on average, arrive at school with smaller vocabularies than children from high-SES families. In an effort to identify precursors to, and possible remedies for, this inequality, we videotaped 50 children from families with a range of different SES interacting with parents at 14 months and assessed their vocabulary skills at 54 months. We found that children from high-SES families frequently used gesture to communicate at 14 months, a relation that was explained by parent gesture use (with speech controlled). In turn, the fact that children from high-SES families have large vocabularies at 54 months was explained by children's gesture use at 14 months. Thus, differences in early gesture help to explain the disparities in vocabulary that children bring with them to school.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/YQ9VHZTY/Rowe and Goldin-Meadow - 2009 - Differences in Early Gesture Explain SES Dispariti.pdf}
}

@article{royPredictingBirthSpoken2015,
  title = {Predicting the Birth of a Spoken Word},
  author = {Roy, Brandon C. and Frank, Michael C. and DeCamp, Philip and Miller, Matthew and Roy, Deb},
  date = {2015-10-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {112},
  number = {41},
  pages = {12663--12668},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1419773112},
  url = {https://pnas.org/doi/full/10.1073/pnas.1419773112},
  urldate = {2024-12-20},
  abstract = {Significance             The emergence of productive language is a critical milestone in a child’s life. Laboratory studies have identified many individual factors that contribute to word learning, and larger scale studies show correlations between aspects of the home environment and language outcomes. To date, no study has compared across many factors involved in word learning. We introduce a new ultradense set of recordings that capture a single child’s daily experience during the emergence of language. We show that words used in distinctive spatial, temporal, and linguistic contexts are produced earlier, suggesting they are easier to learn. These findings support the importance of multimodal context in word learning for one child and provide new methods for quantifying the quality of children’s language input.           ,              Children learn words through an accumulation of interactions grounded in context. Although many factors in the learning environment have been shown to contribute to word learning in individual studies, no empirical synthesis connects across factors. We introduce a new ultradense corpus of audio and video recordings of a single child’s life that allows us to measure the child’s experience of each word in his vocabulary. This corpus provides the first direct comparison, to our knowledge, between different predictors of the child’s production of individual words. We develop a series of new measures of the distinctiveness of the spatial, temporal, and linguistic contexts in which a word appears, and show that these measures are stronger predictors of learning than frequency of use and that, unlike frequency, they play a consistent role across different syntactic categories. Our findings provide a concrete instantiation of classic ideas about the role of coherent activities in word learning and demonstrate the value of multimodal data in understanding children’s language acquisition.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/NKMNP2P4/Roy et al. - 2015 - Predicting the birth of a spoken word.pdf}
}

@article{ruffmanExposureBehavioralRegularities2023,
  title = {Exposure to Behavioral Regularities in Everyday Life Predicts Infants’ Acquisition of Mental State Vocabulary},
  author = {Ruffman, Ted and Chen, Lisa and Lorimer, Ben and Vanier, Sarah and Edgar, Kate and Scarf, Damian and Taumoepeau, Mele},
  date = {2023-07},
  journaltitle = {Developmental Science},
  shortjournal = {Developmental Science},
  volume = {26},
  number = {4},
  pages = {e13343},
  issn = {1363-755X, 1467-7687},
  doi = {10.1111/desc.13343},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/desc.13343},
  urldate = {2025-01-11},
  abstract = {Abstract                                           There are two broad views of children's theory of mind. The mentalist view is that it emerges in infancy and is possibly innate. The minimalist view is that it emerges more gradually in childhood and is heavily dependent on learning. According to minimalism, children initially understand behaviors rather than mental states, and they are assisted in doing so by recognizing repeating patterns in behavior. The regularities in behavior allow them to predict future behaviors, succeed on theory‐of‐mind tasks, acquire mental state words, and eventually, understand the mental states underlying behavior. The present study provided the first clear evidence for the plausibility of this view by fitting head cameras to 54 infants aged 6 to 25 months, and recording their view of the world in their daily lives. At 6 and 12 months, infants viewed an average of 146.5 repeated behaviors per hour, a rate consistent with approximately 560,000 repetitions in their first year, and with repetitions correlating with children's acquisition of mental state words, even after controlling for their general vocabulary and a range of variables indexing social interaction. We also recorded infants’ view of people searching or searching for and retrieving objects. These were 92 times less common and did not correlate with mental state vocabulary. Overall, the findings indicate that repeated behaviors provide a rich source of information for children that would readily allow them to recognize patterns in behavior and help them acquire mental state words, providing the first clear evidence for this claim of minimalism.                                         Research Highlights                                                                        Six‐ to 25‐month‐olds wore head cameras to record home life from infants’ point‐of‐view and help adjudicate between nativist and minimalist views of theory‐of‐mind (ToM).                                                           Nativists say ToM is too early developing to enable learning, whereas minimalists say infants learn to predict behaviors from behavior patterns in environment.                                                           Consistent with minimalism, infants had an incredibly rich exposure (146.5/h, {$>$}560,000 in first year) to repeated behaviors (e.g., drinking from a cup repeatedly).                                                           Consistent with minimalism, more repeated behaviors correlated with infants’ mental state vocabulary, even after controlling for gender, age, searches witnessed and non‐mental state vocabulary.},
  langid = {english}
}

@online{russakovskyImageNetLargeScale2014,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2014},
  doi = {10.48550/ARXIV.1409.0575},
  url = {https://arxiv.org/abs/1409.0575},
  urldate = {2025-01-12},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,I.4.8; I.5.2}
}

@article{saberCurriculumLearningInfant2023,
  title = {Curriculum Learning with Infant Egocentric Videos},
  author = {Saber, Sheybani and Hansaria,, Himanshu and Wood, Justin N. and Smith, Linda B. and Tiganj, Zoran},
  date = {2023},
  journaltitle = {NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages = {54199--54212},
  abstract = {Infants possess a remarkable ability to rapidly learn and process visual inputs. As an infant's mobility increases, so does the variety and dynamics of their visual inputs. Is this change in the properties of the visual inputs beneficial or even critical for the proper development of the visual system? To address this question, we used video recordings from infants wearing head-mounted cameras to train a variety of self-supervised learning models. Critically, we separated the infant data by age group and evaluated the importance of training with a curriculum aligned with developmental order. We found that initiating learning with the data from the youngest age group provided the strongest learning signal and led to the best learning outcomes in terms of downstream task performance. We then showed that the benefits of the data from the youngest age group are due to the slowness and simplicity of the visual experience. The results provide strong empirical evidence for the importance of the properties of the early infant experience and developmental progression in training. More broadly, our approach and findings take a noteworthy step towards reverse engineering the learning mechanisms in newborn brains using image-computable models from artificial intelligence.}
}

@article{shahDriverGazeEstimation2022,
  title = {A {{Driver Gaze Estimation Method Based}} on {{Deep Learning}}},
  author = {Shah, Sayyed Mudassar and Sun, Zhaoyun and Zaman, Khalid and Hussain, Altaf and Shoaib, Muhammad and Pei, Lili},
  date = {2022-05-23},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {22},
  number = {10},
  pages = {3959},
  issn = {1424-8220},
  doi = {10.3390/s22103959},
  url = {https://www.mdpi.com/1424-8220/22/10/3959},
  urldate = {2025-03-05},
  abstract = {Car crashes are among the top ten leading causes of death; they could mainly be attributed to distracted drivers. An advanced driver-assistance technique (ADAT) is a procedure that can notify the driver about a dangerous scenario, reduce traffic crashes, and improve road safety. The main contribution of this work involved utilizing the driver’s attention to build an efficient ADAT. To obtain this “attention value”, the gaze tracking method is proposed. The gaze direction of the driver is critical toward understanding/discerning fatal distractions, pertaining to when it is obligatory to notify the driver about the risks on the road. A real-time gaze tracking system is proposed in this paper for the development of an ADAT that obtains and communicates the gaze information of the driver. The developed ADAT system detects various head poses of the driver and estimates eye gaze directions, which play important roles in assisting the driver and avoiding any unwanted circumstances. The first (and more significant) task in this research work involved the development of a benchmark image dataset consisting of head poses and horizontal and vertical direction gazes of the driver’s eyes. To detect the driver’s face accurately and efficiently, the You Only Look Once (YOLO-V4) face detector was used by modifying it with the Inception-v3 CNN model for robust feature learning and improved face detection. Finally, transfer learning in the InceptionResNet-v2 CNN model was performed, where the CNN was used as a classification model for head pose detection and eye gaze angle estimation; a regression layer to the InceptionResNet-v2 CNN was added instead of SoftMax and the classification output layer. The proposed model detects and estimates head pose directions and eye directions with higher accuracy. The average accuracy achieved by the head pose detection system was 91\%; the model achieved a RMSE of 2.68 for vertical and 3.61 for horizontal eye gaze estimations.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/XHZYYH2H/Shah et al. - 2022 - A Driver Gaze Estimation Method Based on Deep Lear.pdf}
}

@online{simonyanTwoStreamConvolutionalNetworks2014,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  doi = {10.48550/ARXIV.1406.2199},
  url = {https://arxiv.org/abs/1406.2199},
  urldate = {2024-11-14},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{smithContributionsHeadMountedCameras2015,
  title = {Contributions of {{Head-Mounted Cameras}} to {{Studying}} the {{Visual Environments}} of {{Infants}} and {{Young Children}}},
  author = {Smith, Linda B. and Yu, Chen and Yoshida, Hanako and Fausey, Caitlin M.},
  date = {2015-05-27},
  journaltitle = {Journal of Cognition and Development},
  shortjournal = {Journal of Cognition and Development},
  volume = {16},
  number = {3},
  pages = {407--419},
  issn = {1524-8372, 1532-7647},
  doi = {10.1080/15248372.2014.933430},
  url = {http://www.tandfonline.com/doi/full/10.1080/15248372.2014.933430},
  urldate = {2024-12-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/44QZX3Y5/Smith et al. - 2015 - Contributions of Head-Mounted Cameras to Studying .pdf}
}

@article{smithDevelopingInfantCreates2018,
  title = {The {{Developing Infant Creates}} a {{Curriculum}} for {{Statistical Learning}}},
  author = {Smith, Linda B. and Jayaraman, Swapnaa and Clerkin, Elizabeth and Yu, Chen},
  date = {2018-04},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {4},
  pages = {325--336},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.02.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661318300275},
  urldate = {2025-01-11},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/2MNPACN4/Smith et al. - 2018 - The Developing Infant Creates a Curriculum for Sta.pdf}
}

@article{spanglerToddlersEverydayExperiences1989,
  title = {Toddlers' {{Everyday Experiences}} as {{Related}} to {{Preceding Mental}} and {{Emotional Disposition}} and {{Their Relationship}} to {{Subsequent Mental}} and {{Motivational Development}}: {{A Short-Term Longitudinal Study}}},
  shorttitle = {Toddlers' {{Everyday Experiences}} as {{Related}} to {{Preceding Mental}} and {{Emotional Disposition}} and {{Their Relationship}} to {{Subsequent Mental}} and {{Motivational Development}}},
  author = {Spangler, Gottfried},
  date = {1989-09},
  journaltitle = {International Journal of Behavioral Development},
  shortjournal = {International Journal of Behavioral Development},
  volume = {12},
  number = {3},
  pages = {285--303},
  issn = {0165-0254, 1464-0651},
  doi = {10.1177/016502548901200301},
  url = {https://journals.sagepub.com/doi/10.1177/016502548901200301},
  urldate = {2024-12-23},
  abstract = {The aims of the study were (1) to assess the relationship between children's everyday experiences and preceding mental and emotional disposition; (2) to describe the relationships between children's experiences and the children's mental and motivational development; (3) to derive hypotheses about the transactional connections between individual disposition, quality of experience, and mental and motivational development. Mental development and emotional disposition of 24 children were assessed at 12 months. During the second year type and quality of children's everyday experiences were repeatedly observed at home. At 24 months mental development was assessed again. In addition, children's motivation was observed during free play. The results reveal a transactional process for the mental and motivational development during the second year. Type and quality of children's everyday experiences were related to their individual mental and emotional disposition. In addition, level and quality of experiences were associated with subsequent mental and motivational development.},
  langid = {english}
}

@inproceedings{spelmenReviewHandlingImbalanced2018,
  title = {A {{Review}} on {{Handling Imbalanced Data}}},
  booktitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  author = {Spelmen, Vimalraj S and Porkodi, R},
  date = {2018-03},
  pages = {1--11},
  publisher = {IEEE},
  location = {Coimbatore},
  doi = {10.1109/ICCTCT.2018.8551020},
  url = {https://ieeexplore.ieee.org/document/8551020/},
  urldate = {2025-01-02},
  eventtitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  isbn = {978-1-5386-3702-9}
}

@book{stevensPsychophysicsIntroductionIts2017,
  title = {Psychophysics: {{Introduction}} to {{Its Perceptual}}, {{Neural}}, and {{Social Prospects}}},
  shorttitle = {Psychophysics},
  author = {Stevens, S.S. and Marks, Lawrence E.},
  date = {2017-09-29},
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9781315127675},
  url = {https://www.taylorfrancis.com/books/9781351495882},
  urldate = {2025-03-04},
  isbn = {978-1-315-12767-5},
  langid = {english}
}

@article{sullivanSAYCamLargeLongitudinal2021,
  title = {{{SAYCam}}: {{A Large}}, {{Longitudinal Audiovisual Dataset Recorded From}} the {{Infant}}’s {{Perspective}}},
  shorttitle = {{{SAYCam}}},
  author = {Sullivan, Jessica and Mei, Michelle and Perfors, Andrew and Wojcik, Erica and Frank, Michael C.},
  date = {2021-05-26},
  journaltitle = {Open Mind},
  volume = {5},
  pages = {20--29},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00039},
  url = {https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset},
  urldate = {2024-12-20},
  abstract = {Abstract             We introduce a new resource: the SAYCam corpus. Infants aged 6–32 months wore a head-mounted camera for approximately 2 hr per week, over the course of approximately two-and-a-half years. The result is a large, naturalistic, longitudinal dataset of infant- and child-perspective videos. Over 200,000 words of naturalistic speech have already been transcribed. Similarly, the dataset is searchable using a number of criteria (e.g., age of participant, location, setting, objects present). The resulting dataset will be of broad use to psychologists, linguists, and computer scientists.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/A5ANFVQR/Sullivan et al. - 2021 - SAYCam A Large, Longitudinal Audiovisual Dataset .pdf}
}

@online{tervenLossFunctionsMetrics2024,
  title = {Loss {{Functions}} and {{Metrics}} in {{Deep Learning}}},
  author = {Terven, Juan and Cordova-Esparza, Diana M. and Ramirez-Pedraza, Alfonso and Chavez-Urbiola, Edgar A. and Romero-Gonzalez, Julio A.},
  date = {2024-10-12},
  eprint = {2307.02694},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02694},
  url = {http://arxiv.org/abs/2307.02694},
  urldate = {2025-02-03},
  abstract = {When training or evaluating deep learning models, two essential parts are picking the proper loss function and deciding on performance metrics. In this paper, we provide a comprehensive overview of the most common loss functions and metrics used across many different types of deep learning tasks, from general tasks such as regression and classification to more specific tasks in Computer Vision and Natural Language Processing. We introduce the formula for each loss and metric, discuss their strengths and limitations, and describe how these methods can be applied to various problems within deep learning. This work can serve as a reference for researchers and practitioners in the field, helping them make informed decisions when selecting the most appropriate loss function and performance metrics for their deep learning projects.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nelesuffo/Zotero/storage/ABKDVUZ7/Terven et al. - 2024 - Loss Functions and Metrics in Deep Learning.pdf;/Users/nelesuffo/Zotero/storage/QYP2Y2JA/2307.html}
}

@book{tomaselloCulturalOriginsHuman2009,
  title = {Cultural {{Origins}} of {{Human Cognition}}},
  author = {Tomasello, Michael},
  date = {2009},
  publisher = {Harvard University Press},
  location = {Cambridge},
  isbn = {978-0-674-00582-2},
  langid = {english},
  pagetotal = {256}
}

@article{truongCrossviewActionRecognition2024,
  title = {Cross-View Action Recognition Understanding from Exocentric to Egocentric Perspective},
  author = {Truong, Thanh-Dat and Luu, Khoa},
  date = {2024-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  pages = {128731},
  issn = {09252312},
  doi = {10.1016/j.neucom.2024.128731},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231224015029},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/95IM2Q74/Truong and Luu - 2024 - Cross-view action recognition understanding from e.pdf}
}

@online{tsutsuiComputationalModelEarly2020,
  title = {A {{Computational Model}} of {{Early Word Learning}} from the {{Infant}}'s {{Point}} of {{View}}},
  author = {Tsutsui, Satoshi and Chandrasekaran, Arjun and Reza, Md Alimoor and Crandall, David and Yu, Chen},
  date = {2020-06-04},
  eprint = {2006.02802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.02802},
  url = {http://arxiv.org/abs/2006.02802},
  urldate = {2024-12-20},
  abstract = {Human infants have the remarkable ability to learn the associations between object names and visual objects from inherently ambiguous experiences. Researchers in cognitive science and developmental psychology have built formal models that implement in-principle learning algorithms, and then used pre-selected and pre-cleaned datasets to test the abilities of the models to find statistical regularities in the input data. In contrast to previous modeling approaches, the present study used egocentric video and gaze data collected from infant learners during natural toy play with their parents. This allowed us to capture the learning environment from the perspective of the learner's own point of view. We then used a Convolutional Neural Network (CNN) model to process sensory data from the infant's point of view and learn name-object associations from scratch. As the first model that takes raw egocentric video to simulate infant word learning, the present study provides a proof of principle that the problem of early word learning can be solved, using actual visual data perceived by infant learners. Moreover, we conducted simulation experiments to systematically determine how visual, perceptual, and attentional properties of infants' sensory experiences may affect word learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/RQHSGKUR/Tsutsui et al. - 2020 - A Computational Model of Early Word Learning from .pdf;/Users/nelesuffo/Zotero/storage/MQUAE3QR/2006.html}
}

@book{vygotskyMindSocietyDevelopment1978,
  title = {Mind in {{Society}}: {{The Development}} of {{Higher Psychological Processes}}},
  author = {Vygotsky, Lev Semyonovich},
  date = {1978},
  publisher = {Harvard University Press},
  location = {Cambridge, MA}
}

@online{wangFasterPersonReIdentification2020,
  title = {Faster {{Person Re-Identification}}},
  author = {Wang, Guan'an and Gong, Shaogang and Cheng, Jian and Hou, Zengguang},
  date = {2020},
  doi = {10.48550/ARXIV.2008.06826},
  url = {https://arxiv.org/abs/2008.06826},
  urldate = {2024-10-23},
  abstract = {Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a \$F\_β\$ score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8\% more accurate but also 5x faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is \$50\textbackslash times\$ faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/WG9M6GYH/Wang et al. - 2020 - Faster Person Re-Identification.pdf}
}

@online{wangTemporalSegmentNetworks2017,
  title = {Temporal {{Segment Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  date = {2017},
  doi = {10.48550/ARXIV.1705.02953},
  url = {https://arxiv.org/abs/1705.02953},
  urldate = {2024-11-14},
  abstract = {Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0\%), UCF101 (94.9\%), THUMOS14 (80.1\%), and ActivityNet v1.2 (89.6\%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0\%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{xiongCUHKETHZSIAT2016,
  title = {{{CUHK}} \& {{ETHZ}} \& {{SIAT Submission}} to {{ActivityNet Challenge}} 2016},
  author = {Xiong, Yuanjun and Wang, Limin and Wang, Zhe and Zhang, Bowen and Song, Hang and Li, Wei and Lin, Dahua and Qiao, Yu and Van Gool, Luc and Tang, Xiaoou},
  date = {2016},
  doi = {10.48550/ARXIV.1608.00797},
  url = {https://arxiv.org/abs/1608.00797},
  urldate = {2025-01-11},
  abstract = {This paper presents the method that underlies our submission to the untrimmed video classification task of ActivityNet Challenge 2016. We follow the basic pipeline of temporal segment networks and further raise the performance via a number of other techniques. Specifically, we use the latest deep model architecture, e.g., ResNet and Inception V3, and introduce new aggregation schemes (top-k and attention-weighted pooling). Additionally, we incorporate the audio as a complementary channel, extracting relevant information via a CNN applied to the spectrograms. With these techniques, we derive an ensemble of deep models, which, together, attains a high classification accuracy (mAP \$93.23\textbackslash\%\$) on the testing set and secured the first place in the challenge.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{xuContinualEgocentricActivity2023,
  title = {Towards {{Continual Egocentric Activity Recognition}}: {{A Multi-modal Egocentric Activity Dataset}} for {{Continual Learning}}},
  shorttitle = {Towards {{Continual Egocentric Activity Recognition}}},
  author = {Xu, Linfeng and Wu, Qingbo and Pan, Lili and Meng, Fanman and Li, Hongliang and He, Chiyuan and Wang, Hanxin and Cheng, Shaoxu and Dai, Yu},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.10931},
  url = {https://arxiv.org/abs/2301.10931},
  urldate = {2025-01-02},
  abstract = {With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{yildirimAutomaticDetectionDisfluency2009,
  title = {Automatic {{Detection}} of {{Disfluency Boundaries}} in {{Spontaneous Speech}} of {{Children Using Audio}}\&\#x2013;{{Visual Information}}},
  author = {Yildirim, Serdar and Narayanan, Shrikanth},
  date = {2009-01},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE Trans. Audio Speech Lang. Process.},
  volume = {17},
  number = {1},
  pages = {2--12},
  issn = {1558-7916},
  doi = {10.1109/TASL.2008.2006728},
  url = {http://ieeexplore.ieee.org/document/4740159/},
  urldate = {2025-01-12}
}

@article{yoshidaWhatsViewToddlers2008,
  title = {What's in {{View}} for {{Toddlers}}? {{Using}} a {{Head Camera}} to {{Study Visual Experience}}},
  shorttitle = {What's in {{View}} for {{Toddlers}}?},
  author = {Yoshida, Hanako and Smith, Linda B.},
  date = {2008-05-06},
  journaltitle = {Infancy},
  shortjournal = {Infancy},
  volume = {13},
  number = {3},
  pages = {229--248},
  issn = {1525-0008, 1532-7078},
  doi = {10.1080/15250000802004437},
  url = {https://onlinelibrary.wiley.com/doi/10.1080/15250000802004437},
  urldate = {2024-12-23},
  abstract = {This article reports 2 experiments using a new method to study 18‐to 24‐month‐olds' visual experiences as they interact with objects. Experiment 1 presents evidence on the coupling of head and eye movements and thus the validity of the head camera view of the infant's visual field in the geometry of the task context. Experiment 2 demonstrates the use of this method in the naturalistic context of toy play with a parent. The results point to the embodied nature of toddlers' attentional strategies and to importance of hands and hand actions in their visual experience of objects. The head camera thus appears to be a promising method that, despite some limitations, will yield new insights about the ecology and content of young children's experiences.},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/ZVPCJWU6/Yoshida and Smith - 2008 - What's in View for Toddlers Using a Head Camera t.pdf}
}

@inproceedings{zhangAppearancebasedGazeEstimation2015,
  title = {Appearance-Based Gaze Estimation in the Wild},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  date = {2015-06},
  pages = {4511--4520},
  publisher = {IEEE},
  location = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7299081},
  url = {http://ieeexplore.ieee.org/document/7299081/},
  urldate = {2025-03-04},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  file = {/Users/nelesuffo/Zotero/storage/9GCPTTJG/Zhang et al. - 2015 - Appearance-based gaze estimation in the wild.pdf}
}

@online{zhangETHXGazeLargeScale2020,
  title = {{{ETH-XGaze}}: {{A Large Scale Dataset}} for {{Gaze Estimation}} under {{Extreme Head Pose}} and {{Gaze Variation}}},
  shorttitle = {{{ETH-XGaze}}},
  author = {Zhang, Xucong and Park, Seonwook and Beeler, Thabo and Bradley, Derek and Tang, Siyu and Hilliges, Otmar},
  date = {2020},
  doi = {10.48550/ARXIV.2007.15837},
  url = {https://arxiv.org/abs/2007.15837},
  urldate = {2025-03-05},
  abstract = {Gaze estimation is a fundamental task in many applications of computer vision, human computer interaction and robotics. Many state-of-the-art methods are trained and tested on custom datasets, making comparison across methods challenging. Furthermore, existing gaze estimation datasets have limited head pose and gaze variations, and the evaluations are conducted using different protocols and metrics. In this paper, we propose a new gaze estimation dataset called ETH-XGaze, consisting of over one million high-resolution images of varying gaze under extreme head poses. We collect this dataset from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. We show that our dataset can significantly improve the robustness of gaze estimation methods across different head poses and gaze angles. Additionally, we define a standardized experimental protocol and evaluation metric on ETH-XGaze, to better unify gaze estimation research going forward. The dataset and benchmark website are available at https://ait.ethz.ch/projects/2020/ETH-XGaze},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{zhangItsWrittenAll2016,
  title = {It's {{Written All Over Your Face}}: {{Full-Face Appearance-Based Gaze Estimation}}},
  shorttitle = {It's {{Written All Over Your Face}}},
  author = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  date = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1611.08860},
  url = {https://arxiv.org/abs/1611.08860},
  urldate = {2025-03-04},
  abstract = {Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3\% on MPIIGaze and 27.7\% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses.},
  version = {4},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC)}
}

@article{zhangJointFaceDetection2016,
  title = {Joint {{Face Detection}} and {{Alignment}} Using {{Multi-task Cascaded Convolutional Networks}}},
  author = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
  date = {2016-10},
  journaltitle = {IEEE Signal Processing Letters},
  shortjournal = {IEEE Signal Process. Lett.},
  volume = {23},
  number = {10},
  eprint = {1604.02878},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1499--1503},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2016.2603342},
  url = {http://arxiv.org/abs/1604.02878},
  urldate = {2025-02-02},
  abstract = {Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nelesuffo/Zotero/storage/P9QEVZ4Z/Zhang et al. - 2016 - Joint Face Detection and Alignment using Multi-tas.pdf;/Users/nelesuffo/Zotero/storage/JK3IAJHK/1604.html}
}

@article{zhangMPIIGazeRealWorldDataset2019,
  title = {{{MPIIGaze}}: {{Real-World Dataset}} and {{Deep Appearance-Based Gaze Estimation}}},
  shorttitle = {{{MPIIGaze}}},
  author = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  date = {2019-01-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {41},
  number = {1},
  pages = {162--175},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2017.2778103},
  url = {https://ieeexplore.ieee.org/document/8122058/},
  urldate = {2025-03-05},
  file = {/Users/nelesuffo/Zotero/storage/HVK38PL6/Zhang et al. - 2019 - MPIIGaze Real-World Dataset and Deep Appearance-B.pdf}
}

@software{zotero-1479,
  type = {software}
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2024},
  url = {https://www.R-project.org/},
}
@Manual{R-BayesFactor,
  title = {BayesFactor: Computation of Bayes Factors for Common Designs},
  author = {Richard D. Morey and Jeffrey N. Rouder},
  year = {2024},
  note = {R package version 0.9.12-4.7},
  url = {https://CRAN.R-project.org/package=BayesFactor},
}
@Article{R-brms_a,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}
@Article{R-brms_b,
  title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
  author = {Paul-Christian Bürkner},
  journal = {The R Journal},
  year = {2018},
  volume = {10},
  number = {1},
  pages = {395--411},
  doi = {10.32614/RJ-2018-017},
  encoding = {UTF-8},
}
@Article{R-brms_c,
  title = {Bayesian Item Response Modeling in {R} with {brms} and {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2021},
  volume = {100},
  number = {5},
  pages = {1--54},
  doi = {10.18637/jss.v100.i05},
  encoding = {UTF-8},
}
@Manual{R-broom,
  title = {broom: Convert Statistical Objects into Tidy Tibbles},
  author = {David Robinson and Alex Hayes and Simon Couch},
  year = {2024},
  note = {R package version 1.0.7},
  url = {https://CRAN.R-project.org/package=broom},
}
@Article{R-coda,
  title = {CODA: Convergence Diagnosis and Output Analysis for MCMC},
  author = {Martyn Plummer and Nicky Best and Kate Cowles and Karen Vines},
  journal = {R News},
  year = {2006},
  volume = {6},
  number = {1},
  pages = {7--11},
  url = {https://journal.r-project.org/archive/},
  pdf = {https://www.r-project.org/doc/Rnews/Rnews_2006-1.pdf},
}
@Manual{R-cowplot,
  title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 1.1.3},
  url = {https://CRAN.R-project.org/package=cowplot},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
  year = {2023},
  note = {R package version 1.1.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-ggplotify,
  title = {ggplotify: Convert Plot to 'grob' or 'ggplot' Object},
  author = {Guangchuang Yu},
  year = {2023},
  note = {R package version 0.1.2},
  url = {https://CRAN.R-project.org/package=ggplotify},
}
@Manual{R-ggpubr,
  title = {ggpubr: 'ggplot2' Based Publication Ready Plots},
  author = {Alboukadel Kassambara},
  year = {2023},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggpubr},
}
@Manual{R-ggridges,
  title = {ggridges: Ridgeline Plots in 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 0.5.6},
  url = {https://CRAN.R-project.org/package=ggridges},
}
@Manual{R-ggthemes,
  title = {ggthemes: Extra Themes, Scales and Geoms for 'ggplot2'},
  author = {Jeffrey B. Arnold},
  year = {2024},
  note = {R package version 5.1.0},
  url = {https://CRAN.R-project.org/package=ggthemes},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Manual{R-kableExtra,
  title = {kableExtra: Construct Complex Table with 'kable' and Pipe Syntax},
  author = {Hao Zhu},
  year = {2024},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=kableExtra},
}
@Article{R-lubridate,
  title = {Dates and Times Made Easy with {lubridate}},
  author = {Garrett Grolemund and Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {3},
  pages = {1--25},
  url = {https://www.jstatsoft.org/v40/i03/},
}
@Manual{R-magick,
  title = {magick: Advanced Graphics and Image-Processing in R},
  author = {Jeroen Ooms},
  year = {2024},
  note = {R package version 2.8.5},
  url = {https://CRAN.R-project.org/package=magick},
}
@Manual{R-Matrix,
  title = {Matrix: Sparse and Dense Matrix Classes and Methods},
  author = {Douglas Bates and Martin Maechler and Mikael Jagan},
  year = {2024},
  note = {R package version 1.7-1},
  url = {https://CRAN.R-project.org/package=Matrix},
}
@Manual{R-papaja,
  title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
  author = {Frederik Aust and Marius Barth},
  year = {2024},
  note = {R package version 0.1.3},
  url = {https://github.com/crsh/papaja},
  doi = {10.32614/CRAN.package.papaja},
}
@Manual{R-patchwork,
  title = {patchwork: The Composer of Plots},
  author = {Thomas Lin Pedersen},
  year = {2024},
  note = {R package version 1.3.0},
  url = {https://CRAN.R-project.org/package=patchwork},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Hadley Wickham and Lionel Henry},
  year = {2023},
  note = {R package version 1.0.2},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Article{R-Rcpp_a,
  title = {{Rcpp}: Seamless {R} and {C++} Integration},
  author = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {8},
  pages = {1--18},
  doi = {10.18637/jss.v040.i08},
}
@Article{R-Rcpp_b,
  title = {{Extending {R} with {C++}: A Brief Introduction to {Rcpp}}},
  author = {Dirk Eddelbuettel and James Joseph Balamuta},
  journal = {The American Statistician},
  year = {2018},
  volume = {72},
  number = {1},
  pages = {28-36},
  doi = {10.1080/00031305.2017.1375990},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Jennifer Bryan},
  year = {2024},
  note = {R package version 2.1.5},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-readxl,
  title = {readxl: Read Excel Files},
  author = {Hadley Wickham and Jennifer Bryan},
  year = {2023},
  note = {R package version 1.4.3},
  url = {https://CRAN.R-project.org/package=readxl},
}
@Article{R-reshape2,
  title = {Reshaping Data with the {reshape} Package},
  author = {Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2007},
  volume = {21},
  number = {12},
  pages = {1--20},
  url = {http://www.jstatsoft.org/v21/i12/},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.5.1},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2023},
  note = {R package version 3.2.1},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Davis Vaughan and Maximilian Girlich},
  year = {2024},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Article{R-tidyverse,
  title = {Welcome to the {tidyverse}},
  author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686},
}
@Manual{R-tinylabels,
  title = {{tinylabels}: Lightweight Variable Labels},
  author = {Marius Barth},
  year = {2023},
  note = {R package version 0.2.4},
  url = {https://cran.r-project.org/package=tinylabels},
}
@Article{R-zoo,
  title = {zoo: S3 Infrastructure for Regular and Irregular Time Series},
  author = {Achim Zeileis and Gabor Grothendieck},
  journal = {Journal of Statistical Software},
  year = {2005},
  volume = {14},
  number = {6},
  pages = {1--27},
  doi = {10.18637/jss.v014.i06},
}
@Manual{R-data.table,
  title = {data.table: Extension of `data.frame`},
  author = {Tyson Barrett and Matt Dowle and Arun Srinivasan and Jan Gorecki and Michael Chirico and Toby Hocking and Benjamin Schwendinger},
  year = {2024},
  note = {R package version 1.16.2},
  url = {https://CRAN.R-project.org/package=data.table},
}
@Manual{R-jpeg,
  title = {jpeg: Read and write JPEG images},
  author = {Simon Urbanek},
  year = {2022},
  note = {R package version 0.1-10},
  url = {https://CRAN.R-project.org/package=jpeg},
}
@Article{R-lme4,
  title = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01},
}
@Manual{R-png,
  title = {png: Read and write PNG images},
  author = {Simon Urbanek},
  year = {2022},
  note = {R package version 0.1-8},
  url = {https://CRAN.R-project.org/package=png},
}
