@article{aghaeiWhomInteractDetecting2016,
  title = {With {{Whom Do I Interact}}? {{Detecting Social Interactions}} in {{Egocentric Photo-streams}}},
  shorttitle = {With {{Whom Do I Interact}}?},
  author = {Aghaei, Maedeh and Dimiccoli, Mariella and Radeva, Petia},
  year = {2016},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1605.04129},
  urldate = {2024-05-16},
  abstract = {Given a user wearing a low frame rate wearable camera during a day, this work aims to automatically detect the moments when the user gets engaged into a social interaction solely by reviewing the automatically captured photos by the worn camera. The proposed method, inspired by the sociological concept of F-formation, exploits distance and orientation of the appearing individuals -with respect to the user- in the scene from a bird-view perspective. As a result, the interaction pattern over the sequence can be understood as a two-dimensional time series that corresponds to the temporal evolution of the distance and orientation features over time. A Long-Short Term Memory-based Recurrent Neural Network is then trained to classify each time series. Experimental evaluation over a dataset of 30.000 images has shown promising results on the proposed method for social interaction detection in egocentric photo-streams.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/C9FVGBHP/Aghaei et al. - 2016 - With Whom Do I Interact Detecting Social Interact.pdf;/Users/nelesuffo/Zotero/storage/U5S9R9CW/1605.04129v2.pdf}
}

@article{arabaciMultimodalEgocentricActivity2021,
  title = {Multi-Modal Egocentric Activity Recognition Using Multi-Kernel Learning},
  author = {Arabac{\i}, Mehmet Ali and {\"O}zkan, Fatih and Surer, Elif and Jan{\v c}ovi{\v c}, Peter and Temizel, Alptekin},
  year = {2021},
  month = may,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {11},
  pages = {16299--16328},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08789-7},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/P2YJ73YD/Arabacı et al. - 2021 - Multi-modal egocentric activity recognition using .pdf}
}

@misc{brostromBoxMOTCollectionSOTA2023,
  title = {{{BoxMOT}}: {{A}} Collection of {{SOTA}} Real-Time, Multi-Object Trackers for Object Detectors},
  shorttitle = {{{BoxMOT}}},
  author = {Brostr{\"o}m, Mikel},
  year = {2023},
  month = jun,
  doi = {10.5281/ZENODO.7452873},
  urldate = {2024-10-23},
  abstract = {This repo contains a collections of state-of-the-art multi-object trackers. Supported ones at the moment are: DeepOCSORT , BoTSORT , StrongSORT, OCSORT and ByteTrack. DeepOCSORT, BoTSORT and StrongSORT are based on motion + appearance description; OCSORT and ByteTrack are based on motion only. For the methods using appearance description, lightweight state-of-the-art ReID models (LightMBN, OSNet and more) are downloaded automatically as well. We provide examples on how to use this package together with popular object detection models. Right now Yolov8, Yolo-NAS and YOLOX are available.},
  copyright = {GNU Affero General Public License v3.0, Open Access},
  howpublished = {Zenodo}
}

@misc{duStrongSORTMakeDeepSORT2022,
  title = {{{StrongSORT}}: {{Make DeepSORT Great Again}}},
  shorttitle = {{{StrongSORT}}},
  author = {Du, Yunhao and Zhao, Zhicheng and Song, Yang and Zhao, Yanyun and Su, Fei and Gong, Tao and Meng, Hongying},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2202.13514},
  urldate = {2024-10-23},
  abstract = {Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/YGKFB2NL/Du et al. - 2022 - StrongSORT Make DeepSORT Great Again.pdf}
}

@incollection{kapidisObjectDetectionBasedLocation2020,
  title = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}: {{A Systematic Analysis}}},
  shorttitle = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}},
  booktitle = {Smart {{Assisted Living}}},
  author = {Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas P. J. J. and Veltkamp, Remco C.},
  editor = {Chen, Feng and {Garc{\'i}a-Betances}, Rebeca I. and Chen, Liming and {Cabrera-Umpi{\'e}rrez}, Mar{\'i}a Fernanda and Nugent, Chris},
  year = {2020},
  pages = {119--145},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-25590-9_6},
  urldate = {2024-10-23},
  isbn = {978-3-030-25589-3 978-3-030-25590-9},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/XTFMTBU9/Kapidis et al. - 2020 - Object Detection-Based Location and Activity Class.pdf}
}

@misc{lavechinOpensourceVoiceType2020,
  title = {An Open-Source Voice Type Classifier for Child-Centered Daylong Recordings},
  author = {Lavechin, Marvin and Bousbib, Ruben and Bredin, Herv{\'e} and Dupoux, Emmanuel and Cristia, Alejandrina},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2005.12656},
  urldate = {2024-11-14},
  abstract = {Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages. Our model can be used as input for downstream tasks such as estimating the number of words produced by adult speakers, or the number of linguistic units produced by children. Our architecture combines SincNet filters with a stack of recurrent layers and outperforms by a large margin the state-of-the-art system, the Language ENvironment Analysis (LENA) that has been used in numerous child language studies.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Audio and Speech Processing (eess.AS),FOS: Electrical engineering electronic engineering information engineering,I.2.7}
}

@misc{linBMNBoundaryMatchingNetwork2019,
  title = {{{BMN}}: {{Boundary-Matching Network}} for {{Temporal Action Proposal Generation}}},
  shorttitle = {{{BMN}}},
  author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1907.09702},
  urldate = {2024-11-13},
  abstract = {Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{nunez-marcosEgocentricVisionbasedAction2022,
  title = {Egocentric {{Vision-based Action Recognition}}: {{A}} Survey},
  shorttitle = {Egocentric {{Vision-based Action Recognition}}},
  author = {{N{\'u}{\~n}ez-Marcos}, Adri{\'a}n and Azkune, Gorka and {Arganda-Carreras}, Ignacio},
  year = {2022},
  month = feb,
  journal = {Neurocomputing},
  volume = {472},
  pages = {175--197},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.11.081},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/I5LRFE37/Núñez-Marcos et al. - 2022 - Egocentric Vision-based Action Recognition A surv.pdf}
}

@article{truongCrossviewActionRecognition2024,
  title = {Cross-View Action Recognition Understanding from Exocentric to Egocentric Perspective},
  author = {Truong, Thanh-Dat and Luu, Khoa},
  year = {2024},
  month = oct,
  journal = {Neurocomputing},
  pages = {128731},
  issn = {09252312},
  doi = {10.1016/j.neucom.2024.128731},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/95IM2Q74/Truong and Luu - 2024 - Cross-view action recognition understanding from e.pdf}
}

@misc{wangFasterPersonReIdentification2020,
  title = {Faster {{Person Re-Identification}}},
  author = {Wang, Guan'an and Gong, Shaogang and Cheng, Jian and Hou, Zengguang},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2008.06826},
  urldate = {2024-10-23},
  abstract = {Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a \$F\_{$\beta\$$} score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8\% more accurate but also 5x faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is \$50{\textbackslash}times\$ faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/nelesuffo/Zotero/storage/WG9M6GYH/Wang et al. - 2020 - Faster Person Re-Identification.pdf}
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2024},
  url = {https://www.R-project.org/},
}
@Manual{R-BayesFactor,
  title = {BayesFactor: Computation of Bayes Factors for Common Designs},
  author = {Richard D. Morey and Jeffrey N. Rouder},
  year = {2024},
  note = {R package version 0.9.12-4.7},
  url = {https://CRAN.R-project.org/package=BayesFactor},
}
@Article{R-brms_a,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}
@Article{R-brms_b,
  title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
  author = {Paul-Christian Bürkner},
  journal = {The R Journal},
  year = {2018},
  volume = {10},
  number = {1},
  pages = {395--411},
  doi = {10.32614/RJ-2018-017},
  encoding = {UTF-8},
}
@Article{R-brms_c,
  title = {Bayesian Item Response Modeling in {R} with {brms} and {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2021},
  volume = {100},
  number = {5},
  pages = {1--54},
  doi = {10.18637/jss.v100.i05},
  encoding = {UTF-8},
}
@Manual{R-broom,
  title = {broom: Convert Statistical Objects into Tidy Tibbles},
  author = {David Robinson and Alex Hayes and Simon Couch},
  year = {2024},
  note = {R package version 1.0.7},
  url = {https://CRAN.R-project.org/package=broom},
}
@Article{R-coda,
  title = {CODA: Convergence Diagnosis and Output Analysis for MCMC},
  author = {Martyn Plummer and Nicky Best and Kate Cowles and Karen Vines},
  journal = {R News},
  year = {2006},
  volume = {6},
  number = {1},
  pages = {7--11},
  url = {https://journal.r-project.org/archive/},
  pdf = {https://www.r-project.org/doc/Rnews/Rnews_2006-1.pdf},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
  year = {2023},
  note = {R package version 1.1.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-ggpubr,
  title = {ggpubr: 'ggplot2' Based Publication Ready Plots},
  author = {Alboukadel Kassambara},
  year = {2023},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggpubr},
}
@Manual{R-ggridges,
  title = {ggridges: Ridgeline Plots in 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2024},
  note = {R package version 0.5.6},
  url = {https://CRAN.R-project.org/package=ggridges},
}
@Manual{R-ggthemes,
  title = {ggthemes: Extra Themes, Scales and Geoms for 'ggplot2'},
  author = {Jeffrey B. Arnold},
  year = {2024},
  note = {R package version 5.1.0},
  url = {https://CRAN.R-project.org/package=ggthemes},
}
@Article{R-lubridate,
  title = {Dates and Times Made Easy with {lubridate}},
  author = {Garrett Grolemund and Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {3},
  pages = {1--25},
  url = {https://www.jstatsoft.org/v40/i03/},
}
@Manual{R-Matrix,
  title = {Matrix: Sparse and Dense Matrix Classes and Methods},
  author = {Douglas Bates and Martin Maechler and Mikael Jagan},
  year = {2024},
  note = {R package version 1.7-1},
  url = {https://CRAN.R-project.org/package=Matrix},
}
@Manual{R-papaja,
  title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
  author = {Frederik Aust and Marius Barth},
  year = {2024},
  note = {R package version 0.1.3},
  url = {https://github.com/crsh/papaja},
  doi = {10.32614/CRAN.package.papaja},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Hadley Wickham and Lionel Henry},
  year = {2023},
  note = {R package version 1.0.2},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Article{R-Rcpp_a,
  title = {{Rcpp}: Seamless {R} and {C++} Integration},
  author = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {40},
  number = {8},
  pages = {1--18},
  doi = {10.18637/jss.v040.i08},
}
@Article{R-Rcpp_b,
  title = {{Extending {R} with {C++}: A Brief Introduction to {Rcpp}}},
  author = {Dirk Eddelbuettel and James Joseph Balamuta},
  journal = {The American Statistician},
  year = {2018},
  volume = {72},
  number = {1},
  pages = {28-36},
  doi = {10.1080/00031305.2017.1375990},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Jennifer Bryan},
  year = {2024},
  note = {R package version 2.1.5},
  url = {https://CRAN.R-project.org/package=readr},
}
@Article{R-reshape2,
  title = {Reshaping Data with the {reshape} Package},
  author = {Hadley Wickham},
  journal = {Journal of Statistical Software},
  year = {2007},
  volume = {21},
  number = {12},
  pages = {1--20},
  url = {http://www.jstatsoft.org/v21/i12/},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2023},
  note = {R package version 1.5.1},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2023},
  note = {R package version 3.2.1},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham and Davis Vaughan and Maximilian Girlich},
  year = {2024},
  note = {R package version 1.3.1},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Article{R-tidyverse,
  title = {Welcome to the {tidyverse}},
  author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686},
}
@Manual{R-tinylabels,
  title = {{tinylabels}: Lightweight Variable Labels},
  author = {Marius Barth},
  year = {2023},
  note = {R package version 0.2.4},
  url = {https://cran.r-project.org/package=tinylabels},
}
@Manual{R-readxl,
  title = {readxl: Read Excel Files},
  author = {Hadley Wickham and Jennifer Bryan},
  year = {2023},
  note = {R package version 1.4.3},
  url = {https://CRAN.R-project.org/package=readxl},
}
