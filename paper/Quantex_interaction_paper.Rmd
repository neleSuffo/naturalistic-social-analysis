---
title             : "Exploring Aspects of Social Interaction using Machine Learning"
shorttitle        : "Exploring Aspects of Social Interaction using Machine Learning"
author:
  - name: "Nele-Pauline Suffo"
    affiliation: '1'
    corresponding: true
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "nele.suffo@leuphana.de"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name: "Pierre-Etienne Martin"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "pierre_etienne_martin@eva.mpg.de"
  - name: "Anam Zahra"
    affiliation: '2'
    corresponding: false
  - name: "Daniel Haun"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "daniel.haun@eva.mpg.de"
  - name: "Manuel Bohn"
    affiliation: '1, 2'
    corresponding: false
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "manuel.bohn@leuphana.de"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id: '1'
    institution: "Institute of Psychology in Education, Leuphana University Lüneburg"
  - id: '2'
    institution: "Max Planck Institute for Evolutionary Anthropology"
    
abstract: |
  tbd
output:
  papaja::apa6_pdf:
    keep_tex: true
bibliography: "bibliography.bib"


floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
---


```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(ggplot2)
library(brms)
library(ggthemes)
library(ggpubr)
library(BayesFactor)
library(broom)
library(coda)
library(reshape2)
library(ggridges)
library(readxl)
library(dplyr)
library(lubridate)
library(zoo)
library(gridExtra)
library(grid)
library(kableExtra)
library(cowplot)
library(patchwork)
library(magick)
library(ggplotify)
library(png)
library(jpeg)


estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```

# Introduction


# Methodology
The Quantex dataset includes
```{r quantex-statistics, echo=FALSE, message=FALSE, warning=FALSE}
quantex_data <- read_csv2("data/Quantex_data_sheet.csv")
quantex_subject_infos <- read_csv2("data/Quantex_subjects.csv")
quantex_subject_infos <- quantex_subject_infos %>%
  distinct(ID, .keep_all = TRUE)

quantex_unique_data <- quantex_data %>%
  distinct() %>%  # Removes duplicate rows
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes") %>%
  distinct(ID, Minutes_per_ID, .keep_all = TRUE)
quantex_unique_data <- quantex_unique_data %>%
  mutate(ID = as.double(ID))

quantex_cleaned_data <- quantex_data %>%
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes")
quantex_sum_videos <- nrow(quantex_cleaned_data)

quantex_data_gender_count <- quantex_unique_data %>%
  left_join(quantex_subject_infos, by = "ID") %>%
  distinct(ID, .keep_all = TRUE)
quantex_male_count <- quantex_data_gender_count %>%
  filter(gender == "Male") %>%
  nrow()
quantex_female_count <- quantex_data_gender_count %>%
  filter(gender == "Female") %>%
  nrow()

quantex_data_age_count <- quantex_cleaned_data %>%
  left_join(quantex_subject_infos, by = "ID")%>%
  select(ID, birthday, DATE)

quantex_data_age_count <- quantex_data_age_count %>%
  mutate(
    birthday = dmy(birthday),  # Convert birthday to Date type
    Date = dmy(DATE),          # Convert Date to Date type
    Age = as.numeric(difftime(Date, birthday, units = "weeks")) / 52.25,  # Calculate age in years
    Age_group = case_when(
      Age >= 2 & Age < 4 ~ "3+",
      Age >= 4 & Age < 5 ~ "4+",
      Age >= 5 ~ "5+",
      TRUE ~ NA_character_  # For cases where age is missing or not within the desired ranges
    )
  )

quantex_mean_age <- mean(quantex_data_age_count$Age, na.rm = TRUE)
quantex_sd_age <- sd(quantex_data_age_count$Age, na.rm = TRUE)

quantex_age_group_counts <- quantex_data_age_count %>%
  count(Age_group, name = "count")
quantex_count_3_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "3+"]
quantex_count_4_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "4+"]
quantex_count_5_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "5+"]

quantex_filtered_data <- quantex_data %>%
  filter(!is.na(Date), Include == "yes") %>%
  mutate(Date = as.Date(DATE, format = "%d.%m.%Y")) 

quantex_min_minutes <- min(quantex_unique_data$Minutes_per_ID[quantex_unique_data$Minutes_per_ID > 0], na.rm = TRUE)
quantex_max_minutes <- max(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sum_minutes <- sum(as.numeric(quantex_unique_data$Minutes_per_ID), na.rm = TRUE)
quantex_mean_minutes <- mean(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sd_minutes <- sd(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)

quantex_sum_hours <- quantex_sum_minutes/60

# Convert the time strings into seconds
quantex_unique_data$Seconds_per_ID <- as.numeric(hms::as_hms(quantex_unique_data$Minutes_per_ID))
total_seconds <- sum(quantex_unique_data$Seconds_per_ID, na.rm = TRUE)
total_hours <- total_seconds / 3600

quantex_nr_children <- nrow(quantex_unique_data)

quantex_oldest_date <- min(quantex_filtered_data$Date)
quantex_youngest_date <- max(quantex_filtered_data$Date)

quantex_oldest_year <- as.numeric(format(quantex_oldest_date, "%Y"))
quantex_oldest_month <- as.numeric(format(quantex_oldest_date, "%m"))
quantex_youngest_year <- as.numeric(format(quantex_youngest_date, "%Y"))
quantex_youngest_month <- as.numeric(format(quantex_youngest_date, "%m"))

# Calculate the interval between the two dates
quantex_time_span_months <- (quantex_youngest_year - quantex_oldest_year) * 12 + (quantex_youngest_month - quantex_oldest_month)+1

quantex_nr_annotated_videos <- 100
quantex_nr_train_frames <- 72687
quantex_nr_val_frames <- 7720
quantex_nr_test_frames <- 9272
quantex_nr_train_videos <- 51
quantex_nr_val_videos <- 6
quantex_nr_test_videos <- 7

```

## Dataset Description
### Statistics

```{r quantex-minutes-per-child, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Video recording duration (in minutes) per Child in the Quantex Dataset.", fig.height=2.5, fig.width=3.5}
quantex_unique_data$ID <- as.factor(quantex_unique_data$ID)

ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )
quantex_1_plot <- ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )

# Specify the file path and name
# <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
#file_name_1 <- "quantex_minutes_per_id_plot.png"
#quantex_file_path_1 <- file.path(quantex_1_output_folder, file_name_1)

# Save the plot
#ggsave(filename = quantex_file_path_1, plot = quantex_1_plot, width = 8, height = 6, dpi = 300)
```

### Annotation Strategy
The dataset annotations cover four key elements: persons, faces, gaze direction, objects the child interacts with. Gaze information identifies whether a detected person’s gaze is directed toward the child or not. For every detected person (or reflection of a person, such as in a mirror) and face, additional attributes like age and gender are collected. Faces are annotated even when occluded or blurry to ensure comprehensive coverage of interactions. Partially visible faces are also annotated if key facial features, such as the nose, eyes, or mouth, remain identifiable. Objects are categorized into six distinct groups: book, screen, animal, food, toy, and kitchenware, with an additional category for other objects. The dataset focus is on detecting and labeling instances of (social) interaction and engagement through these key categories. The annotation strategy is displayed in Figure \@ref(fig:camera-cvat-activity-classes).

```{r camera-cvat-activity-classes, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A} – Vest with the embedded camera worn by the children, \\textbf{B} – CVAT platform utilized for video annotation, \\textbf{C} – Annotation Strategy in the Quantex dataset."}
img1 <- ggdraw() + draw_image("images/camera_worn_close.png", scale = 0.8)
img2 <- ggdraw() + draw_image("images/cvat.png", scale = 0.8)
img3 <- ggdraw() + draw_image("images/quantex_annotation_strategy_narrow.png", scale = 0.9)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(0.4, 1))

final_layout <- (top_row / img3) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```


## Data Collection
This study collected egocentric video recordings from `r quantex_nr_children` children, aged 3 to 5 years, over a span of `r quantex_time_span_months` months. Participating families lived in a mid-sized city in Germany. To capture the children’s everyday experiences, a wearable vest equipped with a camera was used, as shown in figure \@ref(fig:camera-cvat-activity-classes). The camera, a _PatrolEyes WiFi HD Infrared Police Body Camera_, provided high-definition video (1920x1080p at 30 fps) with a 140-degree wide-angle lens and also recorded audio. Children were free to move around and engage in their usual activities at home without any interference or instructions given to their parents.

## Data Preprocessing
For the video data, the annotation strategy required persons, faces, and objects to be labeled even when only partially visible, as long as key features such as facial landmarks (e.g., nose, eye, or mouth) or parts of a person or object were clearly visible. Frames that were too blurry due to movement were marked as “noise” and excluded from further analysis. Additionally, frames where the child was not wearing the camera, as well as any scenes containing nudity, were also labele d as noise and removed from the dataset. To prepare the video data for analysis, one frame per second was annotated, corresponding to every 30th frame in the video. Similarly, every 30th raw frame was extracted from the annotated video files. No preprocessing was applied to the audio data, which was used in its raw form for analysis.

## Automated Analysis Pipeline

### Person Detection



### Face Detection


```{r face-detection-metrics, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Dataset | Precision | Recall | F1-Score
            Quantex | 0.90   | 0.83 | 0.86"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11 face detection model trained on the Quantex dataset.",
  escape = TRUE
)
```
	




### Gaze Classification

### Voice Detection and Classification

## Feature Extraction




# Results



## Presence of Aspects of Social Interaction

### Presence of a Person


### Presence of a Face

### Presence of Gaze Directed at the Child

### Presence of Language
## Co-occurrence of Aspects of Social Interaction 




# General Discussion

\newpage
# References

\newpage
# Supplementary Material
## Person Detection: Model Selection, Data Preprocessing, and Performance Evaluation
```{r yolo-person-statistics, echo=FALSE, message=FALSE, warning=FALSE}
person_total_num_frames <- 113799
person_total_num_videos <- 80
person_num_frames_train <- 91038
person_class_to_total_ratio <- 45.25

num_person_frames_train_with_persons <- 41192
num_person_frames_train_balanced <- 82384
num_person_frames_val <- 11379
num_person_frames_test <- 11382

person_cm_num_images <- 11382
person_tp <- 1905
person_fp <- 251
person_fn <- 234
person_tn <- person_cm_num_images - person_tp - person_fp - person_fn
person_fpr <- person_fp / (person_fp + person_tn) * 100
num_person <- person_fp + person_fn
per_wrong_person <- person_fp / person_tp * 100
person_recall <- 0.87
person_precision <- 0.92
person_f1_score <- 0.90

person_num_epochs <- 86
```

In this study, we utilized Ultralytics’ YOLO11 [@jocherUltralyticsYOLO112024], a state-of-the-art object detection model recognized for its efficiency and accuracy. Initially, we experimented with Multi-Task Cascaded Convolutional Networks (MTCNN) face detection algorithm [@zhangJointFaceDetection2016]; however, it achieved a recall of less than 50% on our dataset. Furthermore, the MTCNN model demands very specific picture input for fine-tuning, which is time-consuming. In contrast, YOLO11, released in October 2024, introduces new features such as the C2PSA block, which enhances spatial attention within feature maps, allowing the model to focus more precisely on critical areas of an image. Additionally, YOLO11 incorporates the C3K2 block, designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process [@khanamYOLOv11OverviewKey2024]. Moreover, we already had a data preparation pipeline established for YOLO11 due to its application in person detection within our project. Consequently, we selected YOLO11 for face detection and fine-tuned it on our egocentric dataset, captured using chest-mounted cameras, to adapt it to the unique characteristics of our data.

### Dataset Splitting and Balancing
We started data preprocessing with a dataset comprising a total of `r person_total_num_frames` images from `r person_total_num_videos` annotated videos.Prior to partitioning this dataset into training, validation, and testing subsets, we analyzed the proportion of images containing annotated faces versus those without. Our analysis revealed that `r person_class_to_total_ratio`% of the images included at least one annotated face. To maintain this inherent distribution across all subsets, we initially applied a stratified split, ensuring that each subset—training, validation, and testing—preserved the original 19% to 81% ratio of images with faces to images without faces.

However, this stratified split resulted in a significant class imbalance within the training set, which could adversely affect the model’s learning process. In imbalanced datasets, models tend to be biased toward the majority class, often predicting it more frequently while misclassifying or overlooking minority class instances. This can lead to poor recall for the minority class [@hasaninSeverelyImbalancedBig2019]. Additionally, Yolo11's gradient-based learning algorithm struggles to adjust decision boundaries effectively when trained on imbalanced data, potentially causing slow or unstable convergence and requiring extensive hyperparameter tuning [@kaurSystematicReviewImbalanced2020].

To mitigate this issue, we employed an undersampling technique on the training data. Specifically, we identified the number of images containing faces in the training set (`r num_person_frames_train_with_persons` frames) and randomly sampled an equal number of images from the non-face category. This approach balanced the training dataset to consist of 50% images with faces and 50% images without faces, thereby addressing the class imbalance and facilitating more effective model training.

Consequently, the final data distribution was as follows: the balanced training dataset comprised `r num_person_frames_train_balanced` frames, while the validation and test datasets contained `r num_person_frames_val` and `r num_person_frames_test` frames, respectively. Notably, the validation and test sets retained the original 19% face presence, ensuring that the model’s performance evaluation remained representative of the real-world data distribution.

### Training and Convergence
Model training was conducted on a Linux server equipped with an Intel(R) Xeon(R) Silver 4214Y CPU @ 2.20GHz with 48 cores, a Quadro RTX 8000 GPU and 188 GB of RAM. The model was trained for a total of `r person_num_epochs` epochs. The training process utilized YOLO11’s built-in data augmentation, a batch size of 16, a cosine annealing learning rate scheduler, and early stopping after 10 epochs without improvement, with a maximum of 200 epochs. 

The loss function of the YOLOv11 model comprises three main components: Box Loss, Classification Loss, and Distribution Focal Loss (DFL). Box Loss estimates the difference between predicted bounding boxes and ground truth boxes to assess the model's localization accuracy. Classification Loss measures the model's ability to properly identify detected objects; however, in our study, this component is less relevant due to our focus on a single class—faces. DFL improves the model's ability to detect challenging objects by prioritizing difficult-to-detect instances.

During the training process, we observed that all three loss components decreased over time, indicating effective learning and improved performance, as visible in in figure \@ref(fig:face-loss-curves). A steady decrease in Box Loss indicates that the model is becoming increasingly accurate in localizing faces within images. This is consistent with the rapid convergence of Classification Loss, revealing the model's ability to reliably recognize faces. The decrease in DFL over time indicates that the model is getting better at focusing on and correctly identifying difficult-to-detect faces, which improves its overall detection capabilities. These data collectively represent the model's gradual improvement in both localization and identification tasks during the training period.

```{r person-loss-curves, fig.align='center', fig.cap="Training and Validation Loss Curves for the YOLO11 face detection model.", out.width="450px"}
knitr::include_graphics("images/yolo_face_loss_curves.png")
```

```{r person-detection-metrics-detailed, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Dataset | Precision | Recall | F1-Score | False Positive Rate | False Negative Rate
            Quantex | 0.90   | 0.83 | 0.86 | 2.1 | 14.0"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11 face detection model trained on the Quantex dataset. False Positive Rate and False Negative Rate are given in percentages.",
  escape = TRUE
)
```

```{r person-metrics, fig.align='center', fig.cap="\\textbf{A} - Confusion Matrix for the YOLO11 face detection model trained on the Quantex dataset. \\textbf{B} - Precision-Recall Curve for the YOLO11 face detection model."}
# Load images
img1 <- ggdraw() + draw_image("images/yolo_face_confusion_matrix.png", scale = 1)
img2 <- ggdraw() + draw_image("images/yolo_face_pr_curve.png", scale = 1.2)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row) +
  plot_layout(heights = c(1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

### Model Evaluation Metrics
The YOLO11 model achieved a precision of `r person_precision` and a recall of `r person_recall` on the testing set, resulting in an F1-score of `r person_f1_score`. These metrics, summarized in figure \@ref(fig:face-detection-metrics-detailed) indicate the model’s strong performance in accurately identifying faces while minimizing errors. The precision-recall curve, displayed in figure \@ref(fig:person-metrics), further illustrates this performance, with the curve remaining close to the top-left corner. This positioning signifies that the model maintains high precision and recall across various thresholds, underscoring its effectiveness in detecting faces with confidence.

Analysis of the confusion matrix reveals that 86% of all faces are correctly identified by the model, corresponding to `r person_tp` true positives, while `r person_fn` faces were missed (false negatives). False negatives predominantly occurred in scenarios where faces were in the background, blurred due to motion, or occluded by the child’s body. In such instances, adjacent frames often provided clearer views, aiding in more accurate classification. The model exhibited a false positive rate of approximately `r person_fpr`%, with `r person_fp` images incorrectly classified as containing faces when none were present. These false positives were often attributed to objects or toys resembling facial features. Given the relatively low false positive rate, this small number should not raise significant concerns. In face detection systems, a balance between false positives and false negatives is often necessary, and a 2.1% false positive rate is generally considered acceptable. 

To provide a comprehensive understanding of the model’s performance, we have included visual examples of true positives, false positives, and false negatives in figure \@ref(fig:person-detection-examples). These images highlight the model’s strengths and areas where challenges persist, offering insights into specific scenarios that influence detection accuracy.

Overall, the YOLO11 model demonstrates robust performance in face detection tasks. However, challenges remain in dynamic scenarios, particularly with partially visible, rotated, or side-view faces. These findings underscore the complexities inherent in analyzing egocentric video data, where movement and varying perspectives introduce additional challenges.

```{r person-detection-examples, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A}, \\textbf{B} - Examples of True Positives, \\textbf{C}, \\textbf{D} – Examples of False Negatives, \\textbf{E}, \\textbf{F} – Examples of False Positives in the YOLO11 face detection model."}
img1 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img5 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img6 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
middle_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
bottom_row <- (img5 + img6) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row / middle_row / bottom_row) +
  plot_layout(heights = c(1, 1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

## Face Detection: Model Selection, Data Preprocessing, and Performance Evaluation
```{r yolo-face-statistics, echo=FALSE, message=FALSE, warning=FALSE}
total_num_face_frames <- 91706
face_total_num_videos <- 64
num_face_frames_train <- 73364
face_class_to_total_ratio <- 18.69

num_face_frames_train_with_faces <- 13708
num_face_frames_train_balanced <- 27416
num_face_frames_val <- 9169
num_face_frames_test <- 9173

face_cm_num_images <- 9173
face_tp <- 1905
face_fp <- 251
face_fn <- 234
face_tn <- face_cm_num_images - face_tp - face_fp - face_fn
face_fpr <- face_fp / (face_fp + face_tn) * 100
num_faces <- face_fp + face_fn
per_wrong_faces <- face_fp / face_tp * 100
face_recall <- 0.87
face_precision <- 0.92
face_f1_score <- 0.90

face_num_epochs <- 86
```

In this study, we utilized Ultralytics’ YOLO11 [@jocherUltralyticsYOLO112024], a state-of-the-art object detection model recognized for its efficiency and accuracy. Initially, we experimented with Multi-Task Cascaded Convolutional Networks (MTCNN) face detection algorithm [@zhangJointFaceDetection2016]; however, it achieved a recall of less than 50% on our dataset. Furthermore, the MTCNN model demands very specific picture input for fine-tuning, which is time-consuming. In contrast, YOLO11, released in October 2024, introduces new features such as the C2PSA block, which enhances spatial attention within feature maps, allowing the model to focus more precisely on critical areas of an image. Additionally, YOLO11 incorporates the C3K2 block, designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process [@khanamYOLOv11OverviewKey2024]. Moreover, we already had a data preparation pipeline established for YOLO11 due to its application in person detection within our project. Consequently, we selected YOLO11 for face detection and fine-tuned it on our egocentric dataset, captured using chest-mounted cameras, to adapt it to the unique characteristics of our data.

### Dataset Splitting and Balancing
We started data preprocessing with a dataset comprising a total of `r total_num_face_frames` images from `r face_total_num_videos` annotated videos.Prior to partitioning this dataset into training, validation, and testing subsets, we analyzed the proportion of images containing annotated faces versus those without. Our analysis revealed that 19% of the images included at least one annotated face. To maintain this inherent distribution across all subsets, we initially applied a stratified split, ensuring that each subset—training, validation, and testing—preserved the original 19% to 81% ratio of images with faces to images without faces.

However, this stratified split resulted in a significant class imbalance within the training set, which could adversely affect the model’s learning process. In imbalanced datasets, models tend to be biased toward the majority class, often predicting it more frequently while misclassifying or overlooking minority class instances. This can lead to poor recall for the minority class [@hasaninSeverelyImbalancedBig2019]. Additionally, Yolo11's gradient-based learning algorithm struggles to adjust decision boundaries effectively when trained on imbalanced data, potentially causing slow or unstable convergence and requiring extensive hyperparameter tuning [@kaurSystematicReviewImbalanced2020].

To mitigate this issue, we employed an undersampling technique on the training data. Specifically, we identified the number of images containing faces in the training set (`r num_face_frames_train_with_faces` frames) and randomly sampled an equal number of images from the non-face category. This approach balanced the training dataset to consist of 50% images with faces and 50% images without faces, thereby addressing the class imbalance and facilitating more effective model training.

Consequently, the final data distribution was as follows: the balanced training dataset comprised `r num_face_frames_train_balanced` frames, while the validation and test datasets contained `r num_face_frames_val` and `r num_face_frames_test` frames, respectively. Notably, the validation and test sets retained the original 19% face presence, ensuring that the model’s performance evaluation remained representative of the real-world data distribution.

### Training and Convergence
Model training was conducted on a Linux server equipped with an Intel(R) Xeon(R) Silver 4214Y CPU @ 2.20GHz with 48 cores, a Quadro RTX 8000 GPU and 188 GB of RAM. The model was trained for a total of `r face_num_epochs` epochs. The training process utilized YOLO11’s built-in data augmentation, a batch size of 16, a cosine annealing learning rate scheduler, and early stopping after 10 epochs without improvement, with a maximum of 200 epochs. 

The loss function of the YOLOv11 model comprises three main components: Box Loss, Classification Loss, and Distribution Focal Loss (DFL). Box Loss estimates the difference between predicted bounding boxes and ground truth boxes to assess the model's localization accuracy. Classification Loss measures the model's ability to properly identify detected objects; however, in our study, this component is less relevant due to our focus on a single class—faces. DFL improves the model's ability to detect challenging objects by prioritizing difficult-to-detect instances.

During the training process, we observed that all three loss components decreased over time, indicating effective learning and improved performance, as visible in in figure \@ref(fig:face-loss-curves). A steady decrease in Box Loss indicates that the model is becoming increasingly accurate in localizing faces within images. This is consistent with the rapid convergence of Classification Loss, revealing the model's ability to reliably recognize faces. The decrease in DFL over time indicates that the model is getting better at focusing on and correctly identifying difficult-to-detect faces, which improves its overall detection capabilities. These data collectively represent the model's gradual improvement in both localization and identification tasks during the training period.

```{r face-loss-curves, fig.align='center', fig.cap="Training and Validation Loss Curves for the YOLO11 face detection model.", out.width="450px"}
knitr::include_graphics("images/yolo_face_loss_curves.png")
```

```{r face-detection-metrics-detailed, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Dataset | Precision | Recall | F1-Score | False Positive Rate | False Negative Rate
            Quantex | 0.90   | 0.83 | 0.86 | 2.1 | 14.0"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11 face detection model trained on the Quantex dataset. False Positive Rate and False Negative Rate are given in percentages.",
  escape = TRUE
)
```

```{r face-metrics, fig.align='center', fig.cap="\\textbf{A} - Confusion Matrix for the YOLO11 face detection model trained on the Quantex dataset. \\textbf{B} - Precision-Recall Curve for the YOLO11 face detection model."}
# Load images
img1 <- ggdraw() + draw_image("images/yolo_face_confusion_matrix.png", scale = 1)
img2 <- ggdraw() + draw_image("images/yolo_face_pr_curve.png", scale = 1.2)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row) +
  plot_layout(heights = c(1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

### Model Evaluation Metrics
The YOLO11 model achieved a precision of `r face_precision` and a recall of `r face_recall` on the testing set, resulting in an F1-score of `r face_f1_score`. These metrics, summarized in figure \@ref(fig:face-detection-metrics-detailed) indicate the model’s strong performance in accurately identifying faces while minimizing errors. The precision-recall curve, displayed in figure \@ref(fig:face-metrics), further illustrates this performance, with the curve remaining close to the top-left corner. This positioning signifies that the model maintains high precision and recall across various thresholds, underscoring its effectiveness in detecting faces with confidence.

Analysis of the confusion matrix reveals that 86% of all faces are correctly identified by the model, corresponding to `r face_tp` true positives, while `r face_fn` faces were missed (false negatives). False negatives predominantly occurred in scenarios where faces were in the background, blurred due to motion, or occluded by the child’s body. In such instances, adjacent frames often provided clearer views, aiding in more accurate classification. The model exhibited a false positive rate of approximately `r face_fpr`%, with `r face_fp` images incorrectly classified as containing faces when none were present. These false positives were often attributed to objects or toys resembling facial features. Given the relatively low false positive rate, this small number should not raise significant concerns. In face detection systems, a balance between false positives and false negatives is often necessary, and a 2.1% false positive rate is generally considered acceptable. 

To provide a comprehensive understanding of the model’s performance, we have included visual examples of true positives, false positives, and false negatives in figure \@ref(fig:face-detection-examples). These images highlight the model’s strengths and areas where challenges persist, offering insights into specific scenarios that influence detection accuracy.

Overall, the YOLO11 model demonstrates robust performance in face detection tasks. However, challenges remain in dynamic scenarios, particularly with partially visible, rotated, or side-view faces. These findings underscore the complexities inherent in analyzing egocentric video data, where movement and varying perspectives introduce additional challenges.

```{r face-detection-examples, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A}, \\textbf{B} - Examples of True Positives, \\textbf{C}, \\textbf{D} – Examples of False Negatives, \\textbf{E}, \\textbf{F} – Examples of False Positives in the YOLO11 face detection model."}
img1 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img5 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img6 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
middle_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
bottom_row <- (img5 + img6) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row / middle_row / bottom_row) +
  plot_layout(heights = c(1, 1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```


\newpage
# References
<!--e used `r cite_r("bibliography.bib")` for all our analyses.-->    
```{r create_r-references}
r_refs(file = "bibliography.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage
# Appendix





