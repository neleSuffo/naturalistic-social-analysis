---
title             : "Exploring Aspects of Social Interaction using Machine Learning"
shorttitle        : "Exploring Aspects of Social Interaction using Machine Learning"
author:
  - name: "Nele-Pauline Suffo"
    affiliation: '1'
    corresponding: true
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "nele.suffo@leuphana.de"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name: "Pierre-Etienne Martin"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "pierre_etienne_martin@eva.mpg.de"
  - name: "Anam Zahra"
    affiliation: '2'
    corresponding: false
  - name: "Daniel Haun"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "daniel.haun@eva.mpg.de"
  - name: "Manuel Bohn"
    affiliation: '1, 2'
    corresponding: false
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "manuel.bohn@leuphana.de"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id: '1'
    institution: "Institute of Psychology in Education, Leuphana University Lüneburg"
  - id: '2'
    institution: "Max Planck Institute for Evolutionary Anthropology"
    
abstract: |
  Childrens everyday experiences are known to shape childrens development but only few studies investigate how children actually spent their time at home in naturalistic setting. More particular we were interested in how children's social interactions with others or interactions with objects are observable in their everyday life. To do so, we utilized the Quantex Dataset, an egocentric video and audio dataset of children aged 3-5 years, to investigate the presence of persons, faces, gaze, and objects in children's everyday interactions. We trained a YOLO11 model to detect persons and faces in the videos and analyzed the presence of gaze and objects in the videos. We furthermore applied a pre-trained voice type classifier to detect speech in the audio data. Our results show that children's everyday interactions are characterized by the presence of persons and faces, with the child's gaze directed towards others in 60% of the interactions. Additionally, children interacted with objects in 40% of the videos, with toys being the most common object category. Agr group analysis revealed that children aged 3 years showed more interactions with objects compared to older children. Our findings provide insights into the diversity of children's everyday experiences and highlight the importance of multimodal data for understanding children's social interactions and engagement.
  
keywords: "Quantex Dataset, egocentric video, audio dataset, children, social interactions, object interactions, gaze, multimodal data, computer vision, audio analysis, developmental psychology"
output:
  papaja::apa6_pdf:
    keep_tex: true
    extra_dependencies: ["colortbl", "booktabs", "array", "multirow"]
bibliography: "bibliography.bib"


floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
---


```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(ggplot2)
library(brms)
library(ggthemes)
library(ggpubr)
library(BayesFactor)
library(broom)
library(coda)
library(reshape2)
library(ggridges)
library(readxl)
library(dplyr)
library(lubridate)
library(zoo)
library(gridExtra)
library(grid)
library(kableExtra)
library(cowplot)
library(patchwork)
library(magick)
library(ggplotify)
library(png)
library(jpeg)
library(lme4)
library(data.table)
library(broom)
library(brms)


estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```
```{r quantex-statistics, echo=FALSE, message=FALSE, warning=FALSE}
quantex_data <- read_csv2("data/Quantex_data_sheet.csv")
quantex_subject_infos <- read_csv2("data/Quantex_subjects.csv")
quantex_subject_infos <- quantex_subject_infos %>%
  distinct(ID, .keep_all = TRUE)

quantex_unique_data <- quantex_data %>%
  distinct() %>%  # Removes duplicate rows
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes") %>%
  distinct(ID, Minutes_per_ID, .keep_all = TRUE)
quantex_unique_data <- quantex_unique_data %>%
  mutate(ID = as.double(ID))

quantex_cleaned_data <- quantex_data %>%
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes")
quantex_sum_videos <- nrow(quantex_cleaned_data)

quantex_data_gender_count <- quantex_unique_data %>%
  left_join(quantex_subject_infos, by = "ID") %>%
  distinct(ID, .keep_all = TRUE)
quantex_male_count <- quantex_data_gender_count %>%
  filter(gender == "Male") %>%
  nrow()
quantex_female_count <- quantex_data_gender_count %>%
  filter(gender == "Female") %>%
  nrow()

quantex_data_age_count <- quantex_cleaned_data %>%
  left_join(quantex_subject_infos, by = "ID")%>%
  select(ID, birthday, DATE)

quantex_data_age_count <- quantex_data_age_count %>%
  mutate(
    birthday = dmy(birthday),  # Convert birthday to Date type
    Date = dmy(DATE),          # Convert Date to Date type
    Age = as.numeric(difftime(Date, birthday, units = "weeks")) / 52.25,  # Calculate age in years
    Age_group = case_when(
      Age >= 2 & Age < 4 ~ "3+",
      Age >= 4 & Age < 5 ~ "4+",
      Age >= 5 ~ "5+",
      TRUE ~ NA_character_  # For cases where age is missing or not within the desired ranges
    )
  )

quantex_mean_age <- mean(quantex_data_age_count$Age, na.rm = TRUE)
quantex_sd_age <- sd(quantex_data_age_count$Age, na.rm = TRUE)

quantex_age_group_counts <- quantex_data_age_count %>%
  count(Age_group, name = "count")
quantex_count_3_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "3+"]
quantex_count_4_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "4+"]
quantex_count_5_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "5+"]

quantex_filtered_data <- quantex_data %>%
  filter(!is.na(Date), Include == "yes") %>%
  mutate(Date = as.Date(DATE, format = "%d.%m.%Y")) 

quantex_min_minutes <- min(quantex_unique_data$Minutes_per_ID[quantex_unique_data$Minutes_per_ID > 0], na.rm = TRUE)
quantex_max_minutes <- max(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sum_minutes <- sum(as.numeric(quantex_unique_data$Minutes_per_ID), na.rm = TRUE)
quantex_mean_minutes <- mean(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sd_minutes <- sd(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)

quantex_sum_hours <- quantex_sum_minutes/60

# Convert the time strings into seconds
quantex_unique_data$Seconds_per_ID <- as.numeric(hms::as_hms(quantex_unique_data$Minutes_per_ID))
total_seconds <- sum(quantex_unique_data$Seconds_per_ID, na.rm = TRUE)
total_hours <- total_seconds / 3600

quantex_nr_children <- nrow(quantex_unique_data)

quantex_oldest_date <- min(quantex_filtered_data$Date)
quantex_youngest_date <- max(quantex_filtered_data$Date)

quantex_oldest_year <- as.numeric(format(quantex_oldest_date, "%Y"))
quantex_oldest_month <- as.numeric(format(quantex_oldest_date, "%m"))
quantex_youngest_year <- as.numeric(format(quantex_youngest_date, "%Y"))
quantex_youngest_month <- as.numeric(format(quantex_youngest_date, "%m"))

# Calculate the interval between the two dates
quantex_time_span_months <- (quantex_youngest_year - quantex_oldest_year) * 12 + (quantex_youngest_month - quantex_oldest_month)+1

quantex_num_videos <- 634 # Total number of videos in the dataset
quantex_num_frames <- 19023571 # Total number of frames in the dataset
```

```{r yolo-person-statistics, echo=FALSE, message=FALSE, warning=FALSE}
 det_total_num_frames <- 113799
 det_total_num_videos <- 80
 det_num_frames_train <- 91038
 det_class_to_total_ratio <- 29.75
 
 num_det_frames_train <- 91039
 num_det_frames_val <- 11380
 num_det_frames_test <- 11380
 
 det_recall <- 0.79
 det_precision <- 0.89
 det_f1_score <- 0.84
 det_map <- 0.858
 det_recall <- 0.80
 det_precision <- 0.91
 det_f1_score <- 0.85
 det_map <- 0.870
 
 tp_child <- 0.81
 tp_adult <- 0.91
 tp_child_face <- 0.82
 tp_adult_face <- 0.91
 tp_child_body_parts <- 0.94
 tp_book <- 0.9
 tp_screen <- 0.93
 tp_toy <- 0.76
 tp_kitchenware <- 0.75
 tp_other_object <- 0.79
 
 fn_kitchenware <- 0.22
 fn_toy <- 0.22
 fn_other_object <- 0.19
 
 ap_toy <- 0.77
 ap_kitchenware <- 0.77
 ap_other_object <- 0.82
 
 det_num_epochs <- 86
 det_training_time <- 200
```

# Introduction 
According to various developmental psychologists, children’s everyday experiences play a vital role in their development [@piagetPartCognitiveDevelopment1964; @vygotskyMindSocietyDevelopment1978; @rogoffImportanceUnderstandingChildrens2018; @carpendaleWhatMakesUs2020; @smithDevelopingInfantCreates2018; @tomaselloCulturalOriginsHuman2009; @heyesCognitiveGadgetsCultural2018]. Everyday interactions, in particular, have been recognized for decades as crucial in the process of actively constructing knowledge [@piagetPartCognitiveDevelopment1964] and in transforming sensory experiences into structured understanding [@vygotskyMindSocietyDevelopment1978]. Building upon these foundational theories, more recent research has examined the mechanisms of social interaction further. For instance, @tomaselloCulturalOriginsHuman2009 introduced the concept of shared intentionality, illustrating how collaborative activities enable children to comprehend others’ intentions and perspectives, leading to cooperative behaviors and cultural learning .

Whereas theoretical frameworks and controlled laboratory studies have significantly advanced our understanding of children’s social development, they often fail to capture the complexities of interactions occurring in naturalistic settings. Observing children in their everyday environments offers a more authentic view of their social behaviors; however, this approach presents challenges due to the extensive data collection and analysis required.

To address these challenges, researchers have increasingly turned to data-driven approaches that utilize sensors and recording devices to gather objective data on social interactions. For instance, @onnelaUsingSociometersQuantify2014 employed wearable sensors to analyze social interactions in adult work settings, capturing the duration of close proximity between individuals. The study inferred that women were more talkative than men and more likely to be physically close to other women in group settings. @rossanoHow24yearold2022 examined social interactions among 31 two- to four-year-olds using 563 hours of video and audio recordings from a preschool during free play sessions over seven days. Manual interaction labels revealed that four-year-olds engaged in more cooperative social interactions and experienced fewer conflicts than two-year-olds, with object play and conversations being the most common forms of social engagement in both age groups. @daiLongitudinalDataCollection2022 investigated social interactions of 174 preschool children over three years, collecting voice and proximity data using wearable wireless RFID tags to study the co-development of social interactions and language acquisition. They employed manually labeled interaction data to train a temporal segment model that automatically identified periods of free play or class play, concluding that classmates frequently engaged in both contexts. @lemaignanPInSoRoDatasetSupporting2018 created a dataset comprising 45 hours of manually labeled social interactions between 45 child-child pairs and 30 child-robot pairs, including video and audio recordings, 3D facial data, skeletal information, and game interactions. By not providing specific instructions to the children, the researchers aimed to capture interactions in naturalistic settings. However, each laboratory session was limited to 40 minutes.

While these studies have advanced our understanding of social interactions, they often focus on controlled environments or are constrained by limited observation periods. Moreover, the manual data collection and analysis involved remain labor-intensive and time-consuming, and the current body of research lacks comprehensive data-driven studies analyzing children’s social interactions within their home environments.

The present study investigates social interactions in naturalistic home settings over an extended period. The corresponding **Quantex** dataset currently includes  `r quantex_sum_hours` hours of egocentric video and audio recordings from children aged 3 to 5 years. Here we focus on specific patterns of social interactions, including:

- **Presence of Individuals**: Utilizing YOLO11 for person detection to identify when others are present in the child’s environment.
- **Presence of Faces**: Employing YOLO11 face detection to recognize faces the child encounters.
- **Object Interactions**: Analyzing the objects with which the child interacts using YOLO11 object detection
- **Gaze Behaviors**: Classifying gaze direction with YOLO11-cls to determine when others are looking at the child.
- **Person Proximity**: Estimating the distance between the child and others to quantify social engagement.
- **Speech Dynamics**: Implementing voice type classification to differentiate between the child’s speech and that of others, distinguishing between peers and adults.
	
The primary objectives of this study are to quantify interaction patterns by measuring the frequency of each identified interaction type, both individually and in combination. Additionaly, we compare these patterns across different age groups within the 3 to 5-year range to identify developmental variations and milestones. Understanding these interaction patterns can inform developmental psychology about the actual nature of social interactions in children’s everyday lives.

# Methodology {#methodology}
The following sections provide a detailed description of the data collection process, the structure and characteristics of the dataset, the annotation strategy, and the preprocessing applied to the data prior to analysis. Additionally, an overview of the automated analysis pipeline is provided, giving details about the models used for person, face and object detection, gaze classification, and the application of a pre-trained voice type classifier. 

## Participants Recruitment and Data Collection {#participants}
This study collected egocentric video recordings from `r quantex_nr_children` children, aged 3 to 5 years, over a span of `r quantex_time_span_months` months [MB: in addition, it would be interesting to know in what time intervall the videos were collected for each child, also a more detailed overview of the number of children per age group would be nice]. 

### Recruitment Process
Children were recruited from a mid-sized German city via an internal lab database, ensuring balanced age distribution. The study was approved by the ethics committee, and all families provided written consent. Parents received detailed study information via phone, and families agreeing to participate were given a vest with an embedded camera, which children were asked to wear for approximately two hours. Recordings were restricted to the home for privacy and securely stored on the MPI-EVA server. Parents could receive the camera via home delivery or self-pickup.

### Distribution of Study Materials and Instructions
At the handover, parents received one or two vests (various sizes), a charged camera unit, and detailed study/data protection information. They were trained on camera operation and encouraged to practice. The consent form listed the participating child and potential individuals captured. Parents were given five privacy information sheets to distribute as needed. A follow-up call was scheduled to ensure recording success, typically planned for the following weekend.

### Return of Equipment and Final Data Confirmation
Parents were contacted to confirm recording success, with flexible rescheduling if needed. Once completed, a pickup/drop-off appointment was arranged to collect the camera, vest, and consent form. Parents confirmed their email for receiving recordings. As a thank-you, children received a Labyrinth game.

## Materials {#materials}
To capture children’s everyday experiences, a wearable vest equipped with a _PatrolEyes WiFi HD Infrared Police Body Camera_ was used (see Figure \@ref(fig:camera-cvat-activity-classes)). The camera recorded high-definition video (1920x1080p at 30 fps) with a 140-degree wide-angle lens and also captured audio. Children were free to move around and engage in their usual activities at home without any interference or instructions given to their parents. 

## Dataset Overview {#dataset}
The Quantex dataset includes video and audio recordings from `r quantex_nr_children` children aged 3 to 5 years (M=`r quantex_mean_age`, SD=`r quantex_sd_age`). The dataset contains `r quantex_count_3_plus` videos from three-year-olds, `r quantex_count_4_plus` videos from four-year-olds, and `r quantex_count_5_plus` videos from five-year-olds. The number of videos per child varies, as parents decide when and how often to record. The recording duration per child ranges from `r quantex_min_minutes` to `r quantex_max_minutes` minutes (M=`r quantex_mean_minutes`, SD=`r quantex_sd_minutes`). The total duration of all video recordings in the dataset is `r quantex_sum_hours` hours. Figure \@ref(fig:quantex-minutes-per-child) shows the distribution of video duration per child.

```{r quantex-minutes-per-child, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Video recording duration (in minutes) per Child in the Quantex Dataset.", fig.height=2.5, fig.width=3.5}
quantex_unique_data$ID <- as.factor(quantex_unique_data$ID)

ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )
quantex_1_plot <- ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )

# Specify the file path and name
# <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
#file_name_1 <- "quantex_minutes_per_id_plot.png"
#quantex_file_path_1 <- file.path(quantex_1_output_folder, file_name_1)

# Save the plot
#ggsave(filename = quantex_file_path_1, plot = quantex_1_plot, width = 8, height = 6, dpi = 300)
```

## Annotation Strategy {#annotation-strategy}
The dataset annotations cover four key elements: persons, faces, gaze direction, and objects. Each detected person (or reflection) and face includes attributes like a unique ID, age, and gender. Gaze annotations indicate whether a person is looking at the child. Faces are annotated even if occluded or blurry, as long as key features remain visible.

The egocentric chest-mounted camera introduced challenges like motion blur and oblique angles, making gaze classification difficult, even for human annotators. Many frames had occluded or distorted gaze information. Objects are annotated only when actively interacted with and categorized into seven groups: book, screen, animal, food, toy, kitchenware, and other. The annotation strategy is summarized in Figure \@ref(fig:camera-cvat-activity-classes).

The Quantex dataset includes `r quantex_num_videos` videos, with `r det_total_num_videos` annotated, totaling `r det_total_num_frames` frames. Every 30th frame (one per second) was annotated to balance workload while ensuring meaningful data. These annotations serve as ground truth for training a model to analyze the remaining videos. While video data underwent structured annotation and validation, audio data was analyzed in raw form.

```{r camera-cvat-activity-classes, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A} – Vest with the embedded camera worn by the children, \\textbf{B} – CVAT platform utilized for video annotation, \\textbf{C} – Annotation Strategy in the Quantex dataset."}
img1 <- ggdraw() + draw_image("images/camera_worn_close.png", scale = 0.8)
img2 <- ggdraw() + draw_image("images/cvat.png", scale = 0.8)
img3 <- ggdraw() + draw_image("images/quantex_annotation_strategy_narrow.png", scale = 0.9)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(0.4, 1))

final_layout <- (top_row / img3) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```
# Automated Analysis Pipeline {#automatic-analysis}
Our automated analysis pipeline consists of four key modules: detection of person, faces, and objects, gaze classification, proximity heuristic, voice type classification. Each module operates independently, utilizing separate machine learning models. Except for the voice type classifier, all models were trained on the Quantex dataset.

The pipeline follows a sequential process:

- YOLO11x detection model identifies the presence of individuals (persons and faces) and types of objects the key child interacts with, both in social and independent play contexts, in the video frames.
- Gaze classification determines whether detected faces are looking at the child.
- Proximity heuristic estimates the distance between the child and others based on face size and aspect ratio.
- Voice type classification detects the presence of speech and identifies whether the speaker is the key child, another child, or an adult.

By integrating these modules, our pipeline enables a comprehensive analysis of children’s everyday experiences, capturing both social interactions and independent play.

In the following sections, we describe each module in detail, including training data, model architecture, and evaluation metrics. A full technical analysis of each algorithm is provided in the [Supplementary Material].

## Yolo11x: Multi-Class Detection of Persons, Faces, and Objects {#yolo11x}
We used Ultralytics’ YOLO11, a "state-of-the-art real-time object detector" [@jocherUltralyticsYOLO112024], pretrained on COCO [@linMicrosoftCOCOCommon2014]. Released in October 2024, YOLO11 introduces C2PSA and C3K2 blocks for enhanced spatial attention and feature aggregation [@khanamYOLOv11OverviewKey2024]. Given our egocentric dataset’s dynamic nature, we selected YOLO11x, the largest model (56.9M parameters, 194.9 GFLOPs), offering the highest accuracy (mAP$^{val}_{50-95}$ = 54.7) among YOLO11 variants.

### Dataset Annotation and Preprocessing
Egocentric recordings often include the child’s body parts, so we assigned all individuals the label “person” with unique IDs, marking the key child as ID = 1. This ID was later mapped to a “child body parts” category. We refined the “person” and “face” labels by:

- Differentiating the key child from others.
- Distinguishing adults from children/infants in both full-body and face detections.

Our fine-tuned YOLO11x model also classifies five key object categories: toy, book, kitchenware, screen, and other object. Due to low occurrences, “animal” and “food” were merged into “other object,” ensuring all final object classes had at least 2,000 instances.

The Quantex dataset contains `r quantex_num_videos` videos recorded at 30fps, resulting in `r quantex_num_frames` frames. We annotated `r det_total_num_videos` videos (`r det_total_num_frames` frames) and applied a stratified 80/10/10 split to preserve class distributions across training (`r num_det_frames_train`), validation (`r num_det_frames_val`), and testing (`r num_det_frames_test`) sets.

### Training and Evaluation {#training-face}
We trained YOLO11x on a Linux server (Quadro RTX 8000 GPU, 48-core Intel Xeon CPU, 188 GB RAM) for `r det_num_epochs` epochs (`r det_training_time` hours) using 640px images, a batch size of 16, cosine annealing learning rate scheduling [@loshchilovSGDRStochasticGradient2017], and early stopping after 10 epochs without improvement(max 200).

The loss function comprises Box Loss, Classification Loss, and Distribution Focal Loss (DFL) [@tervenLossFunctionsMetrics2024; @liGeneralizedFocalLoss2020], ensuring precise localization and classification. Evaluation with a confusion matrix and precision-recall curves showed strong class separation, achieving a precision of `r det_precision`, recall of `r det_recall`, F1-score of `r det_f1_score`, and mAP of `r det_map` (Table \@ref(tab:det-metrics-short)).
```{r det-metrics-short, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "mAP@0.5 | Precision | Recall  | F1-Score
             0.870  | 0.91      | 0.80    | 0.85"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "YOLO11x detection metrics on the Quantex dataset, reporting mAP@0.5, precision, recall, and F1-score across all classes.",
escape = FALSE,
)
```

```{r det-detection-examples-short, echo=FALSE, dpi=600, fig.align='center', fig.cap="Detection examples for the YOLO11x detection model trained on the Quantex dataset."}
img1 <- ggdraw() + draw_image("images/yolo_person_face_object_1.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/yolo_objects.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/yolo_person_face.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/yolo_person_face_object_2.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
middle_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights

final_layout <- (top_row / middle_row) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

While people, books, and screens had high AP scores, “toy” (`r ap_toy`), “kitchenware” (`r ap_kitchenware`), and “other object” (`r ap_other_object`) were often misclassified as background, likely due to selective annotation of child-interacted objects. Similarly, “infant/child” and “infant/child face” had false negatives, possibly due to occlusions and limited annotated samples. Future improvements could address these issues by expanding annotations and applying data augmentation. Overall, the model effectively detects people, faces, and key objects, as illustrated in Figure \@ref(fig:det-detection-examples-short).

## Yolo11x-cls: Gaze Classification {#yolo11x-cls}
```{r gaze-statistics_short, echo=FALSE, message=FALSE, warning=FALSE}
total_num_gaze_frames <- 20889
final_num_gaze_frames <- 32902
gaze_total_num_videos <- 64
num_gaze_frames_train <- 73364
gaze_to_no_gaze_ratio <- 21.25

num_gaze_frames_train <- 26320
num_gaze_frames_val <- 2088
num_gaze_frames_test <- 2091

gaze_tp <- 343
gaze_fp <- 195
gaze_fn <- 102
gaze_tn <- 1443
gaze_fpr <- gaze_fp / (gaze_fp + gaze_tn) * 100
num_gaze <- gaze_tp + gaze_fn
per_wrong_gaze <- gaze_fp / gaze_tp * 100
gaze_recall <- 0.77
gaze_precision <- 0.64
gaze_f1_score <- 0.70

gaze_num_epochs <- 37
gaze_training_time <- 10.4
```
Selecting a gaze classification model for our egocentric dataset presented challenges due to blurry, partially occluded faces captured at varying angles. Many gaze estimation methods rely on high-quality eye images [@zhangAppearancebasedGazeEstimation2015; @zhangItsWrittenAll2016], making them unsuitable for our data. Additionally, rather than predicting fine-grained gaze direction (e.g., left or right), our focus is on the binary classification of whether a person’s gaze is directed toward the child or not.

Given these constraints, we implemented ResNet- and YOLO-based models, selecting Ultralytics’ YOLO11x [@jocherUltralyticsYOLO112024] after preliminary testing. YOLO11x-cls, the largest variant with 28.4M parameters, achieved the highest accuracy (acc_top1 = 79.5) on our dataset. Its architectural enhancements, such as the C2PSA block, improve attention to critical regions, ensuring robust gaze detection under real-world conditions.

### Data Annotation and Preprocessing
We defined gaze as directed towarda the child if a person’s gaze was oriented toward the child’s face, body, or general direction. Each detected face was labeled as “gaze” or “no gaze.”

For training, we extracted cut-out faces from `r gaze_total_num_videos` videos, resulting in `r total_num_gaze_frames` frames. Since only `r gaze_to_no_gaze_ratio`% of faces were labeled “gaze,” we applied stratified splitting and data augmentation to balance the training set while keeping validation and testing sets in their natural distribution (Table \@ref(tab:gaze-dataset-splits-short)).

### Training and Evaluation
We trained YOLO11x-cls for `r gaze_num_epochs` epochs on a Linux server, using an image size of 640, batch size of 16, cosine annealing [@loshchilovSGDRStochasticGradient2017], and early stopping after 10 epochs. Training took `r gaze_training_time` hours.

The model achieved a precision of `r gaze_precision`, a recall of `r gaze_recall` recall, and an F1-score of `r gaze_f1_score` (Table \@ref(tab:gaze-metrics-short)). The cross-entropy loss curves for training and validation steadily decreased, with no signs of overfitting, indicating good generalization (see [Gaze Classification](#sup-yolo11x-cls) in the Supplementary Material for details).

```{r gaze-dataset-splits-short, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Quantex   | Train Ratio (%) | Training  | Val & Test Ratio (%)  | Validation  | Testing | Total
            Gaze      | 50            | 13160       | 79                    | 1645        | 1646    | 16451
            No Gaze   | 50            | 13160       | 21                    | 443         | 445     | 14048
            Total     | 100           | 26320       | 100                   | 2088        | 2091    | 30499"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Dataset splits for the YOLO11x gaze classification model trained on the Quantex dataset. The table shows the total number of frames, as well as the number of frames with gaze and no gaze in the training, validation, and testing datasets after data augmentation of the minority class (Gaze). 'Gaze' indicates frames where the person's gaze is directed towards the child, while 'No Gaze' indicates frames where the person's gaze is not directed towards the child. Ratios are given in percentages.",
  escape = TRUE
)
```

Despite strong performance, the egocentric perspective introduces challenges. Faces are often occluded or blurred, making gaze classification difficult—even for human annotators, who occasionally required a second inspection. Cut-off faces lacking visible eyes pose an additional issue, requiring the model to rely on head orientation and other facial features, which sometimes leads to misclassification.

Future work could incorporate temporal cues or additional facial landmarks to improve performance. However, given our dataset constraints, YOLO11x-cls provides a solid foundation for analyzing gaze in egocentric video data.

```{r gaze-metrics-short, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Precision | Recall  | F1-Score
            0.64      | 0.77    | 0.70"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11x-cls gaze classification model, for detecting if a person is looking at the key child.",
  escape = TRUE
)
```

## Proximity Heuristic {#proximity-heuristic}
Proximity plays a key role in social interaction, offering insights into the nature of relationships and social engagement [@janssenTrackingRealtimeProximity2024; @hernandez-herediaProximitySensorMeasuring2024; @onnelaUsingSociometersQuantify2014]. As our dataset lacks explicit proximity labels, we developed a heuristic method to estimate the distance between the child and others.

### Formula for Proximity Estimation
We estimate proximity using the size of detected face bounding boxes, assuming that larger boxes indicate closer individuals and smaller ones suggest greater distance. The proximity value is calculated based on face area (width × height) and compared to reference sizes: one for a face within arm’s reach and another for a more distant face (e.g., in the background or outdoors).

Since face area decreases quadratically as distance increases, a simple linear mapping would exaggerate differences for close faces and minimize them for distant ones. To address this, we apply logarithmic scaling, ensuring a more accurate proximity score between 0 and 1:

\[
\text{Proximity} = \frac{\ln(\text{Face Area}) - \ln(\text{Max Reference Area})}{\ln(\text{Min Reference Area}) - \ln(\text{Max Reference Area})}
\]

Where:

- \(\textbf{Face Area}\): Width × height of the detected face’s bounding box.
- \(\textbf{Max Reference Area}\): Area of the furthest detectable face.
- \(\textbf{Min Reference Area}\): Area of the closest detectable face.

This logarithmic transformation compresses proximity values for large faces (close to the camera) and stretches them for small faces (farther away), aligning with human perceptual sensitivity to relative size changes [@stevensPsychophysicsIntroductionIts2017].

```{r proximity-classifications, echo=FALSE, dpi=600, fig.align='center', fig.cap="Proximity Heuristic Examples. Example \\textbf{A} shows a face far away from the camera (proximity score = 0.07), example \\textbf{B} depicts a face slighlty closer (proximity score = 0.2). Example \\textbf{C} shows a face quite close to the camera (proximity score = 0.6), and example \\textbf{D} illustrates a face extremely close to the camera (proximity score = 1)."}


img1 <- ggdraw() + draw_image("images/face_0.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/face_02.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/face_06.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/face_1.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
bottom_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row / bottom_row) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```
### Incorporating Width-to-Height Ratio
Additional to face size, we use the faces' width-to-height ratio as cues for proximity. 
Based on reference images of full, front-facing faces, we define an expected aspect ratio for adults and children. If a detected face’s ratio deviates significantly, we infer that the face is extremely close to the camera, causing partial cropping of the bounding box. If the deviation exceeds a defined threshold ($\epsilon$), the proximity score is set to 1.

This heuristic approach, which combines face area and aspect ratio, provides an efficient and interpretable method for estimating proximity without requiring additional sensors or depth information. Its validity is supported by a strong Pearson correlation coefficient (r = 0.95) between the heuristic estimates and human-annotated proximity values, based on 364 annotated frames from the Quantex dataset. This high correlation indicates that the heuristic effectively captures the relative distances between the child and others in the recordings, thereby enabling the integration of spatial dynamics into our interaction analyses. Examples of proximity estimations are shown in Figure \@ref((fig:proximity-classifications), illustrating the range of proximity scores from 0 (far away) to 1 (extremely close).

## Participation Role Classification (PRC) {#voice-type}
```{r}
f1_kchi = 72.3
f1_cds = 65.3
f1_ohs = 48.7
f1_speech = 80.0
f1_avg = 65.4

f1_vtc_benchmark = 57.3
```
Regarding the audio component of our interaction analysis, we classified audio by speaker and intended target into four types: key child (KCHI), child-directed speech (CDS), overheard speech (OHS), and general speech (SPEECH). To achieve this, we adapted the open-source Voice Type Classifier (VTC) [@lavechinOpensourceVoiceType2020], designed to distinguish key child, other child, female adult, male adult, and general speech. The resulting architecture, referred to as the Participation Role Classifier (PRC), uses a SincNet front-end, a 3-layer bi-directional GRU (512 units), and extended input segments (4.0s) to better model interaction patterns. On our labeled ChildLens dataset (CHILDLENS PAPER REFERENCE), the PRC achieved an average F1 score of `r f1_avg`, matching the original VTC’s performance (`r f1_vtc_benchmark`) and confirming its suitability for our data.

# Results


## Developmental Trends in Social Interaction


### Age-related changes in social engagement 

### Gaze Direction Dynamics
```{r gaze-direction-stats}
child_gaze_overall <- 70.80
adult_gaze_overall <- 64.55
```
- another interesting interaction aspect is the percentage of faces with gaze directed at the child and how this percentage develops over time. we furthermore analyse the percentage of adult faces with gaze directed at the child and the percentage of child faces with gaze directed at the child and whether there are differences between the age groups. as visible in Figure \@ref(fig:gaze-patterns-by-age), the percentage of faces with gaze directed at the child increases with age, with 3-year-olds showing the lowest percentage and 5-year-olds the highest. this trend is consistent for both adult and child faces, indicating a developmental shift towards more focused attention on the child.
- overall the percentage of faces with gaze directed at the child do not show any age-related differences, neither for adult faces nor for child faces. in both face categories the percentage stay rather stable, as shown in Table \@ref(tab:gaze-percentage-overall-comparison). however, when comparing the percentage of adult and child faces with gaze directed at the child (child `r adult_gaze_overall`% vs. adult `r child_gaze_overall`%), we find that child faces are more likely to look at the child than adult faces, with a significant difference across all age groups. this suggests that children are more often the focus of attention in social interactions, regardless of the age of the interacting partner. while it often can also be possible that adults are present but do not interact with the child, the higher percentage of child faces with gaze directed at the child indicates that children are actively engaged in social interactions with the key child.

Another interesting interaction aspect is the percentage of faces with gaze directed at the child and how this percentage develops with increasing age of the key child. We furthermore analyze the percentage of adult faces with gaze directed at the child and the percentage of child faces with gaze directed at the child, and whether there are differences between the age groups. As visible in Figure \@ref(fig:gaze-patterns), the percentage of faces with gaze directed at the child stays rather stable across all age groups. This trend is consistent for both adult and child faces.

Overall, the percentage of faces with gaze directed at the child does not show any age-related differences, neither for adult faces nor for child faces. In both face categories, the percentages remain rather stable, as shown in Table @ref(tab:gaze-percentage-overall-comparison). However, when comparing the percentage of adult and child faces with gaze directed at the child (child r adult_gaze_overall% vs. adult r child_gaze_overall%), we find that child faces are more likely to look at the child than adult faces, with a significant difference across all age groups. This suggests that children are more often the focus of attention in social interactions, regardless of the age of the interacting partner. While it is often possible that adults are present but do not interact with the child, the higher percentage of child faces with gaze directed at the child indicates that children are actively engaged in social interactions with the key child.

```{r gaze-patterns, fig.align='center', fig.cap="Percentage of faces wth gaze directed at child across different age groups.", out.width="450px"}
knitr::include_graphics("images/gaze_patterns_by_age.png")
```

```{r gaze-percentage-overall-comparison, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Group   | Adult Gaze (%) | Child Gaze (%) | t-stat    | p-value   | N
            Overall | 64.55          | 70.80          | -5.191    | 0.0000*** | 401
            Age 3   | 63.95          | 70.88          | -4.416    | 0.0000*** | 139
            Age 4   | 64.90          | 69.74          | -1.924    | 0.0566    | 125
            Age 5   | 64.85          | 71.68          | -3.172    | 0.0019**  | 137"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL

# Print the formatted table
apa_table(
  df,
  caption = "Overall Adult vs. Child Gaze Comparison. The table shows the percentage of adult and child faces with gaze directed at the child across different age groups. The t-statistic and p-value are reported for the overall comparison and each age group comparison.",
  escape = TRUE
)
```
### Presence of a Face

### Presence of Gaze

### Presence of Language
## Co-occurrence of Aspects of Social Interaction 




# General Discussion

\newpage
# References

\newpage
# Supplementary Material
## Outline
This document contains supplementary material for the paper "Exploring Aspects of Social Interaction in Children’s Everyday Lives using Machine Learning: A Multimodal Analysis of the Quantex Dataset". First, we provide an overview of the [Participant Recruitment and Data Collection](#sup-participants), including a detailed description of the [Recruitment and Data Collection](#sup-recruitment) process. We then describe the [Materials](#sup-materials) used in the study, followed by a structured overview of the [Dataset](#sup-dataset), including the [Annotation Strategy](#sup-annotation-strategy). Next, we outline the [Automated Analysis Pipeline](#sup-automatic-analysis), including [YOLO11x: Multi-Class Detection of Persons, Faces, and Objects](#sup-yolo11x), [YOLO11x-cls: Gaze Classification](#sup-yolo11x-cls)), as well as [Voice Type Classification](#sup-voice-type). Finally, we report the [Results](#sup-results) of the automated pipeline, evaluating the performance of the models when applied to all videos in the Quantex dataset.

# Methodology {#sup-methodology}
The following sections provide a detailed description of the data collection process, the structure and characteristics of the dataset, the annotation strategy, and the preprocessing applied to the data prior to analysis. Additionally, an overview of the automated analysis pipeline is provided, giving details about the models used for person and face detection, gaze classification, object detection, and the application of a pre-trained voice type classifier. 

## Participants Recruitment and Data Collection {#sup-participants}
This study collected egocentric video recordings from `r quantex_nr_children` children, aged 3 to 5 years, over a span of `r quantex_time_span_months` months [MB: in addition, it would be interesting to know in what time intervall the videos were collected for each child, also a more detailed overview of the number of children per age group would be nice]. 

### Recruitment Process
Participants were recruited from a mid-sized German city through an existing lab database, with approximately equal distribution across age groups (30% 3-year-olds, 35% 4-year-olds, 35% 5-year-olds). The data collection process was approved by the local ethics committee, and all participating families provided written informed consent, allowing the researchers to use the data for scientific purposes. Participants were recruited from an internal lab database and contacted via phone. Parents received a detailed explanation of the study's purpose and procedures, consistent with the information in the study brochure. Families who agreed to participate were provided with a vest equipped with an embedded camera, which children were asked to wear for approximately two hours, with flexible extension options. For privacy protection, recordings were limited to the home environment. Recorded videos were securely stored on the MPI-EVA server and subsequently made available to parents. To enhance convenience, parents could choose to have the camera delivered to their home or workplace, or opt for self-pickup.

### Distribution of Study Materials and Instructions
At the handover appointment, parents received one or two vests (multiple sizes available), a fully charged camera unit and comprehensive information on the study's purpose and data protection protocols. Parents received hands-on training on camera operation and were encouraged to practice using the equipment. The consent form required listing the participating child and documenting all individuals potentially captured in the recordings. Five copies of data privacy information sheets were provided for distribution to anyone who might be recorded. A follow-up call was scheduled to ensure recording success, typically planned for the weekend following the handover to ensure enough time for recoding.

### Return of Equipment and Final Data Confirmation
On the scheduled date, parents were contacted to confirm recording success and assess any need for additional time. Flexible rescheduling was offered as needed. Once the recording was completed, a pickup or drop-off appointment was arranged to collect the completed consent form, vest, and camera. Parents were also asked to confirm their current email address for sharing the recordings. Children received a "Labyrinth" game as a thank for their participation.

## Materials {#sup-materials}
To capture children’s everyday experiences, a wearable vest equipped with a _PatrolEyes WiFi HD Infrared Police Body Camera_ was used (Figure \@ref(fig:camera-cvat-activity-classes)). The camera recorded high-definition video (1920x1080p at 30 fps) with a 140-degree wide-angle lens and also captured audio. Children were free to move around and engage in their usual activities at home without any interference or instructions given to their parents. 

## Dataset Overview {#sup-dataset}
The Quantex dataset includes video and audio recordings from `r quantex_nr_children` children aged 3 to 5 years (M=`r quantex_mean_age`, SD=`r quantex_sd_age`). The dataset contains `r quantex_count_3_plus` videos from three-year-olds, `r quantex_count_4_plus` videos from four-year-olds, and `r quantex_count_5_plus` videos from five-year-olds. The number of videos per child varies, as parents decide when and how often to record. The recording duration per child ranges from `r quantex_min_minutes` to `r quantex_max_minutes` minutes (M=`r quantex_mean_minutes`, SD=`r quantex_sd_minutes`). The total duration of all video recordings in the dataset is `r quantex_sum_hours` hours. Figure \@ref(fig:sup-quantex-minutes-per-child) shows the distribution of video duration per child.

```{r sup-quantex-minutes-per-child, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Video recording duration (in minutes) per Child in the Quantex Dataset.", fig.height=2.5, fig.width=3.5}
quantex_unique_data$ID <- as.factor(quantex_unique_data$ID)

ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )
quantex_1_plot <- ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )

# Specify the file path and name
# <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
#file_name_1 <- "quantex_minutes_per_id_plot.png"
#quantex_file_path_1 <- file.path(quantex_1_output_folder, file_name_1)

# Save the plot
#ggsave(filename = quantex_file_path_1, plot = quantex_1_plot, width = 8, height = 6, dpi = 300)
```

## Annotation Strategy {#sup-annotation-strategy}
The dataset annotations cover four key elements: persons, faces, gaze direction, and objects the child interacts with. For each detected person (or reflection of a person, such as in a mirror) and face, additional attributes are recorded, including a unique identifier, age (infant, child, teen, adult, unknown), and gender (female, male, unknown). Gaze information indicates whether a detected person’s gaze is directed toward the child or not. Faces are annotated even when occluded or blurry to ensure comprehensive coverage of interactions. Partially visible faces are also annotated if key facial features, such as the nose, eyes, or mouth, remain identifiable. 

The egocentric nature of our video data, recorded from a chest-mounted camera, posed additional challenges such as motion blur and oblique viewing angles, which made gaze classification difficult, even for human annotators. These factors contributed to occlusion or distortion of gaze information in many frames, making accurate gaze direction annotation particularly challenging.

Objects are annotated only when the child is actively interacting with them, either by holding them in their own hands or when another person in the interaction is holding the object in their hands. These objects are categorized into six distinct groups: book, screen, animal, food, toy, and kitchenware, with an additional category for other objects. The annotation strategy is summarized in Figure \@ref(fig:sup-camera-cvat-activity-classes).

The Quantex dataset consists of a total of `r quantex_num_videos` videos. However, only a subset of `r det_total_num_videos` videos was annotated, totaling `r det_total_num_frames` frames. To balance workload while still obtaining meaningful annotations, we applied a frame sampling strategy, annotating every 30th frame, which corresponds to one frame per second. The goal of these annotations was to create ground truth data for training a model that can later be used to analyze the remaining videos in the dataset. While the video data was subject to structured annotation and validation, the audio data was used in its raw form without preprocessing for analysis.

```{r sup-camera-cvat-activity-classes, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A} – Vest with the embedded camera worn by the children, \\textbf{B} – CVAT platform utilized for video annotation, \\textbf{C} – Annotation Strategy in the Quantex dataset."}
img1 <- ggdraw() + draw_image("images/camera_worn_close.png", scale = 0.8)
img2 <- ggdraw() + draw_image("images/cvat.png", scale = 0.8)
img3 <- ggdraw() + draw_image("images/quantex_annotation_strategy_narrow.png", scale = 0.9)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(0.4, 1))

final_layout <- (top_row / img3) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```
# Automated Analysis Pipeline {#sup-automatic-analysis}
Our automated analysis pipeline consists of four key modules: person and face detection, gaze classification, object detection, and voice type classification. Each module operates independently, utilizing separate machine learning models. Except for the voice type classifier, all models were trained on the Quantex dataset.

The pipeline follows a sequential process:

	1.	YOLO11x detection model identifies the presence of individuals (persons and faces) and types of objects the key child interacts with, both in social and independent play contexts, in the video frames.
	2.	Gaze classification determines whether detected faces are looking at the child.
	3.	Voice type classification detects the presence of speech and identifies whether the speaker is the key child, another child, or an adult.

By integrating these modules, our pipeline enables a comprehensive analysis of children’s everyday experiences, capturing both social interactions and independent play.

## YOLO11x: Multi-Class Detection of Persons, Faces, and Objects {#sup-yolo11x}
In our study, we utilized Ultralytics' YOLO11, the "latest iteration in the Ultralytics YOLO series of real-time object detectors" [@jocherUltralyticsYOLO112024], trained on the COCO dataset [@linMicrosoftCOCOCommon2014], a large-scale dataset containing labeled images for 80 object categories commonly found in everyday environments. COCO is widely used for object detection, instance segmentation, and keypoint detection tasks. 

Released in October 2024, YOLO11 introduces architectural improvements such as the C2PSA block (Convolutional Block with Parallel Spatial Attention), which enhances spatial attention within feature maps, allowing the model to focus more precisely on critical areas of an image compared to previous YOLO versions. Additionally, YOLO11 incorporates the C3K2 block, designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process [@khanamYOLOv11OverviewKey2024]. These advancements make the YOLO11 detection model, pretrained on COCO, well-suited for training on our egocentric dataset, which captures dynamic movements from a camera perspective on chest height.

For our study, we utilized YOLO11x, the largest model in the YOLO11 model series, which has 56.9M parameters and 194.9 GFLOPs. With the highest accuracy (mAP$^{val}_{50-95}$ = 54.7) among YOLO11 variants, YOLO11x, combined with its architectural advancements, was the optimal choice for our task, as we had the computational resources to support running a larger model.

### Dataset Annotation and Preprocessing
Our dataset presents unique challenges due to its egocentric viewpoint, as body parts of the child wearing the camera frequently appear in the footage. To prevent misclassification, we employ a dedicated annotation scheme where all individuals are labeled as “person” with unique IDs, consistently assigning the key child (child wearing the camera) ID = 1. During preprocessing, we map this ID to a separate “child body parts” category to distinguish the child’s presence from other individuals.

Additionally, we refine the “person” and “face” categories beyond standard YOLO models by:

- Differentiating the key child from other individuals.
- Distinguishing adults from children/infants in both full-body detections and faces.

Our fine-tuned YOLO11 model also classifies five object categories relevant to the child’s interactions: toy, book, kitchenware, screen, and other object. Due to low occurrences, "animal" (19 instances, 0.015%) and "food" (1,115 instances, 0.84%) were included into "other object", leaving five final object categories with at least 2,000 instances each.

### Dataset Splitting
The full Quantex dataset consists of `r quantex_num_videos` videos, all recorded at 30fps, resulting in `r quantex_num_frames` frames. From this dataset, we annotated `r det_total_num_videos` videos, totaling `r det_total_num_frames` frames. Prior to splitting this dataset into training, validation, and testing datasets, we analyzed how often each class was present in the dataset. The distribution, displayed in detail in Table \@ref(tab:det-class-distribution), revealed an imbalance, with the "Adult" class being the most frequent. To address this imbalance, we applied a stratified split to ensure that each dataset preserved the original class distribution. Following an 80/10/10 split, the final datasets consisted of `r num_det_frames_train` frames for training (80%), `r num_det_frames_val` frames for validation (10%), and `r num_det_frames_test` frames for testing (10%), ensuring an accurate evaluation of model performance relative to real-world data distribution (see Figure \@ref(tab:det-dataset-splits)).

```{r det-class-distribution, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Class                   | Training  | Validation  | Testing | Total
            Adult                   | 25706     | 3213        | 3213    | 32132
            Child/Infant            | 22403     | 2801        | 2800    | 28004
            Adult Face              | 8669       | 1083        | 1084    | 10836
            Child/Infant Face       | 6756      | 844         | 845     | 8445
            Book                    | 8370      | 1046        | 1046    | 10462
            Toy                     | 13870     | 1734        | 1733    | 17337
            Kitchenware             | 1915      | 239         | 240     | 2394
            Screen                  | 3374      | 422         | 422     | 4218
            Other Object            | 15608     | 1950        | 1950    | 19508"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Dataset splits for the YOLO11 detection model trained on the Quantex dataset. The table shows the distribution of annotated persons, faces, and objects in the training, validation, and testing datasets.",
  escape = FALSE
)
```

```{r det-dataset-splits, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "                  | Training  | Validation  | Testing | Total
            Number of frames  | 91039     | 11380       | 11380   | 113799"
  
df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Number of frames in the training, validation, and testing datasets for the YOLO11 detection model.",
  escape = FALSE
)
```
### Training and Convergence {#sup-training-face}
Model training was conducted on a Linux server equipped with an Intel(R) Xeon(R) Silver 4214Y CPU @ 2.20GHz with 48 cores, a Quadro RTX 8000 GPU and 188 GB of RAM. The model was trained for a total of `r det_num_epochs` epochs, taking `r det_training_time` hours to complete. Training utilized YOLO11’s built-in data augmentation, an image size of 640, a batch size of 16, a cosine annealing learning rate scheduler [@loshchilovSGDRStochasticGradient2017], and early stopping after 10 epochs without improvement, with a maximum of 200 epochs.

```{r det-loss-curves, fig.align='center', fig.cap="Training and Validation Loss Curves for the YOLO11x detection model.", out.width="450px"}
knitr::include_graphics("images/yolo_loss_curves.png")
```

The loss function of the YOLO11 model comprises three main components: Box Loss, Classification Loss, and Distribution Focal Loss (DFL) [@tervenLossFunctionsMetrics2024; @liGeneralizedFocalLoss2020]. _Box Loss_ quantifies the difference between predicted bounding boxes and ground truth boxes, ensuring precise localization of detected persons, faces and objects by penalizing inaccuracies in position and size. _Classification Loss_ (DFL) evaluates the model’s ability to correctly assign detected instances to their respective classes, reducing false positives and false negatives. _Distribution Focal Loss_ enhances the model's ability to detect challenging persons, faces and objects, particularly small or partially occluded ones, by refining the localization of bounding box coordinates and emphasizing hard-to-detect instances. Together, these loss components contribute to a more robust and accurate detection model. 

During the training process, we observed that all three loss components decreased over time, indicating effective learning and improved performance, as visible in in Figure \@ref(fig:det-loss-curves). A steady decrease in Box Loss indicates that the model is becoming increasingly accurate in localizing persons, faces and objects within frames. Similarly, the steady convergence of the Classification Loss reveals the model's increasing ability to reliably classifiy the detected instance in one of the relevant classes. The decrease in DFL over time indicates that the model is getting better at focusing on and correctly identifying difficult-to-detect persons, faces or objects, which improves its overall detection capabilities. Conclusively, the loss curves show that the model effectively learned to localize and identify the target classes during the training period.

```{r det-map_all, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "class               | mAP@0.5
             infant/child       | 0.87
             adult              | 0.95
             infant/child face  | 0.85
             adult face         | 0.95
             child body parts   | 0.97
             book               | 0.94
             toy                | 0.78
             kitchenware        | 0.81
             screen             | 0.96
             other object       | 0.83"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Mean Average Precision (mAP@0.5) for the YOLO11x detection model trained on the Quantex dataset. The table shows the mAP@0.5 for each class.",
  escape = FALSE,
)
```
  
  
```{r det-detection-metrics-detailed, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "mAP@0.5 | Precision | Recall  | F1-Score
             0.870  | 0.91      | 0.80    | 0.85"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Detection metrics for the YOLO11x detection model trained on the Quantex dataset. The table reports mean Average Precision at IoU 0.5 (mAP@0.5), along with the averaged precision, recall, and F1-score across all classes.",
escape = FALSE,
)
```

```{r det-metrics, fig.align='center', fig.cap="\\textbf{A} - Confusion Matrix for the YOLO11x detection model trained on the Quantex dataset. \\textbf{B} - Precision-Recall Curve for the YOLO11x detection model."}
# Load images
img1 <- ggdraw() + draw_image("images/confusion_matrix_detection.png", scale = 1.1)
img2 <- ggdraw() + draw_image("images/yolo_precision_recall_curve.png", scale = 1.1)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row) +
  plot_layout(heights = c(1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```


```{r det-detection-examples, echo=FALSE, dpi=600, fig.align='center', fig.cap="Detection examples for the YOLO11x detection model trained on the Quantex dataset."}
img1 <- ggdraw() + draw_image("images/yolo_person_face_object_1.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/yolo_objects.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/yolo_person_face.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/yolo_person_face_object_2.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
middle_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights

final_layout <- (top_row / middle_row) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```
### Model Evaluation Metrics
The performance of the object detection model was evaluated using a confusion matrix and precision-recall (PR) curves. The YOLO11 model achieved a precision of `r det_precision` and a recall of `r det_recall` on the testing set, resulting in an F1-score of `r det_f1_score`. The F1 metric is particularly important as it reflects both the accuracy of identifying each class instance (precision) and the model’s ability to detect all occurrences of the class (recall). The normalized confusion matrix (see figure \@ref(fig:det-metrics)) reveals strong overall performance with mean Average Precision across all classes mAP=`r det_map`. The averaged precision-recall curve remains close to the top-left corner and signifies that the model maintains high precision and recall across various thresholds, underscoring its effectiveness in detecting the different classes. 

To further refine detection results, we adjusted the built-in non-max suppression (NMS) threshold. YOLO already applies an NMS threshold of 0.7, where detections of the same class with an Intersection over Union (IoU) exceeding this threshold are suppressed. After evaluating different thresholds, we set the NMS IoU to 0.5, as this provided the best F1-score. However, this default NMS is only applied to bounding boxes of the same class, and manual inspection of detections revealed that some faces and persons were redundantly detected in multiple classes (e.g., a face detected as both “adult face” and “child face”).

To address this issue, we implemented a custom cross-class NMS to remove duplicate detections across all classes. Using the same NMS logic, we applied an IoU threshold of 0.9 to ensure only nearly identical bounding boxes were removed while retaining valid detections. In cases where two overlapping bounding boxes exceeded this threshold, we kept the detection with the higher confidence score to minimize incorrect suppressions. This adjustment significantly reduced redundant detections while preserving true positive detections, leading to a more accurate and reliable final inference output.

Notably, the confusion matrix demonstrates minimal confusion between most classes, indicating that the model can effectively distinguish between them (see Figure \@ref(fig:det-metrics)). A closer look at class-wise performance reveals that the model excels in detecting people and certain objects. The “infant/child,” “adult,” and “adult face” classes, as well as “child body parts,” “infant/child face,” “book,” and “screen,” all achieve high Average Precision (AP) scores. This indicates that the model reliably identifies these categories with strong precision and recall (see figure \@ref(fig:det-metrics)).

On the other hand, some object classes remain more challenging. In particular, the “toy,” “kitchenware,” and “other object” categories exhibit lower AP scores of `r ap_toy`, `r ap_kitchenware`, and `r ap_other_object`, respectively. The confusion matrix confirms that `r fn_kitchenware*100`% of “kitchenware” and `r fn_toy*100`% of “toy” instances are frequently misclassified as “background.” This issue likely stems from the annotation strategy, which prioritized labeling only objects that the child directly interacted with. As a result, other visually similar objects in the background remain unannotated, introducing ambiguity during training. Additionally, “infant/child” and “infant/child face” also show notable false negative rates of 17%, further contributing to misclassification. These findings suggest that increasing the number of annotated samples—particularly for objects with which the child interacts with—could improve detection performance. Future work could also explore model refinements or data augmentation techniques to enhance the detection of small, occluded, or partially visible objects. In conclusion, the YOLO11 model demonstrates strong performance in detecting people, faces, and several object classes, as displayed in Figure \@ref(fig:det-detection-examples).

## YOLO11x-cls: Gaze Classification {#sup-yolo11x-cls}
```{r gaze-statistics, echo=FALSE, message=FALSE, warning=FALSE}
total_num_gaze_frames <- 20889
final_num_gaze_frames <- 32902
gaze_total_num_videos <- 64
num_gaze_frames_train <- 73364
gaze_to_no_gaze_ratio <- 21.25

num_gaze_frames_train <- 26320
num_gaze_frames_val <- 2088
num_gaze_frames_test <- 2091

gaze_tp <- 343
gaze_fp <- 195
gaze_fn <- 102
gaze_tn <- 1443
gaze_fpr <- gaze_fp / (gaze_fp + gaze_tn) * 100
num_gaze <- gaze_tp + gaze_fn
per_wrong_gaze <- gaze_fp / gaze_tp * 100
gaze_recall <- 0.77
gaze_precision <- 0.64
gaze_f1_score <- 0.70

gaze_num_epochs <- 37
gaze_training_time <- 10.4

adult_gt_heuristic_corr <- 1
child_gt_heuristic_corr <- 1
```
Selecting an appropriate model for gaze classification in our automated pipeline presented unique challenges due to the egocentric perspective of our dataset. Many gaze estimation methods rely on high-quality eye images, either extracted separately [@zhangAppearancebasedGazeEstimation2015] or as part of the full face [@zhangItsWrittenAll2016]. However, our dataset often contains blurry or partially occluded faces captured at varying angles, making such approaches not suitable. Additionally, rather than predicting fine-grained gaze direction (e.g., left or right), our focus is on the binary classification of whether a person’s gaze is directed toward the child or not.

@chengAppearancebasedGazeEstimation2021 provides an overview of the challenges and recent advancements in gaze estimation methods, including approaches that incorporate temporal information, such as Gaze360 [@kellnhoferGaze360PhysicallyUnconstrained2019]. While these methods improve tracking across frames, they are not the primary focus of our study, as we analyze social interactions on a frame-by-frame basis.

Given these constraints of our dataset, we opted for a CNN-based approach, which could be trained on ground truth annotations. Recent studies [@zhangMPIIGazeRealWorldDataset2019; @zhangETHXGazeLargeScale2020; @shahDriverGazeEstimation2022] have explored different CNN based gaze estimation architectures, among which are VGG, ResNet or YOLO architectures. Based on these findings, we implemented a ResNet and YOLO-based model for binary classification (gaze directed toward the child or not). After conducting preliminary tests, Ultralytics’ YOLO11 architecture [@jocherUltralyticsYOLO112024] demonstrated the best performance, which led us to select the YOLO11x classification model, pretrained on ImageNet, for our gaze classification task.

We fine-tuned YOLO11x-cls, the largest model in the YOLO11 classification series, which has 28.4M parameters. The model achieved the highest accuracy among all YOLO11 variants (acc_top1 = 79.5), making it the optimal choice for gaze classification in our dataset. Moreover YOLO11’s architectural enhancements, such as the C2PSA block, improve the model’s ability to focus on critical regions within an image, ensuring robust detection of visual attention toward the child, even under challenging real-world conditions.

### Data Annotation and Preprocessing
We defined gaze as being “directed toward the child” when a person’s gaze was oriented toward the child’s face, body, or general direction of the child. This included instances where the person was looking directly at the camera (worn by the child) or slightly upwards, allowing for an estimation of the likely position of the child’s head. Each detected face was annotated as either “gaze” (gaze directed at the child) or “no gaze” (gaze directed elsewhere).

### Dataset Splitting
For the gaze classification task, we utilized a subset of the Quantex dataset, focusing specifically on frames containing annotated faces. This subset consisted of cut-out faces extracted from the annotated face bounding boxes, resulting in `r total_num_gaze_frames` frames from `r gaze_total_num_videos` annotated videos. An analysis of the gaze labels showed that `r gaze_to_no_gaze_ratio`% of the faces were annotated as having gaze directed toward the child, leading to a class imbalance.

To maintain the original distribution of gaze labels across datasets, we first applied stratified dataset splitting, ensuring that the training, validation, and testing sets reflected the natural ratio of gaze and non-gaze frames. To address class imbalance in the training dataset, we applied data augmentation to increase the number of gaze frames. Since our dataset was already a subset of the Quantex dataset, downsampling to achieve balance would have resulted in a loss of valuable data. As a result, the final dataset included a balanced training set with 50% gaze and 50% no-gaze frames, while the validation and testing sets retained their original, unbalanced distribution. The final data distribution is presented in Table \@ref(tab:gaze-dataset-splits), with `r num_gaze_frames_train` frames in the training set, `r num_gaze_frames_val` frames in the validation set, and `r num_gaze_frames_test` frames in the testing set.

```{r gaze-dataset-splits, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Quantex   | Train Ratio (%) | Training  | Val & Test Ratio (%)  | Validation  | Testing | Total
            Gaze      | 50            | 13160       | 79                    | 1645        | 1646    | 16451
            No Gaze   | 50            | 13160       | 21                    | 443         | 445     | 14048
            Total     | 100           | 26320       | 100                   | 2088        | 2091    | 30499"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Dataset splits for the YOLO11x gaze classification model trained on the Quantex dataset. The table shows the total number of frames, as well as the number of frames with gaze and no gaze in the training, validation, and testing datasets after data augmentation of the minority class (Gaze). 'Gaze' indicates frames where the person's gaze is directed towards the child, while 'No Gaze' indicates frames where the person's gaze is not directed towards the child. Ratios are given in percentages.",
  escape = TRUE
)
```
### Training and Convergence
We trained the gaze classification model on the same Linux server used for person and object detection model training. For details, see [Training and Convergence](#sup-training-face). Training ran for `r gaze_num_epochs` epochs, completing in `r gaze_training_time` hours . Similar to the YOLO detection model, we used an input image size of 640, a batch size of 16, a cosine annealing learning rate scheduler [@loshchilovSGDRStochasticGradient2017], and early stopping after 10 epochs without improvement, with a maximum of 200 epochs.

```{r gaze-loss-confusion-matrix, fig.align='center', fig.cap="\\textbf{A} - Training and Validation Loss Curves for the YOLO11x gaze classification model. \\textbf{B} - Confusion Matrix for the YOLO11x gaze classification model trained on the Quantex dataset."}
# Load images
img1 <- ggdraw() + draw_image("images/gaze_loss_curves.png", scale = 1)
img2 <- ggdraw() + draw_image("images/confusion_matrix_gaze.png", scale = 1)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row) +
  plot_layout(heights = c(1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

Figure \@ref(fig:gaze-loss-confusion-matrix) shows the cross-entropy loss curves for both training and validation. The steady decline in loss over time indicates that the model effectively learned to classify whether a person is looking in the direction of the child or not. The convergence of the training and validation loss curves suggests that the model generalizes well, as there is no indication of overfitting. Since gaze classification is a binary task, we used top-1 accuracy as the primary evaluation metric. Figure \@ref(fig:gaze-loss-confusion-matrix)A also illustrates the increasing accuracy over time, confirming that the model progressively improves its ability to classify gaze correctly.

### Model Evaluation Metrics
The YOLO11x gaze classification model achieved a precision of `r gaze_precision`, a recall of `r gaze_recall`, and an F1-score of `r gaze_f1_score` on the testing set. These metrics, summarized in table \@ref(tab:gaze-metrics), indicate that the model effectively distinguishes between gaze and no-gaze frames, though there is still room for improvement. The egocentric perspective of the dataset presents challenges, as faces are often partially occluded or blurred, making gaze classification difficult—even for human annotators, who occasionally required a second inspection to determine gaze direction.

```{r gaze-metrics, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Precision | Recall  | F1-Score
            0.64      | 0.77    | 0.70"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11x gaze classification model trained on the Quantex dataset to classify whether a person is looking into the direction of the child wearing the camera or not. Precision, recall, and F1-score are given for the testing set.",
  escape = TRUE
)
```

One of the primary challenges arises from cut-off faces, where the eyes are not always visible. In such cases, the model must rely on other facial features, such as head orientation or mouth position, to infer gaze direction. While this approach is often effective, it occasionally leads to misclassifications. Despite these difficulties, the model achieved a satisfactory recall, correctly identifying `r gaze_recall*100`% of all gaze frames.

More advanced gaze estimation methods—such as incorporating temporal information or leveraging additional facial landmarks—could further improve performance. However, given the constraints of our dataset and the focus on binary gaze classification, the YOLO11x model serves as a strong foundation for analyzing the gaze aspect of social interactions in egocentric video data.

## Proximity Heuristic {#sup-proximity}
Proximity between individuals is an important aspect of social interaction. Previous studies have shown that interpersonal distance can provide insights into the nature of relationships and social engagement [@janssenTrackingRealtimeProximity2024; @hernandez-herediaProximitySensorMeasuring2024; @onnelaUsingSociometersQuantify2014]. Since our dataset does not contain explicit proximity labels, we developed a heuristic approach to estimate the distance between the child and other individuals.

### Formula for Proximity Estimation
We infer proximity from the size of the bounding boxes of detected faces, assuming that the size of these bounding boxes provides an implicit cue for proximity. Specifically, we infer that larger bounding boxes correspond to individuals closer to the camera, whereas smaller bounding boxes indicate individuals further away. The proximity value is calculated using the face area, defined as the product of the bounding box width and height. This computed size is then compared to predefined reference face sizes: one corresponding to a face very close to the camera (within arm’s reach) and another representing a face further away (e.g., in the background or outdoors). 

Due to the nonlinear relationship between face size and distance, where face area decreases quadratically as distance increases, a simple linear mapping would make small differences appear too large for close faces and too small for distant ones. To address this, we apply a logarithmic scaling approach, which maps the calculated face area to a proximity score between 0 and 1:

\[
\text{Proximity} = \frac{\ln(\text{Face Area}) - \ln(\text{Max Reference Area})}{\ln(\text{Min Reference Area}) - \ln(\text{Max Reference Area})}
\]

Where:

- \(\textbf{Face Area}\) is the area of the detected face, calculated as the width times the height of the bounding box.
- \(\textbf{Max Reference Area}\) is the reference area corresponding to the furthest detectable face.
- \(\textbf{Min Reference Area}\) is the reference area corresponding to the closest detectable face.

This logarithmic transformation ensures that proximity values are compressed for large faces (close to the camera) and stretched for small faces (farther away), making them perceptually meaningful. Since human sensitivity to size changes depends on relative differences rather than absolute ones, logarithmic scaling aligns with psychophysical principles of distance perception [@stevensPsychophysicsIntroductionIts2017].

```{r proximity-heuristic, echo=FALSE, dpi=600, fig.align='center', fig.cap="Proximity Heuristic examples. Example \\textbf{A} shows a face far away from the camera (proximity score = 0.07), example \\textbf{B} depicts a face slighlty closer (proximity score = 0.2). Example \\textbf{C} shows a face quite close to the camera (proximity score = 0.6), and example \\textbf{D} illustrates a face extremely close to the camera (proximity score = 1)."}


img1 <- ggdraw() + draw_image("images/face_0.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/face_02.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/face_06.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/face_1.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
bottom_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row / bottom_row) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

### Incorporating Width-to-Height Ratio
Along with face size, we use the width-to-height ratio of the detected face as a proximity cue. Reference aspect ratios for adults and children are based on front-facing images. If a detected face's aspect ratio deviates significantly from the expected ratio, we infer that the face is extremely close to the camera, causing partial cropping of the bounding box.

If the deviation exceeds a set threshold ($\epsilon$), the proximity score is set to 1:

\[
r_{\text{expected}} = \frac{w_{\text{expected}}}{h_{\text{expected}}}
\]

\[
\text{If} \quad |r_{\text{detected}} - r_{\text{expected}}| > \epsilon, \quad P = 1
\]

With (\(\epsilon\)=1), this heuristic efficiently estimates proximity using only image-based cues, avoiding the need for additional sensors. It helps analyze social interactions in egocentric recordings, offering insights into children’s spatial relationships. Figure \@ref(fig:proximity-heuristic) illustrates proximity scores from 0 (far) to 1 (very close).

### Proximity Heuristic Evaluation
To validate our proximity heuristic, we conducted a quality check using human annotations. A total of 364 frames (185 adult faces and 179 child faces) were evaluated by two independent annotators. Each annotator directly assigned a proximity value between 0 and 1, with 0 representing the furthest distance and 1 the closest.

Prior to the assessment task, annotators were shown the same reference images used for calculating the reference face sizes in the proximity estimation. They were informed that the closest reference image corresponded to a value of 1, while the furthest reference image corresponded to a value of 0. This ensured a shared understanding of the proximity scale and minimized subjective interpretation differences.

```{r proximity-metrics, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
gt_heuristic_corr <- 0.9492
adult_gt_heuristic_corr <- 0.9548
child_gt_heuristic_corr <- 0.9372
prox_mae <- 0.0914
prox_r2 <- 0.8403

my.data <- "Metrics   | prox              | prox_1  | prox_2  | adult   | adult_1  | adult_2  | child   | child_1 | child_2
            Pearson   | \\textbf{0.9492}  | 0.9373  | 0.9035  | \\textbf{0.9548}  | 0.9518   | 0.9078   | \\textbf{0.9372}  | 0.9163  | 0.8877
            MAE       | 0.0914            | 0.0947  | 0.1596  | 0.1015  | 0.0864   | 0.1573   | 0.0809  | 0.1033  | 0.1621
            RMSE      | 0.1111            | 0.1180  | 0.1875  | 0.1207  | 0.1086   | 0.1857   | 0.1001  | 0.1270  | 0.1893
            R2        | 0.8403            | 0.8615  | 0.4588  | 0.8207  | 0.8861   | 0.4992   | 0.8381  | 0.7958  | 0.3518     
            Bias      | 0.0682            | -0.0134 | 0.1498  | 0.0860  | 0.0254   | 0.1464   | 0.0499  | -0.0536 | 0.1533"
  
# Set proper column names for LaTeX formatting
df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)
names(df) <- c(
  "Metrics","\\( prox_{mean} \\)", "\\( prox_1 \\)", "\\( prox_2 \\)",
  "\\( prox_{mean} \\)", "\\( prox_1 \\)", "\\( prox_2\\)",
  "\\( prox_{mean} \\)", "\\( prox_1 \\)", "\\( prox_2 \\)"
)
df <- df[-1, ]
row.names(df) <- NULL

# Simplified table without striped styling
kable(df, 
      format = "latex", 
      booktabs = TRUE, 
      escape = FALSE,
      caption = "Proximity evaluation metrics. The table shows the Pearson correlation coefficient (Pearson), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Coefficient of Determination (R²), and Bias for the proximity heuristic compared to human annotations."
) %>%
  kable_styling(latex_options = "scale_down") %>%
  add_header_above(c(" " = 1, "all faces" = 3, "adult faces" = 3, "child faces" = 3))
```

```{r proximity-plot, fig.align='center', fig.cap="The plot shows the correlation between the heuristic proximity values and the human-annotated proximity values for both adult and child faces. The red line represents the linear regression fit, while the shaded area indicates the 95% confidence interval.", out.width="450px"}
knitr::include_graphics("images/proximity_eval_plot.png")
```

Using this approach, the ground truth proximity for each frame is calculated as the mean of the two annotator ratings. We then compute how well the heuristic captures the human-annotated proximity values for both adult and child faces. Figure \@ref(fig:proximity-plot)  displays the correlation between the heuristic proximity values and the human-annotated proximity values for both adult and child faces. Both regression lines are close to the diagonal, indicating a strong correlation between the heuristic and human-annotated proximity values. Overall, the heuristic performs well, with a strong Pearson correlation coefficient of `r gt_heuristic_corr` for all faces, `r adult_gt_heuristic_corr` for adult faces and `r child_gt_heuristic_corr` for child faces. The mean absolute error (MAE=`r prox_mae`) for all faces is acceptably low and the coefficient of determination (R²=`r prox_r2`) for all faces indicates that the heuristic explains a significant portion of the variance in human-annotated proximity values. 
To conclude, the proximity heuristic provides a reliable estimate of interpersonal distance in our dataset, allowing us to add another reliable aspect of social interactions without the need for additional sensors or annotations.

## Participation Role Classifier {#sup-voice-direction}
Regarding the audio component of our interaction analysis, we aimed to classify speech not only by its presence but by its source and intended target. Specifically, we distinguish between four speech types: First, speech from the key child wearing the camera (KCHI), second, speech directed at the child from adults or other children (CDS), third, overheard speech (OHS), defined as conversations the child can hear but is not direcetly part of and fourth, general speech (SPEECH), which simply captures any type of speech. This distinction is crucial for understanding the interaction dynamics and the key child's role in the social context.

To achieve this, we adapted the Voice Type Classifier (VTC) [@lavechinOpensourceVoiceType2020], an open-source model originally trained to identify five voice types—key child, other child, female adult, male adult, and general speech. We restructured the model for our new four-class task and refer to this adapted version as the Participation Role Classifier (PRC).

Compared to the original VTC, the PRC enhanced model capacity and sequence modeling depth: the first-layer SincNet output channels were increased from 256 to 512, the bi-directional LSTM was replaced with a bi-directional GRU (3 layers, 512 hidden units), and the fully connected stack was deepened to 256–256–128 units. We also extended the input segment duration from 2.0 to 4.0 seconds to better capture turn-taking dynamics and adjusted the batch size accordingly. These changes improved the model’s ability to distinguish overlapping and context-dependent speech events relevant to our classification task.

```{r vtc-results, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Dataset   | KCHI | CDS  | OHS   | SPEECH  | AVG
            ChildLens | 72.3 | 65.3 | 48.7  |    cc   | 44.6"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)
names(df) <- unname(as.list(df[1,]))
df <- df[-1,] 
row.names(df) <- NULL
apa_table(
  df,
  caption = "Performance of the Participation Role Classifier (PRC) on the ChildLens dataset. The table shows the F1 scores for each voice type (KCHI, CDS, OHS, SPEECH) and the average F1 score across all classes.",
  escape = TRUE
)
```

Although the Quantex dataset lacks explicit audio labels, we validated the adapted model architecture on ChildLens, a similarly structured, labeled dataset from our lab (REFERENCE TO CHILDLENS PAPER). The PRC achieved an average F1 score of `r f1_avg` on ChildLens, (Table \@ref(tab:vtc-results)) comparable to the original VTC's F1 score (`r f1_vtc_benchmark`) on its benchmark dataset [@lavechinOpensourceVoiceType2020]. These results support the architecture’s robustness and suitability for our analysis.

## Results and Analysis {#sup-results}

\newpage
# References
<!--e used `r cite_r("bibliography.bib")` for all our analyses.-->    
```{r create_r-references}
r_refs(file = "bibliography.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage
# Appendix





