---
title             : "Exploring Aspects of Social Interaction using Machine Learning"
shorttitle        : "Exploring Aspects of Social Interaction using Machine Learning"
author:
  - name: "Nele-Pauline Suffo"
    affiliation: '1'
    corresponding: true
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "nele.suffo@leuphana.de"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name: "Pierre-Etienne Martin"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "pierre_etienne_martin@eva.mpg.de"
  - name: "Anam Zahra"
    affiliation: '2'
    corresponding: false
  - name: "Daniel Haun"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "daniel.haun@eva.mpg.de"
  - name: "Manuel Bohn"
    affiliation: '1, 2'
    corresponding: false
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "manuel.bohn@leuphana.de"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id: '1'
    institution: "Institute of Psychology in Education, Leuphana University Lüneburg"
  - id: '2'
    institution: "Max Planck Institute for Evolutionary Anthropology"
    
abstract: |
  Childrens everyday experiences are known to shape childrens development but only few studies investigate how children actually spent their time at home in naturalistic setting. More particular we were interested in how children's social interactions with others or interactions with objects are observable in their everyday life. To do so, we utilized the Quantex Dataset, an egocentric video and audio dataset of children aged 3-5 years, to investigate the presence of persons, faces, gaze, and objects in children's everyday interactions. We trained a YOLO11 model to detect persons and faces in the videos and analyzed the presence of gaze and objects in the videos. We furthermore applied a pre-trained voice type classifier to detect speech in the audio data. Our results show that children's everyday interactions are characterized by the presence of persons and faces, with the child's gaze directed towards others in 60% of the interactions. Additionally, children interacted with objects in 40% of the videos, with toys being the most common object category. Agr group analysis revealed that children aged 3 years showed more interactions with objects compared to older children. Our findings provide insights into the diversity of children's everyday experiences and highlight the importance of multimodal data for understanding children's social interactions and engagement.
  
keywords: "Quantex Dataset, egocentric video, audio dataset, children, social interactions, object interactions, gaze, multimodal data, computer vision, audio analysis, developmental psychology"
output:
  papaja::apa6_pdf:
    keep_tex: true
bibliography: "bibliography.bib"


floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
---


```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(ggplot2)
library(brms)
library(ggthemes)
library(ggpubr)
library(BayesFactor)
library(broom)
library(coda)
library(reshape2)
library(ggridges)
library(readxl)
library(dplyr)
library(lubridate)
library(zoo)
library(gridExtra)
library(grid)
library(kableExtra)
library(cowplot)
library(patchwork)
library(magick)
library(ggplotify)
library(png)
library(jpeg)


estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```
```{r quantex-statistics, echo=FALSE, message=FALSE, warning=FALSE}
quantex_data <- read_csv2("data/Quantex_data_sheet.csv")
quantex_subject_infos <- read_csv2("data/Quantex_subjects.csv")
quantex_subject_infos <- quantex_subject_infos %>%
  distinct(ID, .keep_all = TRUE)

quantex_unique_data <- quantex_data %>%
  distinct() %>%  # Removes duplicate rows
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes") %>%
  distinct(ID, Minutes_per_ID, .keep_all = TRUE)
quantex_unique_data <- quantex_unique_data %>%
  mutate(ID = as.double(ID))

quantex_cleaned_data <- quantex_data %>%
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes")
quantex_sum_videos <- nrow(quantex_cleaned_data)

quantex_data_gender_count <- quantex_unique_data %>%
  left_join(quantex_subject_infos, by = "ID") %>%
  distinct(ID, .keep_all = TRUE)
quantex_male_count <- quantex_data_gender_count %>%
  filter(gender == "Male") %>%
  nrow()
quantex_female_count <- quantex_data_gender_count %>%
  filter(gender == "Female") %>%
  nrow()

quantex_data_age_count <- quantex_cleaned_data %>%
  left_join(quantex_subject_infos, by = "ID")%>%
  select(ID, birthday, DATE)

quantex_data_age_count <- quantex_data_age_count %>%
  mutate(
    birthday = dmy(birthday),  # Convert birthday to Date type
    Date = dmy(DATE),          # Convert Date to Date type
    Age = as.numeric(difftime(Date, birthday, units = "weeks")) / 52.25,  # Calculate age in years
    Age_group = case_when(
      Age >= 2 & Age < 4 ~ "3+",
      Age >= 4 & Age < 5 ~ "4+",
      Age >= 5 ~ "5+",
      TRUE ~ NA_character_  # For cases where age is missing or not within the desired ranges
    )
  )

quantex_mean_age <- mean(quantex_data_age_count$Age, na.rm = TRUE)
quantex_sd_age <- sd(quantex_data_age_count$Age, na.rm = TRUE)

quantex_age_group_counts <- quantex_data_age_count %>%
  count(Age_group, name = "count")
quantex_count_3_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "3+"]
quantex_count_4_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "4+"]
quantex_count_5_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "5+"]

quantex_filtered_data <- quantex_data %>%
  filter(!is.na(Date), Include == "yes") %>%
  mutate(Date = as.Date(DATE, format = "%d.%m.%Y")) 

quantex_min_minutes <- min(quantex_unique_data$Minutes_per_ID[quantex_unique_data$Minutes_per_ID > 0], na.rm = TRUE)
quantex_max_minutes <- max(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sum_minutes <- sum(as.numeric(quantex_unique_data$Minutes_per_ID), na.rm = TRUE)
quantex_mean_minutes <- mean(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sd_minutes <- sd(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)

quantex_sum_hours <- quantex_sum_minutes/60

# Convert the time strings into seconds
quantex_unique_data$Seconds_per_ID <- as.numeric(hms::as_hms(quantex_unique_data$Minutes_per_ID))
total_seconds <- sum(quantex_unique_data$Seconds_per_ID, na.rm = TRUE)
total_hours <- total_seconds / 3600

quantex_nr_children <- nrow(quantex_unique_data)

quantex_oldest_date <- min(quantex_filtered_data$Date)
quantex_youngest_date <- max(quantex_filtered_data$Date)

quantex_oldest_year <- as.numeric(format(quantex_oldest_date, "%Y"))
quantex_oldest_month <- as.numeric(format(quantex_oldest_date, "%m"))
quantex_youngest_year <- as.numeric(format(quantex_youngest_date, "%Y"))
quantex_youngest_month <- as.numeric(format(quantex_youngest_date, "%m"))

# Calculate the interval between the two dates
quantex_time_span_months <- (quantex_youngest_year - quantex_oldest_year) * 12 + (quantex_youngest_month - quantex_oldest_month)+1

quantex_nr_annotated_videos <- 100
quantex_nr_train_frames <- 72687
quantex_nr_val_frames <- 7720
quantex_nr_test_frames <- 9272
quantex_nr_train_videos <- 51
quantex_nr_val_videos <- 6
quantex_nr_test_videos <- 7

```

# Introduction
According to various developmental psychologists, children’s everyday experiences play a vital role in their development [@piagetPartCognitiveDevelopment1964; @vygotskyMindSocietyDevelopment1978; @rogoffImportanceUnderstandingChildrens2018; @carpendaleWhatMakesUs2020; @smithDevelopingInfantCreates2018; @tomaselloCulturalOriginsHuman2009; @heyesCognitiveGadgetsCultural2018]. Everyday interactions, in particular, have been recognized for decades as crucial in the process of actively constructing knowledge [@piagetPartCognitiveDevelopment1964] and in transforming sensory experiences into structured understanding [@vygotskyMindSocietyDevelopment1978]. Building upon these foundational theories, more recent research has examined the mechanisms of social interaction further. For instance, @tomaselloCulturalOriginsHuman2009 introduced the concept of shared intentionality, illustrating how collaborative activities enable children to comprehend others’ intentions and perspectives, leading to cooperative behaviors and cultural learning .

Whereas theoretical frameworks and controlled laboratory studies have significantly advanced our understanding of children’s social development, they often fail to capture the complexities of interactions occurring in naturalistic settings. Observing children in their everyday environments offers a more authentic view of their social behaviors; however, this approach presents challenges due to the extensive data collection and analysis required.

To address these challenges, researchers have increasingly turned to data-driven approaches that utilize sensors and recording devices to gather objective data on social interactions. For instance, @onnelaUsingSociometersQuantify2014 employed wearable sensors to analyze social interactions in adult work settings, capturing the duration of close proximity between individuals. The study inferred that women were more talkative than men and more likely to be physically close to other women in group settings. @rossanoHow24yearold2022 examined social interactions among 31 two- to four-year-olds using 563 hours of video and audio recordings from a preschool during free play sessions over seven days. Manual interaction labels revealed that four-year-olds engaged in more cooperative social interactions and experienced fewer conflicts than two-year-olds, with object play and conversations being the most common forms of social engagement in both age groups. @daiLongitudinalDataCollection2022 investigated social interactions of 174 preschool children over three years, collecting voice and proximity data using wearable wireless RFID tags to study the co-development of social interactions and language acquisition. They employed manually labeled interaction data to train a temporal segment model that automatically identified periods of free play or class play, concluding that classmates frequently engaged in both contexts. @lemaignanPInSoRoDatasetSupporting2018 created a dataset comprising 45 hours of manually labeled social interactions between 45 child-child pairs and 30 child-robot pairs, including video and audio recordings, 3D facial data, skeletal information, and game interactions. By not providing specific instructions to the children, the researchers aimed to capture interactions in naturalistic settings. However, each laboratory session was limited to 40 minutes.

While these studies have advanced our understanding of social interactions, they often focus on controlled environments or are constrained by limited observation periods. Moreover, the manual data collection and analysis involved remain labor-intensive and time-consuming, and the current body of research lacks comprehensive data-driven studies analyzing children’s social interactions within their home environments.

To overcome these limitations, our study investigates social interactions in naturalistic home settings over an extended period. We have created the **Quantex** dataset which currently includes  `r quantex_sum_hours` hours of egocentric video and audio recordings from children aged 3 to 5 years and enables the analysis of specific patterns of social interactions, including:

- Presence of Individuals: Utilizing YOLO11 for person detection to identify when others are present in the child’s environment.
- Presence of Faces: Employing YOLO11 face detection to recognize faces the child encounters.
- Object Interactions: Analyzing the objects with which the child interacts using YOLO11 object detection
- Gaze Behaviors: Classifying gaze direction with YOLO11-cls to determine when others are looking at the child.
- Speech Dynamics: Implementing voice type classification to differentiate between the child’s speech and that of others, distinguishing between peers and adults.
	
The primary objectives of this study are to quantify interaction patterns by measuring the frequency and duration of each identified interaction type, both individually and in combination. Additionaly we compare these patterns across different age groups within the 3 to 5-year range to identify developmental variations and milestones. Understanding these interaction patterns can inform developmental psychology about the actual nature of social interactions in children’s everyday lives.

# Methodology
This chapter outlines the methodology used in this study to collect, annotate, and analyze video and audio recordings of children’s everyday interactions. The aim of the study is to investigate key aspects of social interactions and engagement, such as the presence of persons, faces, gaze direction, and objects the child interacts with. The following sections provide a detailed description of the data collection process, the structure and characteristics of the dataset, the annotation strategy, and the preprocessing applied to the data prior to analysis. Additionally, an overview of the automated analysis pipeline is provided, giving details about the models used for person and face detection, gaze classification, object detection, and the application of a pre-trained voice type classifier. 


## Data Collection
This study collected egocentric video recordings from `r quantex_nr_children` children, aged 3 to 5 years, over a span of `r quantex_time_span_months` months. Participating families lived in a mid-sized city in Germany. Data collection is ongoing, and the number of children will continue to increase as the study progresses. The data collection process was approved by the local ethics committee, and all participating families provided written informed consent, allowing the researchers to use the data for scientific purposes. In accordance with data privacy regulations, every child was assigned a unique anonymized ID to protect their identity. Moreover, the video recordings are stored on a secure server and are only accessible to the research team, all of whom have signed confidentiality agreements.

To capture the children’s everyday experiences, a wearable vest equipped with a _PatrolEyes WiFi HD Infrared Police Body Camera_ was used (Figure \@ref(fig:camera-cvat-activity-classes)). The camera recorded high-definition video (1920x1080p at 30 fps) with a 140-degree wide-angle lens and also captured audio. The children were free to move around and engage in their usual activities at home without any interference or instructions given to their parents. 

As of now, the ongoing data collection process has resulted in a total of `r quantex_sum_videos` video recordings, with a combined duration of `r quantex_sum_hours` hours.

## Dataset Overview
The Quantex dataset includes video and audio recordings from `r quantex_nr_children` children aged 3 to 5 years (M=`r quantex_mean_age`, SD=`r quantex_sd_age`). The dataset contains `r quantex_count_3_plus` videos from three-year-olds, `r quantex_count_4_plus` videos from four-year-olds, and `r quantex_count_5_plus` videos from five-year-olds. The number of videos per child varies, as parents decide when and how often to record. The recording duration per child ranges from `r quantex_min_minutes` to `r quantex_max_minutes` minutes (M=`r quantex_mean_minutes`, SD=`r quantex_sd_minutes`). The total duration of all video recordings in the dataset is `r quantex_sum_hours` hours. Figure \@ref(fig:quantex-minutes-per-child) shows the distribution of video duration per child.

```{r quantex-minutes-per-child, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Video recording duration (in minutes) per Child in the Quantex Dataset.", fig.height=2.5, fig.width=3.5}
quantex_unique_data$ID <- as.factor(quantex_unique_data$ID)

ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )
quantex_1_plot <- ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )

# Specify the file path and name
# <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
#file_name_1 <- "quantex_minutes_per_id_plot.png"
#quantex_file_path_1 <- file.path(quantex_1_output_folder, file_name_1)

# Save the plot
#ggsave(filename = quantex_file_path_1, plot = quantex_1_plot, width = 8, height = 6, dpi = 300)
```

## Annotation Strategy
The dataset annotations cover four key elements: persons, faces, gaze direction, and objects the child interacts with. For each detected person (or reflection of a person, such as in a mirror) and face, additional attributes, such as age and gender, are collected. Gaze information indicates whether a detected person’s gaze is directed toward the child or not. Faces are annotated even when occluded or blurry to ensure comprehensive coverage of interactions. Partially visible faces are also annotated if key facial features, such as the nose, eyes, or mouth, remain identifiable. Objects are annotated only when the child is actively interacting with them. These objects are categorized into six distinct groups: book, screen, animal, food, toy, and kitchenware, with an additional category for other objects. The annotation strategy is summarized in Figure \@ref(fig:camera-cvat-activity-classes).

The annotations were generated manually by a team of human annotators. Each video was randomly assigned to an initial annotator, and then reviewed by a second annotator to ensure consistency and accuracy. This peer review process helped to identify and resolve discrepancies, ensuring high-quality annotations. 

```{r camera-cvat-activity-classes, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A} – Vest with the embedded camera worn by the children, \\textbf{B} – CVAT platform utilized for video annotation, \\textbf{C} – Annotation Strategy in the Quantex dataset."}
img1 <- ggdraw() + draw_image("images/camera_worn_close.png", scale = 0.8)
img2 <- ggdraw() + draw_image("images/cvat.png", scale = 0.8)
img3 <- ggdraw() + draw_image("images/quantex_annotation_strategy_narrow.png", scale = 0.9)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(0.4, 1))

final_layout <- (top_row / img3) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

## Data Preprocessing
For the video data, the annotation strategy required persons, faces, and objects to be labeled even when only partially visible, as long as key features such as facial landmarks (e.g., nose, eyes, or mouth) or parts of a person or object were clearly visible. To prepare the video data for analysis, one frame per second was annotated, corresponding to every 30th frame in the video. This frame sampling was chosen to balance the need for a representative sample of the video while keeping the analysis manageable. Similarly, every 30th raw frame was extracted from the annotated video files for further processing. No preprocessing was applied to the audio data, which was used in its raw form for analysis.

## Automated Analysis Pipeline
Our automated analysis pipeline consists of four key modules: person and face detection, gaze classification, object detection, and voice type classification. Each module operates independently, utilizing separate machine learning models. Except for the voice type classifier, all models were trained on the Quantex dataset.

The pipeline follows a sequential process:
	1.	YOLO11x detection model identifies the presence of individuals (persons and faces) and types of objects the key child interacts with, both in social and independent play contexts, in the video frames.
	2.	Gaze classification determines whether detected faces are looking at the child.
	3.	Voice type classification detects the presence of speech and identifies whether the speaker is the key child, another child, or an adult.

By integrating these modules, our pipeline enables a comprehensive analysis of children’s everyday experiences, capturing both social interactions and independent play

In the following sections, we describe each module in detail, including training data, model architecture, and evaluation metrics. A full technical analysis of each algorithm is provided in the [Supplementary Material].

### Person & Face Detection
	
### Gaze Classification

### Object Detection 

### Voice Detection and Classification


# Results

## Presence of Aspects of Social Interaction

### Presence of a Person

### Presence of a Face

### Presence of Gaze

### Presence of Language
## Co-occurrence of Aspects of Social Interaction 




# General Discussion

\newpage
# References

\newpage
# Supplementary Material
## YOLO11x: Multi-Class Detection of Persons, Faces, and Objects
```{r yolo-person-statistics, echo=FALSE, message=FALSE, warning=FALSE}
det_total_num_frames <- 113799
det_total_num_videos <- 80
det_num_frames_train <- 91038
det_class_to_total_ratio <- 29.75

num_det_frames_train <- 91038
num_det_frames_val <- 11379
num_det_frames_test <- 11382

det_recall <- 0.79
det_precision <- 0.89
det_f1_score <- 0.84
det_map <- 0.858

tp_child <- 0.81
tp_adult <- 0.91
tp_child_face <- 0.82
tp_adult_face <- 0.91
tp_child_body_parts <- 0.94
tp_book <- 0.9
tp_screen <- 0.93
tp_toy <- 0.76
tp_kitchenware <- 0.75
tp_other_object <- 0.79

fn_kitchenware <- 0.22
fn_toy <- 0.22
fn_other_object <- 0.19

ap_toy <- 0.77
ap_kitchenware <- 0.77
ap_other_object <- 0.82

det_num_epochs <- 86
det_training_time <- 200
```

In our study, we utilized Ultralytics' YOLO11, the "latest iteration in the Ultralytics YOLO series of real-time object detectors" [@jocherUltralyticsYOLO112024], trained on the COCO dataset. Released in October 2024, YOLO11 introduces architectural improvements such as the C2PSA block (Convolutional Block with Parallel Spatial Attention), which enhances spatial attention within feature maps, allowing the model to focus more precisely on critical areas of an image compared to previous YOLO versions. Additionally, YOLO11 incorporates the C3K2 block, designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process [@khanamYOLOv11OverviewKey2024]. These advancements make the YOLO11 detection model, pretrained on COCO, well-suited for training on our egocentric dataset, which captures dynamic movements from a camera perspective on chest height.

### Dataset Preprocessing
Our dataset presents unique challenges due to its egocentric viewpoint, as the body parts of the child wearing the camera frequently appear in the footage. To prevent misclassification, we adopt a dedicated annotation scheme where all individuals in the scene are labeled as “person,” each assigned a unique ID. The key child, who wears the camera, is consistently assigned ID = 1. During preprocessing, we map the key child (ID = 1) into a separate category, “child body parts,” to distinguish their presence from other individuals. Furthermore, we refine the “person” and “face” categories by introducing additional distinctions that standard YOLO models do not inherently make. Specifically, we differentiate between (1) the key child (who wears the camera) and other individuals, (2) adults and children/infants for both full-body detections and faces. These modifications allow for a more precise analysis of social interactions while reducing false positives caused from the key child’s own body while capturing detailed information on the presence and classification of people and objects in the child’s environment. To address these challenges, our fine-tuned YOLO11 model is trained to:

- Recognize and differentiate between the key child’s body parts and other individuals.
- Distinguish between adults and children/infants for both full-body detections and faces.
- Identify and classify six specific object categories (toy, book, food, kitchenware, screen, other object) relevant to the child’s interactions.

While our dataset originally included seven object categories, we merged "animal" and "food" into the "other object" category due to their low occurrences (19 and 1,115 instances, respectively), whereas all other categories had more than 2,000 instances. As a result, our final model was trained on five object categories.

### Dataset Splitting
We started data splitting with a dataset comprising a total of `r det_total_num_frames` frames from `r det_total_num_videos` annotated videos. Prior to splitting this dataset into training, validation, and testing datasets, we analyzed how often each class was present in the dataset. The class distribution, displayed in detail in table \@ref(tab:det-class-distribution), revealed that the dataset was imbalanced, with the "neither" class (frames without any of the relevant classes) being the most frequent. To address this imbalance, we applied a stratified split to ensure that each dataset preserved the original class distribution. As a result, the final data distribution (see figure \@ref(det-dataset-splits)) consisted of `r num_det_frames_train` frames in the training dataset, `r num_det_frames_val` frames in the validation and `r num_det_frames_test` frames in the testing dataset, guaranteeing that the model's performance evaluation remained accurate to the real-world data distribution.

```{r det-class-distribution, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Class                   | Training  | Validation  | Testing | Total
            Adult                   | 25706     | 3213        | 3213    | 32132
            Child/Infant            | 22403     | 2801        | 2800    | 28004
            Adult Face              | 669       | 1083        | 1084    | 10836
            Child/Infant Face       | 6756      | 844         | 845     | 8445
            Book                    | 8370      | 1046        | 1046    | 10462
            Toy                     | 13870     | 1734        | 1733    | 17337
            Kitchenware             | 1915      | 239         | 240     | 2394
            Screen                  | 3374      | 422         | 422     | 4218
            Other Object            | 15608     | 1950        | 1950    | 19508
            Neither                 | 32341     | 4004        | 4005    | 40350"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Dataset splits for the YOLO11 detection model trained on the Quantex dataset. The table shows the distribution of annotated persons, faces, and objects in the training, validation, and testing datasets.",
  escape = FALSE
)
```

```{r det-dataset-splits, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "                  | Training  | Validation  | Testing | Total
            Number of images  | 91039     | 11380       | 11380   | 113799"
  
df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Number of images in the training, validation, and testing datasets for the YOLO11 detection model.",
  escape = FALSE
)
```
### Training and Convergence
Model training was conducted on a Linux server equipped with an Intel(R) Xeon(R) Silver 4214Y CPU @ 2.20GHz with 48 cores, a Quadro RTX 8000 GPU and 188 GB of RAM. The model was trained for a total of `r det_num_epochs` epochs, taking `r det_training_time` hours to complete. Training utilized YOLO11’s built-in data augmentation, an image size of 640, a batch size of 16, a cosine annealing learning rate scheduler [@loshchilovSGDRStochasticGradient2017], and early stopping after 10 epochs without improvement, with a maximum of 200 epochs.

```{r det-loss-curves, fig.align='center', fig.cap="Training and Validation Loss Curves for the YOLO11x detection model.", out.width="450px"}
knitr::include_graphics("images/yolo_face_loss_curves.png")
```

The loss function of the YOLO11 model comprises three main components: Box Loss, Classification Loss, and Distribution Focal Loss (DFL) [@tervenLossFunctionsMetrics2024; @liGeneralizedFocalLoss2020]. _Box Loss_ quantifies the difference between predicted bounding boxes and ground truth boxes, ensuring precise localization of detected persons, faces and objects by penalizing inaccuracies in position and size. _Classification Loss_ evaluates the model’s ability to correctly assign detected instances to their respective classes, reducing false positives and false negatives. _Distribution Focal Loss_ enhances the model's ability to detect challenging persons, faces and objects, particularly small or partially occluded ones, by refining the localization of bounding box coordinates and emphasizing hard-to-detect instances. Together, these loss components contribute to a more robust and accurate detection model. 

During the training process, we observed that all three loss components decreased over time, indicating effective learning and improved performance, as visible in in figure \@ref(fig:det-loss-curves). A steady decrease in Box Loss indicates that the model is becoming increasingly accurate in localizing persons, faces and objects within frames. Similarly, the steady convergence of the Classification Loss reveals the model's increasing ability to reliably classifiy the detected instance in one of the relevant classes. The decrease in DFL over time indicates that the model is getting better at focusing on and correctly identifying difficult-to-detect persons, faces or objects, which improves its overall detection capabilities. Conclusively, the loss curves show that the model effectively learned to localize and identify the target classes during the training period.

```{r det-detection-metrics-detailed, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Precision | Recall  | F1-Score  | FPR   | FNR
            0.92      | 0.87    | 0.90      | 2.72  | 10.94"
df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11x detection model trained on the Quantex dataset to detect persons, faces and six object classes. False Positive Rate (FPR) and False Negative Rate (FNR) are given in percentages.",
  escape = TRUE
)
```

```{r det-metrics, fig.align='center', fig.cap="\\textbf{A} - Confusion Matrix for the YOLO11x detection model trained on the Quantex dataset. \\textbf{B} - Precision-Recall Curve for the YOLO11x detection model."}
# Load images
img1 <- ggdraw() + draw_image("images/confusion_matrix_detection.png", scale = 1)
img2 <- ggdraw() + draw_image("images/yolo_precision_recall_curve.png", scale = 1.2)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row) +
  plot_layout(heights = c(1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```


```{r det-detection-examples, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A}, \\textbf{B} - Examples of True Positives, \\textbf{C}, \\textbf{D} – Examples of False Negatives, \\textbf{E}, \\textbf{F} – Examples of False Positives in the YOLO11 person detection model."}
img1 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img5 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)
img6 <- ggdraw() + draw_image("images/quantex_at_home_id255944_2022_03_07_01_002760.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
middle_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
bottom_row <- (img5 + img6) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row / middle_row / bottom_row) +
  plot_layout(heights = c(1, 1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```
### Model Evaluation Metrics
The performance of the object detection model was evaluated using a confusion matrix and precision-recall (PR) curves. The YOLO11 model achieved a precision of `r det_precision` and a recall of `r det_recall` on the testing set, resulting in an F1-score of `r det_f1_score`. The normalized confusion matrix (see figure \@ref(fig:det-metrics)) reveals strong overall performance with mean Average Precision across all classes mAP=`r det_map`. The averaged precision-recall curve remains close to the top-left corner and signifies that the model maintains high precision and recall across various thresholds, underscoring its effectiveness in detecting the different classes. Notably, the confusion matrix demonstrates minimal confusion between most classes, indicating that the model can effectively distinguish between them (see figure \@ref(fig:det-metrics)).

Analysis of the confusion matrix reveals that the classes "infant/child" (`r tp_child`), "adult" (`r tp_adult`), "adult face" (`r tp_adult`), "child body parts" (`r tp_child_body_parts`), "infant/child face" (`r tp_child_face`), "book" (`r tp_book`) and "screen" (`r tp_screen`) exhibit high Average Precision (AP) scores in the PR curves (see figure \@ref(fig:det-metrics)), signifying excellent precision and recall.

Contrarily, the "toy", "kitchenware" and "other object" classes show lower performance, with AP scores of `r ap_toy`, `r ap_kitchenware`, and `r ap_other_object`, respectively. The confusion matrix supports this observation, indicating that `r fn_kitchenware*100`% of "kitchenware" and `r fn_toy*100`% of "toy" items are frequently misclassified as "background". This misclassification is likely due to the annotation strategy, which prioritized labeling only the objects with which the child directly interacts. As a result, many other objects in the scene remain unannotated, despite being visually similar to the target objects, leading to confusion during training. Overall, the object classes appear to be more challenging for the model, particularly "kitchenware", "toy" and "other object". These results suggest that increasing the number of annotated samples, particularly for the objects the child directly interacts with, could improve detection performance. Additionally, future work could also explore model adjustments or data augmentation techniques to enhance the model’s ability to distinguish these difficult classes, especially for small or partially visible objects.

Overall, the YOLO11 model demonstrates robust performance in detecting persons, faces and objects. However, challenges remain in detecting certain object classes, particularly "kitchenware", "toy" and "other object", which require further investigation to improve model performance. These findings underscore the complexities inherent in analyzing egocentric video data, where movement and varying perspectives introduce additional challenges.

## YOLO11x-cls: Gaze Classification
```{r gaze-statistics, echo=FALSE, message=FALSE, warning=FALSE}
total_num_gaze_frames <- 20889
final_num_gaze_frames <- 32902
gaze_total_num_videos <- 64
num_gaze_frames_train <- 73364
gaze_to_no_gaze_ratio <- 21.25

num_gaze_frames_train <- 26320
num_gaze_frames_val <- 2088
num_gaze_frames_test <- 2091

gaze_tp <- 343
gaze_fp <- 195
gaze_fn <- 102
gaze_tn <- 1443
gaze_fpr <- gaze_fp / (gaze_fp + gaze_tn) * 100
num_gaze <- gaze_tp + gaze_fn
per_wrong_gaze <- gaze_fp / gaze_tp * 100
gaze_recall <- 0.77
gaze_precision <- 0.64
gaze_f1_score <- 0.70

gaze_num_epochs <- 37
gaze_training_time <- 10.4
```

Selecting an appropriate model for gaze classification in our automated pipeline presented unique challenges due to the egocentric perspective of our dataset. Many gaze estimation methods rely on high-quality eye images, either extracted separately [@zhangAppearancebasedGazeEstimation2015] or as part of the full face [@zhangItsWrittenAll2016]. However, our dataset often contains blurry or partially occluded faces captured at varying angles, making such approaches not suitable. Additionally, rather than predicting fine-grained gaze direction (e.g., left or right), our focus is on the binary classification of whether a person’s gaze is directed toward the child.

[@chengAppearancebasedGazeEstimation2021] provides an overview of the challenges and recent advancements in gaze estimation methods, including approaches that incorporate temporal information, such as Gaze360 [@kellnhoferGaze360PhysicallyUnconstrained2019]. While these methods improve tracking across frames, they are not the primary focus of our study, as we analyze social interactions on a frame-by-frame basis.

Given these constraints, we opted for a CNN-based approach trained on ground truth annotations. Recent studies [@zhangMPIIGazeRealWorldDataset2019; @zhangETHXGazeLargeScale2020; @shahDriverGazeEstimation2022] have explored different CNN based gaze estimation architectures, among which are VGG, ResNet or YOLO architectures. Based on these findings, we implemented a VGG, ResNet, and YOLO-based model for binary classification (gaze directed toward the child or not). In our preliminary tests, Ultralytics’ YOLO11 architecture [@jocherUltralyticsYOLO112024] demonstrated the best performance, leading us to select the YOLO11x classification model, pretrained on ImageNet, for our gaze classification task.

Given these constraints, we opted for a CNN-based approach trained on ground truth annotations. Recent studies [@zhangMPIIGazeRealWorldDataset2019; @zhangETHXGazeLargeScale2020; @shahDriverGazeEstimation2022] have explored different CNN-based gaze estimation architectures, including VGG, ResNet, and YOLO. Based on these findings, we implemented and compared three models; VGG, ResNet, and YOLO; for binary classification (gaze directed toward the child or not). In our preliminary tests, Ultralytics’ YOLO11 architecture [@jocherUltralyticsYOLO112024] demonstrated the best performance, leading us to select the YOLO11x classification model, pretrained on ImageNet, for our gaze classification task.

The egocentric nature of our video data, recorded from a chest-mounted camera, posed additional challenges such as motion blur and oblique viewing angles, making gaze classification difficult even for human annotators. To address this, we defined gaze as “directed toward the child” when a person’s gaze was oriented toward the child’s face, body, or general direction. This included cases where the person was looking directly at the camera (worn by the child) or slightly upward, estimating the likely position of the child’s head. Each detected face was annotated as either “gaze” (gaze directed at the child) or “no gaze” (gaze directed elsewhere).

YOLO11’s architectural enhancements, such as the C2PSA block, improve the model’s ability to focus on critical regions within an image, making it well-suited for our task. We fine-tuned the pretrained YOLO11x-cls model for gaze classification, enabling it to robustly determine whether individuals in the scene were engaged in visual attention toward the child.

### Dataset Splitting
For the gaze classification task, we utilized a subset of the Quantex dataset, focusing specifically on frames containing annotated faces. This subset consisted of cut-out faces extracted from the annotated face bounding boxes, resulting in `r total_num_gaze_frames` frames from `r gaze_total_num_videos` annotated videos. An analysis of the gaze labels showed that `r gaze_to_no_gaze_ratio`% of the faces were annotated as having gaze directed toward the child, leading to a class imbalance.

To maintain the original distribution of gaze labels across datasets, we first applied stratified dataset splitting, ensuring that the training, validation, and testing sets reflected the natural ratio of gaze and non-gaze frames. To address class imbalance in the training dataset, we applied data augmentation to increase the number of gaze frames. Since our dataset was already a subset of the Quantex dataset, downsampling to achieve balance would have resulted in a loss of valuable data. As a result, the final dataset included a balanced training set with 50% gaze and 50% no-gaze frames, while the validation and testing sets retained their original, unbalanced distribution. The final data distribution is presented in Table \@ref(tab:gaze-dataset-splits), with `r num_gaze_frames_train` frames in the training set, `r num_gaze_frames_val `frames in the validation set, and `r num_gaze_frames_test` frames in the testing set.

```{r gaze-dataset-splits, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Quantex   | Train Ratio | Training  | Val & Test Ratio  | Validation  | Testing | Total
            Gaze      | 50          | 13160     | 79                | 1645        | 1646    | 16451
            No Gaze   | 50          | 13160     | 21                | 443         | 445     | 14048
            Total     | 100         | 26320     | 100               | 2088        | 2091    | 30499"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Dataset splits for the YOLO11x gaze classification model trained on the Quantex dataset. The table shows the total number of frames, as well as the number of frames with gaze and no gaze in the training, validation, and testing datasets after data augmentation of the minory class (Gaze). 'Gaze' indicates frames where the person's gaze is directed towards the child, while 'No Gaze' indicates frames where the person's gaze is not directed towards the child. Ratios are given in percentages.",
  escape = TRUE
)
```
### Training and Convergence
We trained the gaze classification model on the same Linux server used for person and object detection model training. Training ran for `r gaze_num_epochs` epochs, completing in `r gaze_training_time` hours. Similar to the YOLO detection model, we used an input image size of 640, a batch size of 16, a cosine annealing learning rate scheduler [@loshchilovSGDRStochasticGradient2017], and early stopping after 10 epochs without improvement, with a maximum of 200 epochs.

```{r gaze-loss-confusion-matrix, fig.align='center', fig.cap="\\textbf{A} - Training and Validation Loss Curves for the YOLO11x gaze classification model. \\textbf{B} - Confusion Matrix for the YOLO11x gaze classification model trained on the Quantex dataset."}
# Load images
img1 <- ggdraw() + draw_image("images/gaze_loss_curves.png", scale = 1)
img2 <- ggdraw() + draw_image("images/confusion_matrix_gaze.png", scale = 1)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row) +
  plot_layout(heights = c(1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

Figure \@ref(fig:gaze-loss-confusion-matrix) shows the cross-entropy loss curves for both training and validation. The steady decline in loss over time indicates that the model effectively learned to classify whether a person is looking in the direction of the child or not. The convergence of the training and validation loss curves suggests that the model generalizes well, as there is no indication of overfitting. Since gaze classification is a binary task, we used top-1 accuracy as the primary evaluation metric. Figure \@ref(fig:gaze-loss-confusion-matrix) also illustrates the increasing accuracy over time, confirming that the model progressively improves its ability to classify gaze correctly.

### Model Evaluation Metrics
The YOLO11x gaze classification model achieved a precision of `r gaze_precision`, a recall of `r gaze_recall`, and an F1-score of `r gaze_f1_score` on the testing set. These metrics, summarized in table \@ref(tab:gaze-metrics), indicate that the model effectively distinguishes between gaze and no-gaze frames, though there is still room for improvement. The egocentric perspective of the dataset presents challenges, as faces are often partially occluded or blurred, making gaze classification difficult—even for human annotators, who occasionally required a second inspection to determine gaze direction.

```{r gaze-metrics, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Precision | Recall  | F1-Score
            0.63      | 0.77    | 0.70"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Evaluation metrics for the YOLO11x gaze classification model trained on the Quantex dataset to classify whether a person is looking into the direction of the child wearing the camera or not. Precision, recall, and F1-score are given for the testing set.",
  escape = TRUE
)
```

One of the primary challenges arises from cut-off faces, where the eyes are not always visible. In such cases, the model must rely on other facial features, such as head orientation or mouth position, to infer gaze direction. While this approach is often effective, it occasionally leads to misclassifications. Despite these difficulties, the model achieved a satisfactory recall, correctly identifying `r gaze_recall*100`% of all gaze frames. The F1-score of 0`r gaze_f1_score*100` balances precision and recall, providing a comprehensive measure of the model’s performance.

More advanced gaze estimation methods—such as incorporating temporal information or leveraging additional facial landmarks—could further improve performance. However, given the constraints of our dataset and the focus on binary gaze classification, the YOLO11x model serves as a strong foundation for analyzing the gaze aspect of social interactions in egocentric video data.

## Proximity Heuristic
Proximity between individuals is an important aspect of social interaction. Previous studies have shown that interpersonal distance can provide insights into the nature of relationships and social engagement [@janssenTrackingRealtimeProximity2024; @hernandez-herediaProximitySensorMeasuring2024; @onnelaUsingSociometersQuantify2014]. Since our dataset does not contain explicit proximity labels, we developed a heuristic approach to estimate the distance between the child and other individuals.

Our heuristic relies on the bounding boxes of detected faces, assuming that the size of these bounding boxes provides an implicit cue for proximity. Specifically, we infer that larger bounding boxes correspond to individuals who are closer to the camera, whereas smaller bounding boxes indicate individuals positioned further away. To quantify this relationship, we calculate the area of a detected face bounding box as the product of its width and height. This computed size is then compared to a predefined reference bounding box. For both adults and children/infants, we selected two reference face sizes: one corresponding to a face very close to the camera (within arm’s reach) and another representing a face further away (e.g., in the background or outdoors). By normalizing the detected face sizes relative to these reference sizes, we obtain a proximity score ranging from 0 to 1, where a value of 1 indicates a face extremely close to the camera, and 0 represents a face that is far away.

```{r proximity-classifications, echo=FALSE, dpi=600, fig.align='center', fig.cap="Proximity heuristic examples. Example \\textbf{A} shows a face far away from the camera (proximity score = 0.07), example \\textbf{B} depicts a face slighlty closer (proximity score = 0.2). Example \\textbf{C} shows a face quite close to the camera (proximity score = 0.6), and example \\textbf{D} illustrates a face extremely close to the camera (proximity score = 1)."}


img1 <- ggdraw() + draw_image("images/face_0.jpg", scale = 0.95)
img2 <- ggdraw() + draw_image("images/face_02.jpg", scale = 0.95)
img3 <- ggdraw() + draw_image("images/face_06.jpg", scale = 0.95)
img4 <- ggdraw() + draw_image("images/face_1.jpg", scale = 0.95)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(1, 1))

# Combine img1 and img2 into the first row with equal heights
bottom_row <- (img3 + img4) + 
  plot_layout(widths = c(1, 1))

final_layout <- (top_row / bottom_row) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

To better account for the nonlinear relationship between face size and distance, we applied a logarithmic transformation to the bounding box areas. Since face area decreases quadratically as distance increases, a simple linear mapping would make small differences appear too large for close faces and too small for distant ones. The logarithmic transformation helps balance this by compressing large values and stretching smaller ones, creating a more natural and meaningful proximity scale. This approach aligns with how humans perceive size, as our sensitivity to changes depends on relative differences rather than absolute ones [@stevensPsychophysicsIntroductionIts2017]. By using this scaling, we ensure that proximity estimates remain consistent across different face sizes and distances.

In addition to absolute size, we incorporate a width-to-height ratio as an additional cue for proximity estimation. This ratio was determined separately for adult and child/infant faces based on reference images depicting full, front-facing faces. If a detected face exhibits a significant deviation from the expected aspect ratio, we infer that the face is partially cropped due to extreme proximity to the camera. In such cases, the proximity score is set to 1. An example illustrating this case is shown in Figure \@ref(fig:proximity-close-face).

This heuristic provides an interpretable and computationally efficient method for estimating proximity in egocentric recordings, allowing us to examine the spatial dynamics of social interactions without the need for explicit depth information or additional sensors.

## Voice Type Classification
Regarding the audio component of our interaction analysis, we aimed to use a model that does not only detect the presence of speech but also distinguishes between the key child wearing the camera and other speakers. This distinction is crucial for understanding the dynamics of the interactions and the role of the key child in the social context. To achieve this, we applied the Voice Type Classifier [@lavechinOpensourceVoiceType2020], an open-source model designed to identify five different voice types: key child, other child, female adult, male adult, and speech in general. The model is based on a convolutional neural network (CNN) architecture and was trained on 260 hours of child-centered recordings across 10 different languages. 

Although the Quantex dataset does not include explicit audio labels, we are confident in the model’s suitability for our data. Prior testing on a similar labeled dataset which was also collected in our lab, ChildLens, demonstrated that the Voice Type Classifier achieved an F1 score of 58.1 (reference to ChildLens paper), which is comparable to the F1 score of 57.3 reported on the original training dataset. These results indicate that the model generalizes well to our child-centered recordings, making it a reliable choice for our analysis.

We applied the Voice Type Classifier to the extracted audio data from the Quantex dataset using the code provided by the authors of the voice type classifier [@lavechinVoiceTypeClassifier2020]. The model was used to detect the presence of speech and classify the speaker into one of the five predefined categories.

\newpage
# References
<!--e used `r cite_r("bibliography.bib")` for all our analyses.-->    
```{r create_r-references}
r_refs(file = "bibliography.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage
# Appendix





