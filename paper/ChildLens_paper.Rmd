---
title             : "ChildLens: An Egocentric Video Dataset for Activity Analysis in Children"
shorttitle        : "ChildLens Dataset"
author:
  - name: "Nele-Pauline Suffo"
    affiliation: '1'
    corresponding: true
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "nele.suffo@leuphana.de"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name: "Pierre-Etienne Martin"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "pierre_etienne_martin@eva.mpg.de"
  - name: "Daniel Haun"
    affiliation: '2'
    corresponding: false
    address: "Deutscher Pl. 6, 04103 Leipzig"
    email: "daniel.haun@eva.mpg.de"
  - name: "Manuel Bohn"
    affiliation: '1, 2'
    corresponding: false
    address: "Universitätsallee 1, 21335 Lüneburg"
    email: "manuel.bohn@leuphana.de"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id: '1'
    institution: "Institute of Psychology in Education, Leuphana University Lüneburg"
  - id: '2'
    institution: "Max Planck Institute for Evolutionary Anthropology"
    
abstract: |
  We present ChildLens, an egocentric video and audio dataset capturing naturalistic everyday experiences in in children aged 3–5 years and including detailed activity labels. A total of 106 hours of experiences were recorded from 61 children in their home environment using a 140° wide-lens camera equipped with a microphone embedded in a child-friendly vest. Annotations include five location classes and 14 activity classes, covering audio-only, video-only, and multimodal activities. Captured through a vest equipped with an embedded camera, ChildLens provides a rich resource for analyzing children’s daily interactions and behaviors. We provide an overview of the dataset, the collection process, and the labeling strategy. Additionally, we present benchmark performance of two state-of-the-art models on the dataset: the Boundary-Matching Network for temporal activity localization and the Voice-Type Classifier for detecting and classifying speech in audio. Finally, we analyze the dataset specifications and their influence on model performance. The ChildLens dataset will be freely available for research purposes via an institutional repository (listed on the [ChildLens website](https://www.eva.mpg.de/comparative-cultural-psychology/technical-development/childlens/)). It provides rich data to advance computer vision and audio analysis techniques and thereby removes a critical obstacle for the study of the context in which children develop.
keywords: "child development, egocentric video, audio dataset, multimodal learning, computer vision, developmental psychology"

output:
  papaja::apa6_pdf:
    keep_tex: true
bibliography: "bibliography.bib"


floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
---


```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(ggplot2)
library(brms)
library(ggthemes)
library(ggpubr)
library(BayesFactor)
library(broom)
library(coda)
library(reshape2)
library(ggridges)
library(readxl)
library(dplyr)
library(lubridate)
library(zoo)
library(gridExtra)
library(grid)
library(kableExtra)
library(cowplot)
library(patchwork)
library(magick)
library(ggplotify)


estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```

```{r video-statistics, echo=FALSE, message=FALSE, warning=FALSE}

annotation_progress <- 49
data <- read_csv2("data/ChildLens_data_sheet.csv")
subject_infos <- read_csv2("data/ChildLens_subjects.csv")
subject_infos <- subject_infos %>%
  distinct(ID, .keep_all = TRUE)

unique_data <- data %>%
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Labeled == "yes") %>%
  distinct(ID, Minutes_per_ID, .keep_all = TRUE)
unique_data <- unique_data %>%
  mutate(ID = as.double(ID))

cleaned_data <- data %>%
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Labeled == "yes")
sum_videos <- nrow(cleaned_data)

data_gender_count <- unique_data %>%
  left_join(subject_infos, by = "ID") %>%
  distinct(ID, .keep_all = TRUE)
male_count <- data_gender_count %>%
  filter(gender == "Male") %>%
  nrow()
female_count <- data_gender_count %>%
  filter(gender == "Female") %>%
  nrow()

data_age_count <- cleaned_data %>%
  left_join(subject_infos, by = "ID")%>%
  select(ID, birthday, Date)

data_age_count <- data_age_count %>%
  mutate(
    birthday = dmy(birthday),  # Convert birthday to Date type
    Date = dmy(Date),          # Convert Date to Date type
    Age = as.numeric(difftime(Date, birthday, units = "weeks")) / 52.25,  # Calculate age in years
    Age_group = case_when(
      Age >= 3 & Age < 4 ~ "3+",
      Age >= 4 & Age < 5 ~ "4+",
      Age >= 5 ~ "5+",
      TRUE ~ NA_character_  # For cases where age is missing or not within the desired ranges
    )
  )

mean_age <- mean(data_age_count$Age, na.rm = TRUE)
sd_age <- sd(data_age_count$Age, na.rm = TRUE)

age_group_counts <- data_age_count %>%
  count(Age_group, name = "count")
count_3_plus <- age_group_counts$count[age_group_counts$Age_group == "3+"]
count_4_plus <- age_group_counts$count[age_group_counts$Age_group == "4+"]
count_5_plus <- age_group_counts$count[age_group_counts$Age_group == "5+"]

filtered_data <- data %>%
  filter(!is.na(Date), Labeled == "yes") %>%
  mutate(Date = as.Date(Date, format = "%d.%m.%Y")) 

min_minutes <- min(unique_data$Minutes_per_ID[unique_data$Minutes_per_ID > 0], na.rm = TRUE)
max_minutes <- max(unique_data$Minutes_per_ID, na.rm = TRUE)
sum_minutes <- sum(unique_data$Minutes_per_ID, na.rm = TRUE)
mean_minutes <- mean(unique_data$Minutes_per_ID, na.rm = TRUE)
sd_minutes <- sd(unique_data$Minutes_per_ID, na.rm = TRUE)

sum_hours <- sum_minutes/60
nr_children <- nrow(unique_data)

oldest_date <- min(filtered_data$Date)
youngest_date <- max(filtered_data$Date)

oldest_year <- as.numeric(format(oldest_date, "%Y"))
oldest_month <- as.numeric(format(oldest_date, "%m"))
youngest_year <- as.numeric(format(youngest_date, "%Y"))
youngest_month <- as.numeric(format(youngest_date, "%m"))

# Calculate the interval between the two dates
time_span_months <- (youngest_year - oldest_year) * 12 + (youngest_month - oldest_month)+1
```

# Introduction
In developmental psychology, everyday experiences play a central role when theorizing about the causes and dynamics of developmental change [@piagetPartCognitiveDevelopment1964; @vygotskyMindSocietyDevelopment1978; @rogoffImportanceUnderstandingChildrens2018; @carpendaleWhatMakesUs2020; @smithDevelopingInfantCreates2018; @tomaselloCulturalOriginsHuman2009; @heyesCognitiveGadgetsCultural2018]. Famously, the two central processes in Piaget's theory of cognitive development - assimilation and accommodation - describe how children's cognitive abilities change in line with the experiences they make [@vygotskyMindSocietyDevelopment1978]. Vygotsky emphasized the role of social interactions between children and (knowledgeable) adults for the acquisition of culturally relevant knowledge [@vygotskyMindSocietyDevelopment1978]. Contemporary theorists rest on similar ideas. For example, @tomaselloCulturalOriginsHuman2009 pointed out how everyday social interactions, particularly those involving shared intentionality, foster uniquely human forms of communication, cooperation and cognition. For @heyesCognitiveGadgetsCultural2018, culturally evolved "cognitive gadgets" are transmitted via language in conversations between adults and children. 

These broad theoretical accounts are based on empirical support. Reviewing all such studies is beyond the scope of this paper so we want to point out only a few examples from the domain of language development. Across languages and cultural settings, the amount of language children hear is related to their language development [@bergelsonEverydayLanguageInput2023]. There is also a relationship between the amount of conversational turn-taking children are engaged in and their vocabulary growth [@ferjanramirezParentCoachingIncreases2020; @donnellyLongitudinalRelationshipConversational2021]. @royPredictingBirthSpoken2015 showed that words that are heard in distinct contexts at distinct times are more likely to be learned. @ruffmanExposureBehavioralRegularities2023 used head-mounted video cameras to study how repeated behaviors in everyday life correlate with the acquisition of mental state vocabulary. Taken together, such studies illustrate how important is to study naturalistic everyday experiences to understand children's development. However, studies linking everyday experience and development are still vastly underrepresented [@debarbaroTenLessonsInfants2022; @rogoffImportanceUnderstandingChildrens2018] because they come with a set of unique challenges.

Perhaps the most significant obstacle in this field is the extensive amount of data needed to comprehensively study children’s everyday experiences. Traditional methods, such as manual annotation, are time-consuming and impractical for large-scale datasets. To address this, Computer Vision or Natural Language Processing models offer scalable solutions for analyzing social interactions and behaviors. For instance, OpenPose [@caoOpenPoseRealtimeMultiPerson2018] allows the tracking of human body, face, and hand poses which provides valuable insights into gestures and engagement. YOLOv8 [@redmonYouOnlyLook2015] offers efficient object detection and models like I3D [@carreiraQuoVadisAction2017] provide an automated solution for classifying activities in video data. For audio, Wave2Vec 2.0 [@baevskiWav2vec20Framework2020] provides robust speech-to-text and speech representation capabilities, enabling the study of conversational dynamics. Together, these models facilitate the efficient analysis of multimodal data. However, even the best model architecture needs diverse, high-quality datasets to learn from. A notable example of such a dataset is ImageNet [@russakovskyImageNetLargeScale2014]; in fact, the release of this dataset sparked the development of some of the most prominent Computer Vision models available today. Similarly, expanding publicly available datasets in developmental psychology could accelerate progress in studying children’s everyday experiences. 

Several publicly available datasets have made valuable contributions to our understanding of children’s social and communicative behavior. For example, the SAYCam dataset [@sullivanSAYCamLargeLongitudinal2021] provides audio-video recordings from 3 infants (6–32 months) who wore head-mounted cameras over two years, to capture naturalistic speech and behaviors. Similarly, the DAMI-P2C dataset [@chenDyadicAffectParentChild2023] includes audio and video recordings of parent-child interactions during story reading, with annotations for body movements in a controlled environment. The MMDB dataset [@rehgDecodingChildrensSocial2013] offers multimodal data (audio, video, physiological) of children (15–30 months) engaged in semi-structured play interactions, recorded in a lab. Another example is the UpStory dataset [@fraileUpStoryUppsalaStorytelling2024], which features audio and video of primary school children (8-10 years) in dyadic storytelling interactions, also recorded in a lab setting. Additionally, the BabyView dataset [@longBabyViewDatasetHighresolution2024] provides high-resolution, egocentric video of children aged 6 months to 5 years, recorded at home and in preschool environments, with annotations for speech transcription and pose estimation. Whereas these datasets vary in age, setting, and target behaviors, they collectively highlight the need for more naturalistic, at-home datasets that can capture the full range of children’s daily activities.

To address this gap, we introduce the publicly available ChildLens dataset, which focuses on activity annotations for children aged 3–5 years. The dataset consists of 106 hours of video and audio recordings collected from 61 children in their home environment through a 140° wide-lens camera, recording video and audio, which was embedded in a child-friendly vest. It includes detailed annotations for five location classes and 14 activity classes, which are further categorized based on whether the child is interacting alone or with others. Designed to support research in developmental psychology and computer vision, the ChildLens dataset offers a rich resource for advancing multimodal learning and studying the full spectrum of children’s daily activities. As of now, `r annotation_progress`% of the dataset are annotated. The remaining videos will be annotated when new funding is available.

# Dataset Generation
This section outlines the steps taken to create the ChildLens dataset. We provide detailed information on the video collection process, the labeling strategy employed, and the generation of activity labels.

## Step 1: Collection of Egocentric Videos
The ChildLens dataset consists of egocentric videos recorded by children aged 3 to 5 years over a period of `r time_span_months` months. A total of `r nr_children` children from families living in a mid-sized city in Germany, participated in the study. The videos were captured at home using a camera embedded in a vest worn by the children, as shown in figure \@ref(fig:camera-superannotate-activity-classes). This setup allowed the children to move freely throughout their homes while recording their activities. The camera, a _PatrolEyes WiFi HD Infrared Police Body Camera_, was equipped with a 140° wide-angle lens and captured the space in front of the child with a resolution of 1920x1080p at 30 fps. The camera also recorded high-quality audio, allowing us to capture the child's speech and other sounds in the environment. 

In order to obtain a decent coverage for the different activity classes we planned to annotate, we handed parents a short checklist of activities to record. The focus was on capturing everyday activities that children typically engage in. Parents were therefore asked to include the following elements in the recordings:

- Child spends time in different rooms and performs various activities in each room
- Child is invited to read a book together with an adult
- Child is invited to play with toys alone
- Child is invited to play with toys with someone else (adult or child)
- Child is invited to draw/craft something

## Step 2: Creation of Labeling Strategy
To create a comprehensive labeling strategy for the ChildLens dataset, we first defined a list of activities that children typically engage in. This list was inspired by previous research on activities that children are known to participate in [@hofferthHowAmericanChildren2001; @ginsburgImportancePlayPromoting2007]. From this, we derived a detailed catalog of activities that were likely to be captured in the videos and chose to make the activity classes more granular by distinguishing between activities like "making music" and "singing/humming" or "drawing" and "crafting things". 

After an initial review of the videos, we decided to add another class "overheard speech" to capture situations in which the child is not directly involved in a conversation but can hear it. We also added "pretend play" as a separate class to capture situations in which the child is engaged in imaginative play. This approach allowed us to capture the diversity of activities that children engage in and create a comprehensive dataset for activity analysis.

## Step 3: Manual Labeling Process
Before the actual annotation process, a setup meeting was held to introduce the annotators to the labeling strategy. To familiarize themselves with the task, the annotators were assigned 25 sample videos to practice and gain hands-on experience. These initial annotations were reviewed by the research team, and feedback was provided to refine the approach. A total of three feedback loops were conducted to ensure that the annotators follow the labeling strategy properly.

The videos were manually annotated by native German speakers who watched each video and labeled the activities present in the footage. Annotators marked the start and end points of each activity. For audio annotations, we implemented a 2-second rule for the categories ‘other person talking’ and ‘child talking’: if the break between two utterances was 2 seconds or less, it was considered a single event; breaks longer than 2 seconds split the activity into separate instances.


# Dataset Overview

#### Activity Classes
The ChildLens dataset includes 14 activity classes and 5 location classes. A brief description of each class can be found in \@ref(tab:activity-classes-description). The location classes describe the current location of the child in the video and include _livingroom_, _playroom_, _bathroom_, _hallway_, and _other_.
The activity classes are categorized based on the child’s interactions within the video and can be divided into _person-only_ activities (e.g. "child talking", "other person talking"), and _person-object_ activities (e.g. "drawing", "playing with object"). These activities are further categorized into _audio-based_, _visual-based_, and _multimodal_ activities, as presented in Figure \@ref(fig:camera-superannotate-activity-classes). Below is an overview of the different activity types:

- **Audio-based activities**: _child talking_, _other person talking_, _overheard speech_, _singing / humming_, _listening to music / audiobook_
- **Visual-based activities**: _watching something_, _drawing_, _crafting things_, _dancing_
- **Multimodal activities**: _playing with object_, _playing without object_, _making music_, _pretend play_, _reading book_ 

```{r activity-classes-description, echo=FALSE, dpi=600, fig.align='center'}
# Define the table data as a string with "|" delimiters
activity_data <- "Activity Class                | Description: The child is ...
                  Child talking                 | ... talking to themselves or to someone else.
                  Singing/Humming               | ... singing or humming a song or a melody.
                  Listening to music/audiobook  | ... listening to music or an audiobook.
                  Watching something            | ... watching a movie or video on either a screen or a device.
                  Drawing                       | ... drawing or coloring a picture.
                  Crafting things               | ... engaged in a craft activity, such as making a bracelet.
                  Dancing                       | ... dancing to music or moving to a rhythm.
                  Playing with object           | ... playing or interacting with an object, such as a toy or a ball.
                  Playing without object        | ... playing without an object, such as playing hide and seek
                  Pretend play                  | ... engaged in imaginative play, such as pretending to be a doctor.
                  Reading a book                | ... reading a book or looking at pictures in a book.
                  Making music                  | ... playing a musical instrument or making music in another way.
                                                |     
                  Other person talking          | Another person is talking to the child.
                  Overheard Speech              | Conversations the child can hear but is not directly involved in."

# Read the table into a data frame
activity_classes_table <- read.delim(
  textConnection(activity_data), 
  header = FALSE, 
  sep = "|", 
  strip.white = TRUE, 
  stringsAsFactors = FALSE
)

# Assign column names from the first row
names(activity_classes_table) <- unname(as.list(activity_classes_table[1, ]))

# Remove the first row (header row) from the data frame
activity_classes_table <- activity_classes_table[-1, ]

# Reset row names to NULL
row.names(activity_classes_table) <- NULL

# Use apa_table to display the table with a caption
apa_table(
  activity_classes_table,
  caption = "Activity classes in the ChildLens dataset.",
  escape = TRUE
)

```


```{r camera-superannotate-activity-classes, echo=FALSE, dpi=600, fig.align='center', fig.cap="\\textbf{A} – Vest with the embedded camera worn by the children, \\textbf{B} – Platform utilized for video annotation, \\textbf{C} – Activity classes in the ChildLens dataset."}
img1 <- ggdraw() + draw_image("images/camera_worn_close.png", scale = 0.8)
img2 <- ggdraw() + draw_image("images/SuperAnnotate.png", scale = 0.8)
img3 <- ggdraw() + draw_image("images/ChildLens_activity_classes.png", scale = 1)

# Combine img1 and img2 into the first row with equal heights
top_row <- (img1 + img2) + 
  plot_layout(widths = c(0.4, 1))

final_layout <- (top_row / img3) +
  plot_layout(heights = c(1, 1))  # Adjust row heights if needed

# Add labels (A), (B), (C) to the images
final_labeled_layout <- final_layout +
  plot_annotation(tag_levels = 'A')  # Adds "A", "B", "C" automatically

# Display the final layout
final_labeled_layout
```

#### Statistics
The ChildLens dataset comprises of `r sum_videos` video files with a total of `r sum_hours` hours recorded by `r nr_children` children aged 3 to 5 years (M=`r mean_age`, SD=`r sd_age`). This includes `r count_3_plus` videos from children aged 3, `r count_4_plus` videos from children aged 4, and `r count_5_plus` videos from children aged 5. The video duration per child varies between `r min_minutes` and `r max_minutes` minutes (M=`r mean_minutes`, SD=`r sd_minutes`). A detailed distribution of the video duration per child is shown in figure \@ref(fig:minutes-per-child). 

This diverse dataset includes a varying number of instances across the 14 activity classes. While annotations are still ongoing, the current annotations (`r annotation_progress`% annotated) include between 2 and 319 instances per class. The duration of each instance varies by activity. For instance, audio-based activities like "child talking" may last only a few seconds, while activities like "reading a book" can span several minutes. The table with the total number of instances and summed duration for all activity classes is available in the appendix.

```{r minutes-per-child-density, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
unique_data$ID <- as.factor(unique_data$ID)

ggplot(unique_data, aes(x = Minutes_per_ID)) +
  geom_histogram(binwidth = 10) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10), labels = scales::label_number(accuracy = 1)) +
  labs(x = "Minutes per ID", y = "Count") +
  theme_minimal()
```

```{r age-distribution, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE, fig.cap="Number of videos per age group."}
data_age_count_plot <- data_age_count
data_age_count_plot$ID <- as.factor(data_age_count_plot$ID)

ggplot(data_age_count_plot, aes(x = Age_group, fill = Age_group)) +
  geom_bar(position = "dodge", alpha = 0.7, width = 0.5) +
  scale_fill_manual(values = c("3+" = "#A6ADAF", "4+" = "#A18D92", "5+" = "#BBBB9B")) + 
  theme_minimal() +
  labs(
    x = "Age Group",
    y = "Count",
    fill = "Age Group"
  ) +
  theme(legend.position = "none")

plot_age <- ggplot(data_age_count_plot, aes(x = Age_group, fill = Age_group)) +
  geom_bar(position = "dodge", alpha = 0.7, width = 0.5) +
  scale_fill_manual(values = c("3+" = "#B7B794", "4+" = "#8A8A8A", "5+" = "#B7AD94")) + 
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +  labs(
    x = "Age Group",
    y = "Count",
    fill = "Age Group"
  ) +
  theme(legend.position = "none")

# Specify the file path and name
output_folder <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
file_name <- "video_count_per_age_group.png"
file_path <- file.path(output_folder, file_name)

# Save the plot
ggsave(filename = file_path, plot = plot_age, width = 8, height = 6, dpi = 300)
```

```{r minutes-per-child, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Video recording duration (in minutes) per child ID.", fig.height=2.5, fig.width=3.5}
unique_data$ID <- as.factor(unique_data$ID)

ggplot(unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 4) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )

#unique_data$ID <- as.factor(unique_data$ID)

#plot <- ggplot(unique_data, aes(x = Minutes_per_ID)) +
#  geom_point(aes(y = 0), shape = "I", size = 5) + 
#  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
 # theme_minimal() +
 # theme(
  #  legend.position = "none",
   # axis.title = element_text(size = 25),      # Increase label size
    #axis.text = element_text(size = 25),       # Increase tick label size
    #axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  #) +
  #labs(
  #  x = "Minutes per ID",
  #  y = "Density"
  #)

# Specify the file path and name
#output_folder <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
#file_name <- "minutes_per_id_plot.png"
#file_path <- file.path(output_folder, file_name)

# Save the plot
#ggsave(filename = file_path, plot = plot, width = 3, height = 2, dpi = 100)
```

#### Data Access
The ChildLens dataset will be made available to scientists for research purposes. It includes video and audio recordings, along with activity labels. Due to the sensitive nature of the data—recordings of children in their homes—access will be restricted.

Researchers can submit requests for access through the , which will be carefully reviewed to ensure proper handling and compliance with privacy standards. Please contact [ra@eva.mpg.de](mailto:ra@eva.mpg.de) to request access to the dataset. As noted above, the annotation process is still ongoing and the dataset will be updated regularly. A brief project overview, information about how to access the dataset along with the latest dataset version can be found [here](https://www.eva.mpg.de/comparative-cultural-psychology/technical-development/childlens/) 

#### Exhaustive multi-label annotations
The dataset provides detailed annotations for each video file. These annotations specify the child’s current location within the video, the start and end times of each activity, the activity class, and whether the child is engaged alone or with somebody else. For every person involved in the activity, we capture age class and gender. If multiple activities occur simultaneously in a video, each activity is individually labeled. For example, if a segment shows a child “reading a book” while also “talking,” two separate annotations are created: one for “reading a book” and another for “child talking.” This exhaustive labeling strategy ensures that each activity is accurately represented in the dataset. 

# Benchmark Performance
In this chapter, we present the results of applying two model architectures to the ChildLens dataset for two specific tasks: temporal activity localization using video data and voice type classification using audio data. For temporal activity localization, we used the Boundary-Matching Network (BMN) model [@linBMNBoundaryMatchingNetwork2019], a state-of-the-art approach in this domain, and trained it from scratch on the unique activity classes in the ChildLens video data. For voice type classification, we applied the Voice Type Classifier (VTC) [@lavechinOpensourceVoiceType2020], also state-of-the-art, which was trained on similar data. Both models provide initial results and establish a benchmark for future research. 

```{r bmn-architecture, dpi=600, fig.align='center', eval=FALSE, fig.cap="Boundary Matching Network architecture applied to ChildLens video data"}
knitr::include_graphics("images/BMN_architecture.png")
#A short architecture overview of the models is presented in figure \@ref(fig:bmn-architecture) and figure \@ref(fig:vtc-architecture).
```

```{r vtc-architecture, dpi=600, fig.align='center', eval=FALSE, fig.cap="Voice Type Classifier architecture applied to ChildLens audio data"}
knitr::include_graphics("images/VTC_architecture.png")
```

## Temporal Activity Localization
The BMN generates action proposals by predicting activity start and end boundaries and classifying these proposals into activity classes. The architecture consists of two main components: (1) a proposal generation network, which identifies candidate proposals, and (2) a proposal classification network, which classifies these proposals. The model prioritizes proposals with high recall and high temporal overlap with ground truth.
BMN performance is evaluated using Average Recall (AR) and Area Under the Curve (AUC) metrics. AR is computed at various Intersection over Union (IoU) thresholds and for different Average Numbers of Proposals (AN) as AR@AN, where AN ranges from 0 to 100. AR@100 reflects recall performance with 100 proposals per video, while AUC quantifies the trade-off between recall and number of generated proposal. On the ActivityNet-1.3 test set [@heilbronActivityNetLargescaleVideo2015], BMN demonstrates effective activity localization with an AR@100 of 72.46 and an AUC of 64.47.

### Data Preparation
The BMN implementation, including video preprocessing and model training, was conducted using the MMAction2 toolbox [@mmaction2contributorsOpenMMLabsNextGeneration2020]. Data preparation involved several key steps, such as raw frame extraction and the generation of both RGB and optical flow features for each video. Before training the model, we analyzed the distribution of activity instances across the classes for the annotated videos to assess the data’s sufficiency for both training and testing. A detailed summary of the activity instances and their total durations can be found in the appendix.

Our analysis highlighted a significant class imbalance in the dataset, both in terms of instance count and the total duration of recordings. Given the primary goal of establishing initial benchmark results, no data augmentation methods were employed to mitigate this imbalance. Instead, we focused on the more frequent activity classes, which also had the longest durations: “Playing with Object” (22.85 hours of recording), “Drawing” (6.24 hours of recording), and “Reading a Book” (5.48 hours of recording).

For feature extraction and model training optimization, the videos were divided into clips of 4000 frames each (correspond to approx. 2 minutes and 13 seconds). This resulted in a total of 1130 clips. However, only 995 clips had annotations, so we split these annotated clips into training, validation, and test subsets, using an 80-10-10 split. The training set was used for model optimization, the validation set guided hyperparameter tuning and overfitting prevention, and the test set was reserved for evaluating the model’s generalization ability on unseen data.

### Implementation Details
The BMN model was trained from scratch on the ChildLens dataset to predict the start and end boundaries of activity classes in the videos. The model was implemented using MMAction2, an open-source toolbox for video understanding based on PyTorch [@mmaction2contributorsOpenMMLabsNextGeneration2020]. Training took place on a Linux server with 48 cores and 187 GB of RAM. The Adam optimizer was used with a learning rate of 0.001 and a batch size of 4. To avoid overfitting, early stopping based on validation loss was applied during training.

### Evaluation
The performance of the BMN model on the ChildLens dataset, compared to its evaluation on ActivityNet-1.3, is summarized in Table \@ref(tab:bmn-results), with AR@100 and AUC reported for both datasets. The results indicate that the BMN model generalizes well to the new domains included in the ChildLens dataset. These benchmark results highlight the potential for integrating the ChildLens dataset with existing models like BMN. Automating the analysis of this dataset can streamline the study of children’s activities and interactions, facilitating more efficient research in developmental psychology and related fields.

```{r bmn-results, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Dataset           | AR@100  | AUC
            ActivityNet-1.3   | 72.46   | 64.47
            ChildLens         | 77.43   | 69.21"


df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)

# Set the column names
names(df) <- unname(as.list(df[1,]))
df <- df[-1,]  # Remove the first row with column names
row.names(df) <- NULL


# Print the updated table
apa_table(
  df,
  caption = "Comparison of BMN performance on the ActivityNet-1.3 dataset (used for model evaluation) and the ChildLens dataset, highlighting the Average Recall for 100 proposals (AR@100) and the Area Under the Curve (AUC).",
  escape = TRUE
)
```

## Voice Type Classification
Voice Type Classification is the task of identifying audio utterances and assigning them to predefined classes. In line with previous work, we focus on the following classes: `Key Child (KCHI)`, `Other Child (CHI)`, `Male Speech (MAL)`, `Female Speech (FEM)`, and `Speech (SPEECH)` [@lavechinOpensourceVoiceType2020]. The Voice Type Classifier model (VTC) is designed to perform this task efficiently. Its architecture is composed of a SincNet layer, two bi-directional LSTMs, and three feed-forward layers. The model takes a 2-second audio chunk as input and outputs a score between 0 and 1 for each class. The VTC was originally trained on the BabyTrain dataset which includes 260 hours of child-centered audio in multiple languages from children mostly aged 0-3 years.

The model architecture utilizes the open-source pyannote library for speaker diarization [@plaquetPowersetMulticlassCross2023;@bredinPyannoteaudio21Speaker2023] which provides pre-trained models and pipelines for various audio processing tasks. By adjusting the model architecture and training process using pyannote, we evaluated the ChildLens dataset's quality through three distinct VTC training setups: We first applied the pretrained VTC model directly to the ChildLens dataset. In the second setup we fine-tuned the VTC model on the ChildLens data. Finally, we trained the VTC model from scratch using only the ChildLens dataset. These setups test the dataset’s annotation consistency and standalone value, with performance measured by the \( F_1 \)-measure, which combines precision and recall.

```{r vtc-implementation}
#Test set: 343it [36:01,  6.30s/it]
#Test set: 343it [11:00,  1.93s/it]
#Test set: 343it [10:54,  1.91s/it]
#Test set: 343it [10:54,  1.91s/it]
#Test set: 343it [10:59,  1.92s/it]
#Test set: 343it [11:00,  1.92s/it]
#- describe recent advancements in model architecture adjustments (following pyannote library used for vtc implementation): [@baroudiSpecializingSelfSupervisedSpeech2024]
#- describe SSeRiouSS model architecture
vtc_minutes_run <- 5461/60
num_audio_files <- 343
num_epochs_finetune <- 200
train_time_finetune <- 12.86
num_epochs_scratch <- 200
train_time_scratch <- 12.86
```
### Data Preparation
We used the following mapping strategy to align our audio-based activity classes with the VTC's output classes. Minutes in parentheses indicates the total duration of annotated audio for each class.

- Child talking & Singing/Humming → **`Key Child`** (859.27 min)
- Other person talking:
  - If age = `"Child"` → **`Other Child`** (43.98 min)
  - If age = `"Adult"` & gender = `"Female"` → **`Female Speech`** (455.90 min)
  - If age = `"Adult"` & gender = `"Male"` → **`Male Speech`** (200.41 min)
- Overheard Speech, Child talking, Singing/Humming, Other person talking  → **`Speech`** (2361.87 min)

"Listening to music/audiobook" was excluded, as it’s often background audio, lacks speaker age/gender details, and includes music irrelevant to VTC. "Overheard Speech" refers to speech not directed at the key child and was mapped to SPEECH due to missing age/gender annotations. This may underestimate VTC performance, for example, correct FEM predictions have SPEECH as ground truth. We retain this mapping, as re-annotation would be time-consuming. Moreover, the class is conceptually incompatible with the VTC output: it captures the recipient of speech rather than the speaker type. It is intended for more advanced models (for instance NLP architectures) that can infer not just who is talking, but also to whom, based on semantic content. 

```{r vtc-classes-statistics, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
vtc_og_f1 <- 44.6
vtc_og_kchi <- 68.5
vtc_og_chi <- 4.5
vtc_og_mal <- 19.6
vtc_og_fem <- 46.7
vtc_og_speech <- 82.2

vtc_ft_f1 <- 53.2
vtc_ft_kchi <- 75.7
vtc_ft_chi <- 12.0
vtc_ft_mal <- 39.2
vtc_ft_fem <- 54.2
vtc_ft_speech <- 85.2

vtc_cl_f1 <- 55.0
vtc_cl_kchi <- 74.7
vtc_cl_chi <- 9.6
vtc_cl_mal <- 53.8
vtc_cl_fem <- 52.5
vtc_cl_speech <- 84.7

```
### Implementation Details
We first applied the pretrained VTC model directly, using the available VTC implementation. We then fine-tuned the model on our ChildLens dataset for `r num_epochs_finetune` epochs (`r train_time_finetune` hours) on the same Linux server as the BMN model. Finally, we trained the VTC model from scratch for `r num_epochs_scratch` epochs (`r train_time_scratch` hours) using the ChildLens dataset to assess its standalone value

```{r vtc-results, echo=FALSE, message=FALSE, warnings=FALSE, results='asis', eval=TRUE}
my.data <- "Dataset       | Model         | KCHI | CHI  | MAL   | FEM  | SPEECH | AVG
            ACLEW-Random  | VTC-OG        | 68.7 | 33.2 | 42.9  | 63.4 | 78.4   | 57.3
            ChildLens     | VTC-OG        | 68.5 | 4.5  | 19.6  | 46.7 | 82.2   | 44.6
            ChildLens     | VTC-FT        | 75.7 | 12.0 | 39.2  | 54.2 | 85.2   | 53.2
            ChildLens     | VTC-CL        | 74.7 | 9.6  | 53.8  | 52.5 | 84.7   | 55.0"

df <- read.delim(textConnection(my.data), header=FALSE, sep="|", strip.white=TRUE, stringsAsFactors=FALSE)
names(df) <- unname(as.list(df[1,]))
df <- df[-1,] 
row.names(df) <- NULL
apa_table(
  df,
  caption = "Comparison of Voice Type Classifier (VTC) performance on the ACLEW-Random dataset and the three setups utilizing the ChildLens dataset. The evaluation metrics are reported for the original VTC model (VTC-OG), the VTC fine-tuned on ChildLens (VTC-FT) and the VTC trained from scratch on ChildLens (VTC-CL). The table reports the F1 score per class and the average F1 score (AVG).
",
  escape = TRUE,
)
```

### Evaluation
Table \@ref(tab:vtc-results) shows \( F_1 \)-scores for three VTC setups on the ChildLens dataset: the original model \( VTC_{og} \), fine-tuned model \( VTC_{ft} \), and model trained from scratch \( VTC_{cl} \), compared to the benchmark dataset. \( VTC_{og} \) scores `r vtc_og_f1`, slightly below the benchmark, with best performance on `KCHI` (`r vtc_og_kchi`) and worst on `CHI` (`r vtc_og_chi`). Fine-tuning \( VTC_{ft} \) training from scratch \( VTC_{cl} \) achieve `r vtc_ft_f1` and `r vtc_cl_f1`, respectively, showing significant improvement. The low `CHI` scores are primarily due to frequent misclassifications as `KCHI`. This likely results from the vocal similarity between the key child and other children in the ChildLens dataset. In many cases, distinguishing between them may rely more on acoustic cues like proximity to the microphone than on vocal characteristics. In contrast, the original VTC dataset includes audio from infants and toddlers, where the distinction between the key child (often babbling or younger) and other speakers is more pronounced, making classification tasks easier. Importantly, these scores are expected to improve as more annotated data becomes available, since annotation is still ongoing. Figure \@ref(fig:vtc-evaluations) visualizes how model predictions compare to the ground truth annotations for the \( VTC_{ft} \) model.

```{r vtc-evaluations, dpi=180, fig.align='center', fig.cap="VTC Predictions compared to Ground Truth Annotations"}
knitr::include_graphics("images/vtc_performance_evaluation.png")
```

# General Discussion
We present the ChildLens dataset, a unique egocentric video-audio dataset that documents naturalistic everyday experiences,in preschool children. This dataset is particularly distinctive due to its diversity in terms of the number of children it includes and the variety of activity labels it covers. By encorporating both visual and auditory data, the ChildLens dataset provides comprehensive annotations for a broad spectrum of key activities, including multi-modal social interactions. These annotations support the training and evaluation of models for the automatic analysis of children’s activities and therefore allow for scaling up data collection. A rich and representative dataset like ChildLens is essential for understanding individual differences in children's daily experiences and how they relate to different cognitive and social outcomes.

In comparison to other freely available datasets, the ChildLens dataset stands out due to its broad age span and diverse set of activity labels. Most existing datasets either focus on toddlers, are limited to dyadic interactions or were recorded in lab settings, with all of them lacking a comprehensive range of activity labels. Furthermore, most of these datasets either capture only audio or video. In contrast, ChildLens includes naturalistic recordings from children’s home environments, collected over an extended period, and features a wide variety of activity types. Particularly noteworthy is the activity class `Overheard Speech` which captures speech that children can hear but are not directly involved in. This class is important for studying the impact of overheard speech on children's cognitive development - an area that has largely relied on labor-intensive manual annotations due to the difficulty of automatically distinguishing between child-available and child-directed speech [@bergelsonEverydayLanguageInput2023a]. Existing models, like the classifier developed by @bangAutomatedClassifierPeriods2023 could be enhanced by incorporating visual features - such as eye contact or the use of gestures - in addition to audio inputs. The ChildLens dataset also captures whether children are engaged in activities alone or with others and provides basic demographic information - age, sex -about all individuals involved.

The usefulness of the ChildLens dataset is demonstrated by its successful application to well-established models. For example, the pretrained Voice-Type Classifier for audio transcription achieves performance comparable to previous datasets, while the Boundary-Matching Network (BMN) produces robust results for activity localization, consistent with its performance on widely used datasets such as ActiviyNet. One way of using the ChildLens dataset to advance methodological development are multi-method approaches. For example, activity localization could be further enhanced by incorporating object identification, allowing for better tracking of the objects children interact with during daily routines. Such an approach has beed used in adult-focused studies [@kazakosLittleHelpMy2021]. Research by @bambachLendingHandDetecting2015 also emphasizes the importance of hand detection in egocentric video for activity recognition. Their use of Convolutional Neural Networks (CNNs) for hand segmentation demonstrates how such techniques can help differentiate between activities. To apply a similar approach to the ChildLens dataset, we would first need to run a pretrained hand detection model on the dataset. In a second step, we would train a CNN to classify frames containing hands into activity classes, following the method described by @bambachLendingHandDetecting2015. If the performance of the hand detection performance is insufficient, additional hand annotations would be required to improve the model's accuracy.

The integration of visual and auditory data in the ChildLens dataset enables a more detailed and comprehensive understanding of children’s daily experiences. Complex activities such as pretend play and reading a book, which require both audio and video for accurate detection, exemplify the strength of this multimodal approach. While previous studies, such as those analyzing disfluencies in children’s speech during computer game play [@yildirimAutomaticDetectionDisfluency2009], have demonstrated that combining visual and auditory information can improve performance, few studies have explored this in the context of children’s naturalistic activities. With ChildLens, the combination of naturalistic data and multimodal analysis creates new opportunities for in-depth insights into children’s cognitive, emotional, and social development, particularly for activities best captured through both modalities.

Despite its strengths, the ChildLens dataset also has its limitations. First, there is class imbalance, especially in underrepresented activity classes, which could affect model training and evaluation. More frequent activities, such as “child talking” (7447 instances, 649 minutes) and “playing with object” (317 instances, 1371 minutes), dominate the dataset, whereas less common activities like “dancing” (2 instances, 0.57 minutes) and “making music” (2 instances, 2.13 minutes) are scarcely represented. Similarly, activities like “pretend play” (59 instances, 158.84 minutes) and “reading a book” (81 instances, 328.70 minutes) appear less frequently. This imbalance may lead to skewed model performance, making it harder to accurately classify rare activities. Possible solutions to this challenge could involve merging rare activity classes into broader categories or excluding them from model training, though these approaches may reduce the dataset’s diversity. Other methods, such as resampling or augmentation, could help balance the dataset and improve model performance [@spelmenReviewHandlingImbalanced2018; @alaniClassifyingImbalancedMultimodal2020]. Second, there is sampling bias. Since the recordings are largely influenced by parental decisions about when and how often activities are captured, certain activities or settings may be overrepresented or underrepresented based on these preferences. Furthermore, the dataset primarily focuses on families from a mid-sized German city, limiting its geographic and cultural diversity. Expanding the dataset to include a broader range of families from different regions and cultures would enhance its generalizability and applicability to various research contexts.

The study of children’s everyday experiences is crucial for understanding their cognitive, emotional, and social development. These daily interactions provide important insights into how children learn, grow, and engage with their environment. The ChildLens dataset makes a valuable contribution to this field by offering a rich multi-modal resource that captures children’s experiences in naturalistic settings. With its comprehensive annotations and potential to automate the analysis of children’s activities, the dataset enables researchers to develop, fine-tune and apply automized processing algorithms that help to scale-up the study of situated development. By virtue of being an openly accessible resource, the ChildLens dataset creates new opportunities for understanding the complexities of early childhood development and provides a foundation for future research in this area.

\newpage
# References
<!--e used `r cite_r("bibliography.bib")` for all our analyses.-->    
```{r create_r-references}
r_refs(file = "bibliography.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage
# Appendix

## Activity Class Statistics
```{r activity-classes-statistics, echo=FALSE, dpi=600, fig.align='center'}
# Define the table data as a string with "|" delimiters
activity_data <- "Category | Activity Class | Instance Count | Total Duration (min)
       Audio   | Child talking        | 7447             | 649.10
               | Other person talking | 6113             | 455.29
               | Overheard Speech     | 1898             | 299.44
               | Singing/Humming      | 277             | 82.00
               | Listening to music/audiobook | 68     | 222.14
       Video   | Watching something   | 2             | 5.09
               | Drawing              | 62             | 374.91
               | Crafting things      | 26             | 109.14
               | Dancing              | 2             | 0.57
      Multimodal | Playing with object  | 317             | 1371.06
                 | Playing without object | 25           | 28.87
                 | Pretend play         | 59             | 158.84
                 | Reading a book         | 81             | 328.70
                 | Making music         | 3             | 2.13"
# Read the table into a data frame
activity_classes_table <- read.delim(
  textConnection(activity_data), 
  header = FALSE, 
  sep = "|", 
  strip.white = TRUE, 
  stringsAsFactors = FALSE
)

# Assign column names from the first row
names(activity_classes_table) <- unname(as.list(activity_classes_table[1, ]))

# Remove the first row (header row) from the data frame
activity_classes_table <- activity_classes_table[-1, ]

# Reset row names to NULL
row.names(activity_classes_table) <- NULL

# Use apa_table to display the table with a caption
apa_table(
  activity_classes_table,
  caption = "Number of video instances and the total duration (in minutes).",
  escape = TRUE
)
```

```{r quantex-statistics, echo=FALSE, message=FALSE, warning=FALSE}
quantex_data <- read_csv2("data/Quantex_data_sheet.csv")
quantex_subject_infos <- read_csv2("data/Quantex_subjects.csv")
quantex_subject_infos <- quantex_subject_infos %>%
  distinct(ID, .keep_all = TRUE)

quantex_unique_data <- quantex_data %>%
  distinct() %>%  # Removes duplicate rows
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes") %>%
  distinct(ID, Minutes_per_ID, .keep_all = TRUE)
quantex_unique_data <- quantex_unique_data %>%
  mutate(ID = as.double(ID))

quantex_cleaned_data <- quantex_data %>%
  filter(!is.na(Minutes_per_ID), Minutes_per_ID != 0, 
         !is.na(ID), Include == "yes")
quantex_sum_videos <- nrow(quantex_cleaned_data)

quantex_data_gender_count <- quantex_unique_data %>%
  left_join(quantex_subject_infos, by = "ID") %>%
  distinct(ID, .keep_all = TRUE)
quantex_male_count <- quantex_data_gender_count %>%
  filter(gender == "Male") %>%
  nrow()
quantex_female_count <- quantex_data_gender_count %>%
  filter(gender == "Female") %>%
  nrow()

quantex_data_age_count <- quantex_cleaned_data %>%
  left_join(quantex_subject_infos, by = "ID")%>%
  select(ID, birthday, DATE)

quantex_data_age_count <- quantex_data_age_count %>%
  mutate(
    birthday = dmy(birthday),  # Convert birthday to Date type
    Date = dmy(DATE),          # Convert Date to Date type
    Age = as.numeric(difftime(Date, birthday, units = "weeks")) / 52.25,  # Calculate age in years
    Age_group = case_when(
      Age >= 2 & Age < 4 ~ "3+",
      Age >= 4 & Age < 5 ~ "4+",
      Age >= 5 ~ "5+",
      TRUE ~ NA_character_  # For cases where age is missing or not within the desired ranges
    )
  )

quantex_mean_age <- mean(data_age_count$Age, na.rm = TRUE)
quantex_sd_age <- sd(data_age_count$Age, na.rm = TRUE)

quantex_age_group_counts <- quantex_data_age_count %>%
  count(Age_group, name = "count")
count_3_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "3+"]
count_4_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "4+"]
count_5_plus <- quantex_age_group_counts$count[quantex_age_group_counts$Age_group == "5+"]

quantex_filtered_data <- quantex_data %>%
  filter(!is.na(Date), Include == "yes") %>%
  mutate(Date = as.Date(DATE, format = "%d.%m.%Y")) 

quantex_min_minutes <- min(quantex_unique_data$Minutes_per_ID[quantex_unique_data$Minutes_per_ID > 0], na.rm = TRUE)
quantex_max_minutes <- max(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sum_minutes <- sum(as.numeric(quantex_unique_data$Minutes_per_ID), na.rm = TRUE)
quantex_mean_minutes <- mean(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
quantex_sd_minutes <- sd(quantex_unique_data$Minutes_per_ID, na.rm = TRUE)
# Convert the time strings into seconds
quantex_unique_data$Seconds_per_ID <- as.numeric(hms::as_hms(quantex_unique_data$Minutes_per_ID))
total_seconds <- sum(quantex_unique_data$Seconds_per_ID, na.rm = TRUE)
total_hours <- total_seconds / 3600

quantex_nr_children <- nrow(quantex_unique_data)

quantex_oldest_date <- min(quantex_filtered_data$Date)
quantex_youngest_date <- max(quantex_filtered_data$Date)

quantex_oldest_year <- as.numeric(format(quantex_oldest_date, "%Y"))
quantex_oldest_month <- as.numeric(format(quantex_oldest_date, "%m"))
quantex_youngest_year <- as.numeric(format(quantex_youngest_date, "%Y"))
quantex_youngest_month <- as.numeric(format(quantex_youngest_date, "%m"))

# Calculate the interval between the two dates
quantex_time_span_months <- (quantex_youngest_year - quantex_oldest_year) * 12 + (quantex_youngest_month - quantex_oldest_month)+1
```

```{r quantex-minutes-per-child, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
quantex_unique_data$ID <- as.factor(quantex_unique_data$ID)

ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )
quantex_1_plot <- ggplot(quantex_unique_data, aes(x = Minutes_per_ID)) +
  geom_point(aes(y = 0), shape = "I", size = 5) + 
  geom_density(fill = "gray", alpha = 0.5, color = "black") + 
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +
  labs(
    x = "Minutes per ID",
    y = "Density"
  )

# Specify the file path and name
quantex_1_output_folder <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
file_name_1 <- "quantex_minutes_per_id_plot.png"
quantex_file_path_1 <- file.path(quantex_1_output_folder, file_name_1)

# Save the plot
ggsave(filename = quantex_file_path_1, plot = quantex_1_plot, width = 8, height = 6, dpi = 300)
```

```{r quantex-age-distribution, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE, fig.cap="Number of videos per age group."}
quantex_data_age_count_plot <- quantex_data_age_count
quantex_data_age_count_plot$ID <- as.factor(quantex_data_age_count_plot$ID)

ggplot(quantex_data_age_count_plot, aes(x = Age_group, fill = Age_group)) +
  geom_bar(position = "dodge", alpha = 0.7, width = 0.5) +
  scale_fill_manual(values = c("3+" = "#A6ADAF", "4+" = "#A18D92", "5+" = "#BBBB9B")) + 
  theme_minimal() +
  labs(
    x = "Age Group",
    y = "Count",
    fill = "Age Group"
  ) +
  theme(legend.position = "none")

quantex_2_plot_age <- ggplot(quantex_data_age_count_plot, aes(x = Age_group, fill = Age_group)) +
  geom_bar(position = "dodge", alpha = 0.7, width = 0.5) +
  scale_fill_manual(values = c("3+" = "#66787C", "4+" = "#A18D92", "5+" = "#B2BFC3")) + 
  theme(
    legend.position = "none",
    axis.title = element_text(size = 25),      # Increase label size
    axis.text = element_text(size = 25),       # Increase tick label size
    axis.ticks.length = unit(0.5, "cm")        # Increase tick size
  ) +  labs(
    x = "Age Group",
    y = "Count",
    fill = "Age Group"
  ) +
  theme(legend.position = "none")

# Specify the file path and name
quantex_2_output_folder <- "/Users/nelesuffo/Promotion/projects/leuphana-IPE/paper/images" 
quantex_2_file_name <- "quantex_video_count_per_age_group.png"
quantex_2_file_path <- file.path(quantex_2_output_folder, quantex_2_file_name)

# Save the plot
ggsave(filename = quantex_2_file_path, plot = quantex_2_plot_age, width = 8, height = 6, dpi = 300)
```
