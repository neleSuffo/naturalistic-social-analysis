@article{arabaciMultimodalEgocentricActivity2021,
  title = {Multi-Modal Egocentric Activity Recognition Using Multi-Kernel Learning},
  author = {Arabac{\i}, Mehmet Ali and {\"O}zkan, Fatih and Surer, Elif and Jan{\v c}ovi{\v c}, Peter and Temizel, Alptekin},
  year = {2021},
  month = may,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {11},
  pages = {16299--16328},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-08789-7},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/P2YJ73YD/Arabacı et al. - 2021 - Multi-modal egocentric activity recognition using .pdf}
}

@incollection{kapidisObjectDetectionBasedLocation2020,
  title = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}: {{A Systematic Analysis}}},
  shorttitle = {Object {{Detection-Based Location}} and {{Activity Classification}} from {{Egocentric Videos}}},
  booktitle = {Smart {{Assisted Living}}},
  author = {Kapidis, Georgios and Poppe, Ronald and Van Dam, Elsbeth and Noldus, Lucas P. J. J. and Veltkamp, Remco C.},
  editor = {Chen, Feng and {Garc{\'i}a-Betances}, Rebeca I. and Chen, Liming and {Cabrera-Umpi{\'e}rrez}, Mar{\'i}a Fernanda and Nugent, Chris},
  year = {2020},
  pages = {119--145},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-25590-9_6},
  urldate = {2024-10-23},
  isbn = {978-3-030-25589-3 978-3-030-25590-9},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/XTFMTBU9/Kapidis et al. - 2020 - Object Detection-Based Location and Activity Class.pdf}
}

@misc{linBMNBoundaryMatchingNetwork2019,
  title = {{{BMN}}: {{Boundary-Matching Network}} for {{Temporal Action Proposal Generation}}},
  shorttitle = {{{BMN}}},
  author = {Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1907.09702},
  urldate = {2024-11-13},
  abstract = {Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{nunez-marcosEgocentricVisionbasedAction2022,
  title = {Egocentric {{Vision-based Action Recognition}}: {{A}} Survey},
  shorttitle = {Egocentric {{Vision-based Action Recognition}}},
  author = {{N{\'u}{\~n}ez-Marcos}, Adri{\'a}n and Azkune, Gorka and {Arganda-Carreras}, Ignacio},
  year = {2022},
  month = feb,
  journal = {Neurocomputing},
  volume = {472},
  pages = {175--197},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.11.081},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/I5LRFE37/Núñez-Marcos et al. - 2022 - Egocentric Vision-based Action Recognition A surv.pdf}
}

@article{truongCrossviewActionRecognition2024,
  title = {Cross-View Action Recognition Understanding from Exocentric to Egocentric Perspective},
  author = {Truong, Thanh-Dat and Luu, Khoa},
  year = {2024},
  month = oct,
  journal = {Neurocomputing},
  pages = {128731},
  issn = {09252312},
  doi = {10.1016/j.neucom.2024.128731},
  urldate = {2024-10-23},
  langid = {english},
  file = {/Users/nelesuffo/Zotero/storage/95IM2Q74/Truong and Luu - 2024 - Cross-view action recognition understanding from e.pdf}
}
