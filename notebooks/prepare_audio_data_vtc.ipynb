{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e18f98",
   "metadata": {},
   "source": [
    "## Generate GT RTTM File for ChildLens_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Non-positive duration -375.877s for segment in /home/nele_pauline_suffo/ProcessedData/childlens_annotations/keeper/v1/132027.json. Skipping.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Folder and output paths\n",
    "input_folder = \"/home/nele_pauline_suffo/ProcessedData/childlens_annotations/keeper/v1\"\n",
    "output_dir = Path(\"/home/nele_pauline_suffo/ProcessedData/audio_cls_input\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Updated valid event IDs based on new structure\n",
    "valid_event_ids = {\"child_talking\", \"other_person_talking\", \"overheard_speech\", \"singing/humming\"}\n",
    "\n",
    "all_files = []\n",
    "speaker_counts = Counter()\n",
    "files_processed = 0\n",
    "\n",
    "# Step 1: Load all JSON files and collect metadata\n",
    "json_files = glob(f\"{input_folder}/*.json\")\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract video name and calculate duration from annotations\n",
    "        video_name = data.get('video_name', '')\n",
    "        if not video_name:\n",
    "            print(f\"Warning: No video_name found in {json_file}\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate duration from annotations (find max endTime)\n",
    "        annotations = data.get('annotations', [])\n",
    "        if not annotations:\n",
    "            print(f\"Warning: No annotations found in {json_file}\")\n",
    "            continue\n",
    "            \n",
    "        duration = max(ann.get('endTime', 0) for ann in annotations)\n",
    "        \n",
    "        all_files.append({\n",
    "            \"path\": json_file,\n",
    "            \"uri\": video_name,\n",
    "            \"duration\": duration\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {json_file} due to error: {e}\")\n",
    "\n",
    "# Step 2: Sort and split files by total duration\n",
    "all_files.sort(key=lambda x: x[\"duration\"], reverse=True)\n",
    "total_duration = sum(f[\"duration\"] for f in all_files)\n",
    "\n",
    "train_duration, dev_duration, test_duration = 0, 0, 0\n",
    "train_files, dev_files, test_files = [], [], []\n",
    "\n",
    "for f in all_files:\n",
    "    if train_duration < 0.8 * total_duration:\n",
    "        train_files.append(f)\n",
    "        train_duration += f[\"duration\"]\n",
    "    elif dev_duration < 0.1 * total_duration:\n",
    "        dev_files.append(f)\n",
    "        dev_duration += f[\"duration\"]\n",
    "    else:\n",
    "        test_files.append(f)\n",
    "        test_duration += f[\"duration\"]\n",
    "\n",
    "splits = {\n",
    "    \"train\": train_files,\n",
    "    \"dev\": dev_files,\n",
    "    \"test\": test_files\n",
    "}\n",
    "split_durations = {\n",
    "    \"train\": train_duration,\n",
    "    \"dev\": dev_duration,\n",
    "    \"test\": test_duration\n",
    "}\n",
    "\n",
    "# Step 3: Process each split and write RTTM\n",
    "all_df_rows = [] # Initialize list for DataFrame rows\n",
    "all_rttm_lines_combined = [] # Initialize list for all RTTM lines for the complete.rttm\n",
    "\n",
    "for split_name, files_in_split in splits.items():\n",
    "    rttm_lines_split = [] # RTTM lines for the current split\n",
    "\n",
    "    for f_info in files_in_split:\n",
    "        try:\n",
    "            with open(f_info[\"path\"], \"r\") as file_handle:\n",
    "                data = json.load(file_handle)\n",
    "            \n",
    "            uri = data.get('video_name', '')\n",
    "            files_processed += 1\n",
    "            \n",
    "            # Process annotations with new structure\n",
    "            for annotation in data.get('annotations', []):\n",
    "                try:\n",
    "                    event_id = annotation.get('eventId', '')\n",
    "                    \n",
    "                    if event_id not in valid_event_ids:\n",
    "                        continue\n",
    "                        \n",
    "                    # Get timing information\n",
    "                    start_sec = annotation.get('startTime', 0)\n",
    "                    end_sec = annotation.get('endTime', 0)\n",
    "                    duration_sec = end_sec - start_sec\n",
    "                    \n",
    "                    if duration_sec <= 0:\n",
    "                        print(f\"Warning: Non-positive duration {duration_sec:.3f}s for segment in {f_info['path']}. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Map event IDs to speaker IDs\n",
    "                    speaker_id = \"NA\"\n",
    "                    fields = annotation.get('fields', {})\n",
    "                    \n",
    "                    if event_id in [\"child_talking\", \"singing/humming\"]:\n",
    "                        speaker_id = \"KCHI\"\n",
    "                    elif event_id == \"other_person_talking\":\n",
    "                        speaker_id = \"CDS\"\n",
    "                    elif event_id == \"overheard_speech\":\n",
    "                        speaker_id = \"OHS\"     \n",
    "\n",
    "                    rttm_line = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> {speaker_id} <NA> <NA>\"\n",
    "                        \n",
    "                    if speaker_id in [\"KCHI\", \"CDS\", \"OHS\"]:\n",
    "                        rttm_lines_split.append(rttm_line)\n",
    "                        all_rttm_lines_combined.append(rttm_line)\n",
    "                        speaker_counts[speaker_id] += 1    \n",
    "                        \n",
    "                        # Add additional SPEECH line only for non-SPEECH speakers\n",
    "                        if speaker_id != \"SPEECH\":\n",
    "                            rttm_line_speech = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> SPEECH <NA> <NA>\"\n",
    "                            rttm_lines_split.append(rttm_line_speech)\n",
    "                            all_rttm_lines_combined.append(rttm_line_speech)\n",
    "                            speaker_counts[\"SPEECH\"] += 1\n",
    "                        \n",
    "                        row_data_specific = {\n",
    "                            \"audio_file_name\": uri,\n",
    "                            \"Utterance_Start\": round(start_sec, 3),\n",
    "                            \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                            \"Voice_type\": speaker_id,\n",
    "                            \"Utterance_End\": round(end_sec, 3)\n",
    "                        }\n",
    "                        all_df_rows.append(row_data_specific)\n",
    "\n",
    "                        if speaker_id != \"SPEECH\":\n",
    "                            row_data_speech = {\n",
    "                                \"audio_file_name\": uri,\n",
    "                                \"Utterance_Start\": round(start_sec, 3),\n",
    "                                \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                                \"Voice_type\": \"SPEECH\",\n",
    "                                \"Utterance_End\": round(end_sec, 3)\n",
    "                            }\n",
    "                            all_df_rows.append(row_data_speech)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping annotation in {f_info['path']} due to error: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f_info['path']}: {e}\")\n",
    "\n",
    "    # Save to RTTM file for the current split\n",
    "    rttm_path_split = output_dir / f\"{split_name}.rttm\"\n",
    "    with open(rttm_path_split, \"w\") as out_f:\n",
    "        for line in rttm_lines_split:\n",
    "            out_f.write(line + \"\\n\")\n",
    "    print(f\"üìù RTTM file for {split_name} split saved to {rttm_path_split}\")\n",
    "\n",
    "# Save the complete RTTM file after processing all splits\n",
    "complete_rttm_path = output_dir / \"complete.rttm\"\n",
    "with open(complete_rttm_path, \"w\") as out_f:\n",
    "    for line in all_rttm_lines_combined:\n",
    "        out_f.write(line + \"\\n\")\n",
    "print(f\"üìù Complete RTTM file for all splits saved to {complete_rttm_path}\")\n",
    "\n",
    "# Create and save ONE COMBINED DataFrame after processing all splits\n",
    "if all_df_rows:\n",
    "    combined_df = pd.DataFrame(all_df_rows)\n",
    "    # Ensure desired column order\n",
    "    combined_df = combined_df[[\"audio_file_name\", \"Utterance_Start\", \"Utterance_Duration\", \"Voice_type\", \"Utterance_End\"]]\n",
    "    # Define the single output path for the combined pickle file\n",
    "    df_pkl_path = Path(\"/home/nele_pauline_suffo/ProcessedData/audio_cls_input/annotations_gt.pkl\")\n",
    "    # Ensure the directory exists\n",
    "    df_pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    combined_df.to_pickle(df_pkl_path)\n",
    "    print(f\"‚úÖ Combined DataFrame for all splits saved to {df_pkl_path} ({len(combined_df)} rows)\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è No data to create combined DataFrame.\")\n",
    "\n",
    "# Step 3.5: Save a complete UEM file with all video information\n",
    "uem_lines = []\n",
    "for f in all_files:\n",
    "    uri = f[\"uri\"]\n",
    "    start_time = 0.000\n",
    "    end_time = f[\"duration\"]\n",
    "    uem_lines.append(f\"{uri} 1 {start_time:.3f} {end_time:.3f}\")\n",
    "\n",
    "uem_path = output_dir / \"complete.uem\"\n",
    "with open(uem_path, \"w\") as uem_file:\n",
    "    for line in uem_lines:\n",
    "        uem_file.write(line + \"\\n\")\n",
    "print(f\"‚úÖ Combined UEM file for all videos saved to {uem_path} ({len(uem_lines)} segments)\")\n",
    "\n",
    "# Step 4: Summary logs\n",
    "print(f\"\\n‚úÖ Total processed files: {files_processed}\")\n",
    "print(\"\\nüéôÔ∏è Speaker total durations in all splits (in minutes):\")\n",
    "if not combined_df.empty:\n",
    "    speaker_durations = combined_df.groupby(\"Voice_type\")[\"Utterance_Duration\"].sum()\n",
    "    for speaker_id in ['KCHI', 'CDS', 'OHS', 'SPEECH']:\n",
    "        duration_sec = speaker_durations.get(speaker_id, 0.0)\n",
    "        duration_min = duration_sec / 60\n",
    "        print(f\"  {speaker_id}: {duration_min:.2f} min\")\n",
    "else:\n",
    "    print(\"No data available to compute durations.\")\n",
    "\n",
    "print(\"\\nüìä RTTM split durations and video counts:\")\n",
    "for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "    dur = split_durations[split_name]\n",
    "    perc = (dur / total_duration) * 100\n",
    "    count = len(splits[split_name])\n",
    "    print(f\"  {split_name}: {dur:.2f} sec ({perc:.1f}%), {count} videos\")\n",
    "\n",
    "# Generate .lst files for train, development, and test splits\n",
    "for split_name, files in splits.items():\n",
    "    lst_path = output_dir / f\"{split_name}.lst\"\n",
    "    with open(lst_path, \"w\") as lst_file:\n",
    "        for f in files:\n",
    "            lst_file.write(f\"{f['uri']}\\n\")\n",
    "\n",
    "print(\"‚úÖ .lst files created for train, development, and test splits.\")\n",
    "\n",
    "# Generate .uem files for train, development, and test splits\n",
    "for split_name, files in splits.items():\n",
    "    uem_path = output_dir / f\"{split_name}.uem\"\n",
    "    with open(uem_path, \"w\") as uem_file:\n",
    "        for f in files:\n",
    "            try:\n",
    "                # Extract the URI and duration for each video\n",
    "                uri = f[\"uri\"]\n",
    "                start = 0  # Start time is always 0\n",
    "                end = f[\"duration\"]  # End time is the video's duration\n",
    "\n",
    "                # Write a single line for each video\n",
    "                uem_line = f\"{uri} 1 {start:.3f} {end:.3f}\"\n",
    "                uem_file.write(uem_line + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {f['path']}: {e}\")\n",
    "\n",
    "print(\"‚úÖ .uem files created for train, development, and test splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83071ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyannote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
