{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_face_samples():\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "\n",
    "    conn = sqlite3.connect('/home/nele_pauline_suffo/outputs/detection_pipeline_results/detection_results.db')\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "    WITH RankedFaces AS (\n",
    "        SELECT \n",
    "            v.video_id,\n",
    "            v.video_path,\n",
    "            d.frame_number,\n",
    "            d.confidence_score,\n",
    "            d.proximity,\n",
    "            d.object_class,\n",
    "            d.x_min,\n",
    "            d.y_min, \n",
    "            d.x_max,\n",
    "            d.y_max,\n",
    "            CAST(d.proximity * 10 AS INTEGER) AS proximity_bin\n",
    "        FROM Detections d\n",
    "        JOIN Videos v ON d.video_id = v.video_id\n",
    "        WHERE d.object_class IN (2, 3)\n",
    "            AND d.proximity BETWEEN 0 AND 1\n",
    "    ),\n",
    "    PerVideoSamples AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY \n",
    "                    object_class,\n",
    "                    proximity_bin,\n",
    "                    video_id\n",
    "                ORDER BY RANDOM()\n",
    "            ) AS rn_per_video\n",
    "        FROM RankedFaces\n",
    "    ),\n",
    "    DiverseSamples AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY object_class, proximity_bin\n",
    "                ORDER BY RANDOM()\n",
    "            ) AS rn_total\n",
    "        FROM PerVideoSamples\n",
    "        WHERE rn_per_video = 1  -- Only one sample per video per bin per class\n",
    "    )\n",
    "    SELECT \n",
    "        '/home/nele_pauline_suffo/ProcessedData/quantex_videos_processed/' || video_path || '/' || \n",
    "        video_path || '_' || printf('%06d', frame_number) || '.jpg' AS frame_file_name,\n",
    "        video_id,\n",
    "        confidence_score,\n",
    "        proximity,\n",
    "        proximity_bin,\n",
    "        x_min,\n",
    "        y_min,\n",
    "        x_max,\n",
    "        y_max,\n",
    "        CASE \n",
    "            WHEN object_class = 3 THEN 'adult' \n",
    "            WHEN object_class = 2 THEN 'child' \n",
    "        END AS age_group,\n",
    "        CAST(proximity_bin/10.0 AS TEXT) || '-' || CAST((proximity_bin + 1)/10.0 AS TEXT) AS proximity_range\n",
    "    FROM DiverseSamples\n",
    "    WHERE rn_total <= 10  -- Get max 10 per bin per class\n",
    "    ORDER BY age_group, proximity_bin, video_id;\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    # Log summary statistics\n",
    "    summary_df = df.groupby(['age_group', 'proximity_range']).agg({\n",
    "        'frame_file_name': 'count'\n",
    "    }).reset_index()\n",
    "    summary_df.columns = ['Age Group', 'Proximity Range', 'Total Samples']\n",
    "    \n",
    "    logging.info(\"\\nSummary per bin:\")\n",
    "    logging.info(summary_df.to_string())\n",
    "    \n",
    "    # Save to CSV with bounding box information\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(f\"/home/nele_pauline_suffo/outputs/proximity_sampled_frames/proximity_samples_{current_time}.csv\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def copy_face_samples(df):\n",
    "    # First verify we have data\n",
    "    if df.empty:\n",
    "        logging.error(\"DataFrame is empty - no samples to copy\")\n",
    "        return\n",
    "        \n",
    "    logging.info(f\"Processing {len(df)} samples\")\n",
    "    logging.info(f\"Distribution:\\n{df.groupby('age_group').size()}\")\n",
    "    \n",
    "    # Define output directories\n",
    "    base_dir = Path('/home/nele_pauline_suffo/outputs/proximity_sampled_frames')\n",
    "    adult_dir = base_dir / 'adult_faces'\n",
    "    child_dir = base_dir / 'child_faces'\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    adult_dir.mkdir(parents=True, exist_ok=True)\n",
    "    child_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy files for each age group\n",
    "    copied_count = {'adult': 0, 'child': 0}\n",
    "    error_count = {'adult': 0, 'child': 0}\n",
    "    \n",
    "    # Define colors for bounding boxes (BGR format)\n",
    "    colors = {\n",
    "        'adult': (0, 255, 0),  # Green for adult\n",
    "        'child': (0, 0, 255)   # Red for child\n",
    "    }\n",
    "    \n",
    "    # Create random orders for each age group\n",
    "    for age_group in ['adult', 'child']:\n",
    "        age_df = df[df['age_group'] == age_group].copy()\n",
    "        # Create random order indices\n",
    "        random_indices = np.random.permutation(len(age_df))\n",
    "        age_df['random_prefix'] = [f\"{i:03d}\" for i in random_indices]\n",
    "        \n",
    "        target_dir = adult_dir if age_group == 'adult' else child_dir\n",
    "        \n",
    "        for _, row in age_df.iterrows():\n",
    "            src_path = Path(row['frame_file_name'])\n",
    "            \n",
    "            if not src_path.exists():\n",
    "                logging.warning(f\"Source file not found: {src_path}\")\n",
    "                error_count[age_group] += 1\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Read the image\n",
    "                img = cv2.imread(str(src_path))\n",
    "                if img is None:\n",
    "                    logging.error(f\"Could not read image: {src_path}\")\n",
    "                    error_count[age_group] += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = int(row['x_min']), int(row['y_min']), int(row['x_max']), int(row['y_max'])\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), colors[age_group], 2)\n",
    "                \n",
    "                # Create new filename with random prefix and proximity\n",
    "                new_filename = f\"{row['random_prefix']}_{src_path.name}\"\n",
    "                dst_path = target_dir / new_filename\n",
    "                \n",
    "                # Save the image with bounding box\n",
    "                cv2.imwrite(str(dst_path), img)\n",
    "                \n",
    "                copied_count[age_group] += 1\n",
    "                if copied_count[age_group] % 10 == 0:\n",
    "                    logging.info(f\"Copied {copied_count[age_group]} {age_group} face images\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {src_path}: {e}\")\n",
    "                error_count[age_group] += 1\n",
    "\n",
    "    # Log final statistics\n",
    "    for age_group in ['adult', 'child']:\n",
    "        logging.info(f\"{age_group.title()} faces - Copied: {copied_count[age_group]}, \"\n",
    "                    f\"Errors: {error_count[age_group]}\")\n",
    "    \n",
    "def create_empty_proximity_xlsx(df):\n",
    "    \"\"\"Create a single Excel file with separate sheets for adult and child faces.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the face samples data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import logging\n",
    "\n",
    "    base_dir = Path('/home/nele_pauline_suffo/outputs/proximity_sampled_frames')\n",
    "    output_path = base_dir / 'proximity_samples_fill_in.xlsx'\n",
    "    \n",
    "    # Create Excel writer object\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        # Process each age group\n",
    "        for age_group in ['adult', 'child']:\n",
    "            folder_path = base_dir / f'{age_group}_faces'\n",
    "            \n",
    "            if not folder_path.exists():\n",
    "                logging.error(f\"Folder not found: {folder_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Get all jpg files from the folder\n",
    "            frame_files = sorted(folder_path.glob('*.jpg'))\n",
    "            \n",
    "            # Create DataFrame with filenames and empty proximity column\n",
    "            output_df = pd.DataFrame({\n",
    "                'frame_file_name': [f.name for f in frame_files],\n",
    "                'proximity': ''  # empty column for manual proximity values\n",
    "            })\n",
    "            \n",
    "            # Write to specific sheet in Excel file\n",
    "            sheet_name = 'Adult Faces' if age_group == 'adult' else 'Child Faces'\n",
    "            output_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "            logging.info(f\"Added {len(output_df)} {age_group} frames to sheet '{sheet_name}'\")\n",
    "    \n",
    "    logging.info(f\"Created Excel file at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 14:14:37,288 - INFO - \n",
      "Summary per bin:\n",
      "2025-04-08 14:14:37,291 - INFO -    Age Group Proximity Range  Total Samples\n",
      "0      adult         0.0-0.1             10\n",
      "1      adult         0.1-0.2             10\n",
      "2      adult         0.2-0.3             10\n",
      "3      adult         0.3-0.4             10\n",
      "4      adult         0.4-0.5             10\n",
      "5      adult         0.5-0.6             10\n",
      "6      adult         0.6-0.7             10\n",
      "7      adult         0.7-0.8             10\n",
      "8      adult         0.8-0.9             10\n",
      "9      adult         0.9-1.0             10\n",
      "10     adult         1.0-1.1             10\n",
      "11     child         0.0-0.1             10\n",
      "12     child         0.1-0.2             10\n",
      "13     child         0.2-0.3             10\n",
      "14     child         0.3-0.4             10\n",
      "15     child         0.4-0.5             10\n",
      "16     child         0.5-0.6             10\n",
      "17     child         0.6-0.7             10\n",
      "18     child         0.7-0.8             10\n",
      "19     child         0.8-0.9             10\n",
      "20     child         0.9-1.0             10\n",
      "21     child         1.0-1.1             10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heuristic_df = get_balanced_face_samples()\n",
    "\n",
    "len(heuristic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 14:22:59,921 - INFO - Processing 220 samples\n",
      "2025-04-08 14:22:59,925 - INFO - Distribution:\n",
      "age_group\n",
      "adult    110\n",
      "child    110\n",
      "dtype: int64\n",
      "2025-04-08 14:23:00,741 - INFO - Copied 10 adult face images\n",
      "2025-04-08 14:23:10,831 - INFO - Copied 20 adult face images\n",
      "2025-04-08 14:23:12,366 - INFO - Copied 30 adult face images\n",
      "2025-04-08 14:23:13,231 - INFO - Copied 40 adult face images\n",
      "2025-04-08 14:23:14,102 - INFO - Copied 50 adult face images\n",
      "2025-04-08 14:23:14,852 - INFO - Copied 60 adult face images\n",
      "2025-04-08 14:23:15,647 - INFO - Copied 70 adult face images\n",
      "2025-04-08 14:23:16,631 - INFO - Copied 80 adult face images\n",
      "2025-04-08 14:23:17,409 - INFO - Copied 90 adult face images\n",
      "2025-04-08 14:23:18,213 - INFO - Copied 100 adult face images\n",
      "2025-04-08 14:23:19,102 - INFO - Copied 110 adult face images\n",
      "2025-04-08 14:23:19,862 - INFO - Copied 10 child face images\n",
      "2025-04-08 14:23:20,748 - INFO - Copied 20 child face images\n",
      "2025-04-08 14:23:21,588 - INFO - Copied 30 child face images\n",
      "2025-04-08 14:23:27,144 - INFO - Copied 40 child face images\n",
      "2025-04-08 14:23:42,340 - INFO - Copied 50 child face images\n",
      "2025-04-08 14:23:48,085 - INFO - Copied 60 child face images\n",
      "2025-04-08 14:23:49,236 - INFO - Copied 70 child face images\n",
      "2025-04-08 14:23:50,069 - INFO - Copied 80 child face images\n",
      "2025-04-08 14:23:50,836 - INFO - Copied 90 child face images\n",
      "2025-04-08 14:23:51,632 - INFO - Copied 100 child face images\n",
      "2025-04-08 14:23:52,519 - INFO - Copied 110 child face images\n",
      "2025-04-08 14:23:52,520 - INFO - Adult faces - Copied: 110, Errors: 0\n",
      "2025-04-08 14:23:52,520 - INFO - Child faces - Copied: 110, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# copy face samples to output directory\n",
    "copy_face_samples(heuristic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 14:27:19,893 - INFO - Added 110 adult frames to sheet 'Adult Faces'\n",
      "2025-04-08 14:27:19,899 - INFO - Added 110 child frames to sheet 'Child Faces'\n",
      "2025-04-08 14:27:19,916 - INFO - Created Excel file at: /home/nele_pauline_suffo/outputs/proximity_sampled_frames/proximity_samples_fill_in.xlsx\n"
     ]
    }
   ],
   "source": [
    "# create empty proximity CSV for manual input\n",
    "create_empty_proximity_xlsx(heuristic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate the heuristic and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xlsx file with proximity values\n",
    "proximity_df_adult_1 = pd.read_excel('/home/nele_pauline_suffo/ProcessedData/proximity_samples_fill_in_1.xlsx', sheet_name=\"Adult Faces\")\n",
    "proximity_df_child_1 = pd.read_excel('/home/nele_pauline_suffo/ProcessedData/proximity_samples_fill_in_1.xlsx', sheet_name='Child Faces')\n",
    "proximity_df_adult_2 = pd.read_excel('/home/nele_pauline_suffo/ProcessedData/proximity_samples_fill_in_2.xlsx', sheet_name=\"Adult Faces\")\n",
    "proximity_df_child_2 = pd.read_excel('/home/nele_pauline_suffo/ProcessedData/proximity_samples_fill_in_2.xlsx', sheet_name='Child Faces')\n",
    "# only keep rows that show NaN in column wrong\n",
    "# only keep rows that show NaN in column wrong\n",
    "proximity_df_adult_clean_1 = proximity_df_adult_1[proximity_df_adult_1['wrong'].isna()]\n",
    "proximity_df_child_clean_1 = proximity_df_child_1[proximity_df_child_1['wrong'].isna()]\n",
    "proximity_df_adult_clean_2 = proximity_df_adult_1[proximity_df_adult_2['wrong'].isna()]\n",
    "proximity_df_child_clean_2 = proximity_df_child_2[proximity_df_child_2['wrong'].isna()]\n",
    "\n",
    "# add column type \"adult\" and drop column \"wrong\"\n",
    "proximity_df_adult_clean_1 = proximity_df_adult_clean_1.drop(columns=['wrong'])\n",
    "proximity_df_child_clean_1 = proximity_df_child_clean_1.drop(columns=['wrong'])\n",
    "proximity_df_adult_clean_2 = proximity_df_adult_clean_2.drop(columns=['wrong'])\n",
    "proximity_df_child_clean_2 = proximity_df_child_clean_2.drop(columns=['wrong'])\n",
    "proximity_df_adult_clean_1['age_group'] = 'adult'\n",
    "proximity_df_child_clean_1['age_group'] = 'child'\n",
    "proximity_df_adult_clean_2['age_group'] = 'adult'\n",
    "proximity_df_child_clean_2['age_group'] = 'child'\n",
    "\n",
    "# combine both dataframes\n",
    "proximity_df = pd.concat([proximity_df_adult_clean_1, proximity_df_child_clean_1, proximity_df_adult_clean_2, proximity_df_child_clean_2], ignore_index=True)\n",
    "\n",
    "# remove the first four values of the frame_file_name column\n",
    "proximity_df['frame_file_name'] = proximity_df['frame_file_name'].str[4:]\n",
    "\n",
    "# remove duplicate rows based on frame_file_name\n",
    "proximity_df_cleared = proximity_df.drop_duplicates(subset=['frame_file_name'])\n",
    "\n",
    "# load csv with proximity heuristic values from csv as dataframe\n",
    "proximity_df_heuristic = pd.read_csv('/home/nele_pauline_suffo/outputs/proximity_sampled_frames/proximity_samples_20250408_141437.csv')[['frame_file_name', 'proximity']]\n",
    "proximity_df_heuristic['frame_file_name'] = proximity_df_heuristic['frame_file_name'].str.split('/').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>proximity_true</th>\n",
       "      <th>proximity_heuristic</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quantex_at_home_id260123_2023_09_06_01_013290.jpg</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_021500.jpg</td>\n",
       "      <td>0.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_011640.jpg</td>\n",
       "      <td>0.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quantex_at_home_id264351_2024_11_23_03_016520.jpg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_04_050240.jpg</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               frame  proximity_true  \\\n",
       "0  quantex_at_home_id260123_2023_09_06_01_013290.jpg            0.87   \n",
       "1  quantex_at_home_id262726_2023_03_26_01_021500.jpg            0.81   \n",
       "2  quantex_at_home_id260275_2022_04_16_01_011640.jpg            0.21   \n",
       "3  quantex_at_home_id264351_2024_11_23_03_016520.jpg            0.73   \n",
       "4  quantex_at_home_id263284_2023_06_25_04_050240.jpg            0.91   \n",
       "\n",
       "   proximity_heuristic age_group  \n",
       "0                  NaN     adult  \n",
       "1                  NaN     adult  \n",
       "2                  NaN     adult  \n",
       "3                  NaN     adult  \n",
       "4                  NaN     adult  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now add the proximity values from the heuristic dataframe to the proximity_df dataframe\n",
    "proximity_eval_df = proximity_df_cleared.merge(proximity_df_heuristic, on='frame_file_name', how='left')\n",
    "proximity_eval_df = proximity_eval_df.rename(columns={'frame_file_name': 'frame', 'proximity_x': 'proximity_true', 'proximity_y': 'proximity_heuristic'})\n",
    "# change column order\n",
    "proximity_eval_df = proximity_eval_df[['frame', 'proximity_true', 'proximity_heuristic', 'age_group']]\n",
    "proximity_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows that have proximity_heuristic as NaN\n",
    "proximity_df_without_heuristic = proximity_eval_df[proximity_eval_df['proximity_heuristic'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation: 0.8954845540630043\n",
      "Spearman Correlation: 0.8723470369321888\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Calculate Pearson Correlation Coefficient\n",
    "pearson_corr = proximity_eval_df['proximity_true'].corr(proximity_eval_df['proximity_heuristic'])\n",
    "print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "\n",
    "# Calculate Spearman Rank Correlation\n",
    "spearman_corr = proximity_eval_df['proximity_true'].corr(proximity_eval_df['proximity_heuristic'], method='spearman')\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "\n",
    "# Visualize the relationship using a scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x='proximity_true', y='proximity_heuristic', data=proximity_eval_df)\n",
    "plt.title('Scatter Plot of True vs. Heuristic Proximity')\n",
    "plt.xlabel('True Proximity')\n",
    "plt.ylabel('Heuristic Proximity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/nele_pauline_suffo/projects/leuphana-IPE/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>proximity_heuristic</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quantex_at_home_id260123_2023_09_06_01_013290.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_021500.jpg</td>\n",
       "      <td>0.29,0.33</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_011640.jpg</td>\n",
       "      <td>0.37</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quantex_at_home_id264351_2024_11_23_03_016520.jpg</td>\n",
       "      <td>1.0,0.6</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_04_050240.jpg</td>\n",
       "      <td>0.59,0.45,0.51</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quantex_at_home_id254922_2022_06_29_01_021030.jpg</td>\n",
       "      <td>0.3,0.16</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_023830.jpg</td>\n",
       "      <td>0.57,0.44,0.22,0.19</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_04_032620.jpg</td>\n",
       "      <td>0.12</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_24_01_032720.jpg</td>\n",
       "      <td>0.23</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quantex_at_home_id266822_2022_11_12_02_002890.jpg</td>\n",
       "      <td>0.47</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>quantex_at_home_id263204_2024_12_30_01_000130.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>quantex_at_home_id271611_2024_09_08_03_035360.jpg</td>\n",
       "      <td>0.89,0.37</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_04_015340.jpg</td>\n",
       "      <td>0.47,0.52,0.47</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quantex_at_home_id264683_2024_09_02_01_036800.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>quantex_at_home_id262565_2022_05_08_01_008050.jpg</td>\n",
       "      <td>0.51,1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>quantex_at_home_id262356_2023_09_10_01_004850.jpg</td>\n",
       "      <td>0.78,0.49</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_029240.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>quantex_at_home_id257608_2021_12_20_02_019350.jpg</td>\n",
       "      <td>0.79</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>quantex_at_home_id271700_2023_06_16_01_013600.jpg</td>\n",
       "      <td>0.59</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quantex_at_home_id258239_2021_02_15_01_032340.jpg</td>\n",
       "      <td>0.31</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_027310.jpg</td>\n",
       "      <td>0.5,0.27</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>quantex_at_home_id267529_2024_12_23_01_006990.jpg</td>\n",
       "      <td>0.48</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>quantex_at_home_id275411_2024_11_22_02_013120.jpg</td>\n",
       "      <td>0.66</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_021030.jpg</td>\n",
       "      <td>0.19</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_038560.jpg</td>\n",
       "      <td>0.07</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_032150.jpg</td>\n",
       "      <td>0.68,0.69,0.74</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>quantex_at_home_id264089_2023_05_14_01_030790.jpg</td>\n",
       "      <td>0.16</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>quantex_at_home_id264089_2023_05_14_01_019040.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>quantex_at_home_id262521_2024_12_15_02_012490.jpg</td>\n",
       "      <td>0.46</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>quantex_at_home_id254922_2022_06_29_01_021380.jpg</td>\n",
       "      <td>0.22</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_027330.jpg</td>\n",
       "      <td>1.0,0.16,0.19,0.14</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_04_025260.jpg</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>quantex_at_home_id260178_2023_08_12_03_042040.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>quantex_at_home_id258541_2023_03_26_01_019320.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>quantex_at_home_id266822_2022_11_12_02_046200.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>quantex_at_home_id264351_2024_11_23_03_010000.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_024290.jpg</td>\n",
       "      <td>0.14</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>quantex_at_home_id262726_2023_04_20_01_037150.jpg</td>\n",
       "      <td>0.6,0.51</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_029230.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_049840.jpg</td>\n",
       "      <td>0.77,1.0,1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>quantex_at_home_id271700_2023_03_27_01_010730.jpg</td>\n",
       "      <td>0.81</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>quantex_at_home_id271700_2023_03_27_01_008790.jpg</td>\n",
       "      <td>0.82</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>quantex_at_home_id267529_2024_12_23_01_041380.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_020250.jpg</td>\n",
       "      <td>0.33,0.3</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_15_02_026780.jpg</td>\n",
       "      <td>0.89,0.76</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_021150.jpg</td>\n",
       "      <td>0.54,0.21</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>quantex_at_home_id271700_2023_03_27_01_026440.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>quantex_at_home_id271611_2024_09_08_03_034960.jpg</td>\n",
       "      <td>0.34,1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>quantex_at_home_id266606_2024_09_15_01_025050.jpg</td>\n",
       "      <td>0.72</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>quantex_at_home_id254922_2022_06_29_01_031090.jpg</td>\n",
       "      <td>0.23</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>quantex_at_home_id254922_2022_06_29_01_030800.jpg</td>\n",
       "      <td>0.25,0.25</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_031530.jpg</td>\n",
       "      <td>0.55</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_052620.jpg</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_031820.jpg</td>\n",
       "      <td>0.74,0.23,0.21</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>quantex_at_home_id263190_2022_07_12_01_010860.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>quantex_at_home_id257578_2021_05_12_02_008910.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_011930.jpg</td>\n",
       "      <td>0.28</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>quantex_at_home_id262333_2024_12_01_03_011230.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_035870.jpg</td>\n",
       "      <td>0.72,0.73,1.0,0.67</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>quantex_at_home_id263986_2022_12_05_01_000190.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_021540.jpg</td>\n",
       "      <td>0.32,0.32</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_043360.jpg</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>quantex_at_home_id263204_2025_01_06_01_048740.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>quantex_at_home_id271700_2023_06_16_01_011050.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>quantex_at_home_id258704_2022_05_10_02_007070.jpg</td>\n",
       "      <td>0.55</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>quantex_at_home_id254922_2022_06_29_01_020570.jpg</td>\n",
       "      <td>0.25</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>quantex_at_home_id263986_2022_12_05_01_017730.jpg</td>\n",
       "      <td>0.76,0.29</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>quantex_at_home_id264351_2024_11_23_03_027610.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>quantex_at_home_id265514_2024_09_03_05_002990.jpg</td>\n",
       "      <td>0.54</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_051080.jpg</td>\n",
       "      <td>0.04</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>quantex_at_home_id262333_2024_12_01_03_011170.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_045970.jpg</td>\n",
       "      <td>0.76</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>quantex_at_home_id263204_2025_01_06_01_010960.jpg</td>\n",
       "      <td>0.93</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_017870.jpg</td>\n",
       "      <td>0.3</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>quantex_at_home_id264089_2023_05_14_01_005880.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_046900.jpg</td>\n",
       "      <td>0.09</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_044640.jpg</td>\n",
       "      <td>0.28,0.11</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_024500.jpg</td>\n",
       "      <td>0.34</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>quantex_at_home_id264356_2023_07_05_04_013650.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>quantex_at_home_id260123_2023_08_31_01_016380.jpg</td>\n",
       "      <td>0.49</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>quantex_at_home_id268898_2022_11_30_01_052810.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>quantex_at_home_id267863_2022_08_23_01_045520.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_042650.jpg</td>\n",
       "      <td>0.84</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>quantex_at_home_id258239_2021_02_15_01_043330.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_01_06_053030.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_049700.jpg</td>\n",
       "      <td>0.88,1.0,0.63</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>quantex_at_home_id267863_2022_08_23_01_028220.jpg</td>\n",
       "      <td>0.74</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_028120.jpg</td>\n",
       "      <td>0.49</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_016280.jpg</td>\n",
       "      <td>0.72</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_029030.jpg</td>\n",
       "      <td>0.85</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_031510.jpg</td>\n",
       "      <td>0.21</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_024520.jpg</td>\n",
       "      <td>0.94,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>quantex_at_home_id262208_2022_11_05_03_011680.jpg</td>\n",
       "      <td>0.66</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_010780.jpg</td>\n",
       "      <td>0.62</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>quantex_at_home_id262565_2022_05_08_01_020570.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>quantex_at_home_id266971_2024_09_03_01_052950.jpg</td>\n",
       "      <td>0.76,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>quantex_at_home_id266971_2024_09_06_04_012540.jpg</td>\n",
       "      <td>0.85</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_01_06_045610.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_024500.jpg</td>\n",
       "      <td>0.9,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>quantex_at_home_id257511_2021_07_06_02_030970.jpg</td>\n",
       "      <td>0.41</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>quantex_at_home_id258541_2023_03_26_01_020720.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>quantex_at_home_id258239_2021_02_15_01_021460.jpg</td>\n",
       "      <td>0.21</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>quantex_at_home_id262356_2023_09_10_01_037340.jpg</td>\n",
       "      <td>0.75</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_016160.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>quantex_at_home_id264683_2024_09_02_01_036780.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_016050.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_017740.jpg</td>\n",
       "      <td>1.0,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_01_06_008910.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_018110.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_048820.jpg</td>\n",
       "      <td>0.2</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_016300.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>quantex_at_home_id262208_2022_11_05_03_034900.jpg</td>\n",
       "      <td>0.74,1.0,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_01_06_010120.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_025660.jpg</td>\n",
       "      <td>0.9</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>quantex_at_home_id264089_2023_05_14_01_015770.jpg</td>\n",
       "      <td>0.25</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_015610.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_042090.jpg</td>\n",
       "      <td>0.83</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_04_006860.jpg</td>\n",
       "      <td>1.0,0.46</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>quantex_at_home_id257609_2022_11_09_01_008380.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>quantex_at_home_id258541_2023_03_26_01_020280.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>quantex_at_home_id263204_2024_12_30_01_041000.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_01_01_047890.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>quantex_at_home_id254922_2022_06_29_02_008350.jpg</td>\n",
       "      <td>1.0,0.42</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>quantex_at_home_id262179_2023_01_22_03_049700.jpg</td>\n",
       "      <td>0.57</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_15_02_006650.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>quantex_at_home_id264089_2023_05_21_01_004690.jpg</td>\n",
       "      <td>0.97</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>quantex_at_home_id257511_2021_07_06_02_037420.jpg</td>\n",
       "      <td>1.0,0.52</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>quantex_at_home_id264089_2023_05_14_01_042700.jpg</td>\n",
       "      <td>0.9</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>quantex_at_home_id257511_2021_07_06_02_031810.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_031520.jpg</td>\n",
       "      <td>0.25</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_017320.jpg</td>\n",
       "      <td>0.43,0.12</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>quantex_at_home_id266971_2024_09_03_01_044080.jpg</td>\n",
       "      <td>0.69</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>quantex_at_home_id257608_2021_12_20_01_049380.jpg</td>\n",
       "      <td>0.53,0.38</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>quantex_at_home_id273855_2023_06_08_01_051980.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>quantex_at_home_id258541_2023_03_26_01_013570.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_017310.jpg</td>\n",
       "      <td>0.43,0.11</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>quantex_at_home_id262179_2023_01_22_03_049740.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>quantex_at_home_id268898_2022_11_30_01_007980.jpg</td>\n",
       "      <td>0.66</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_044470.jpg</td>\n",
       "      <td>0.93</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_027450.jpg</td>\n",
       "      <td>0.78</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>quantex_at_home_id262565_2022_05_08_01_053850.jpg</td>\n",
       "      <td>0.59</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_031500.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>quantex_at_home_id258239_2021_02_15_01_040920.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_04_007130.jpg</td>\n",
       "      <td>0.45,1.0,1.0,0.38</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>quantex_at_home_id258239_2021_02_15_01_005870.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>quantex_at_home_id266971_2024_09_03_01_021280.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_01_06_046500.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_029380.jpg</td>\n",
       "      <td>0.17</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>quantex_at_home_id258239_2021_02_15_01_038750.jpg</td>\n",
       "      <td>0.43,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>quantex_at_home_id257609_2022_11_09_01_018600.jpg</td>\n",
       "      <td>0.67</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>quantex_at_home_id266971_2024_09_03_01_037910.jpg</td>\n",
       "      <td>1.0,1.0</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_042810.jpg</td>\n",
       "      <td>0.82</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_029450.jpg</td>\n",
       "      <td>0.88</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>quantex_at_home_id268898_2022_11_30_01_043160.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>quantex_at_home_id262726_2023_04_20_01_037740.jpg</td>\n",
       "      <td>0.57</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>quantex_at_home_id260120_2023_09_04_02_001440.jpg</td>\n",
       "      <td>0.71</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_018450.jpg</td>\n",
       "      <td>0.66,1.0,0.73,0.82</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_033980.jpg</td>\n",
       "      <td>0.68</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>quantex_at_home_id260772_2022_10_02_01_043450.jpg</td>\n",
       "      <td>0.56</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_043620.jpg</td>\n",
       "      <td>0.9,0.82</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>quantex_at_home_id273855_2023_06_04_02_016450.jpg</td>\n",
       "      <td>0.54</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>quantex_at_home_id260123_2023_08_31_01_018910.jpg</td>\n",
       "      <td>0.64</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>quantex_at_home_id267863_2022_08_23_01_017830.jpg</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>quantex_at_home_id260176_2022_11_06_02_010930.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>quantex_at_home_id261609_2022_04_15_02_011580.jpg</td>\n",
       "      <td>0.74,0.7</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_015430.jpg</td>\n",
       "      <td>0.6,0.63</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_007910.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_021940.jpg</td>\n",
       "      <td>0.77</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_029760.jpg</td>\n",
       "      <td>0.97</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_002850.jpg</td>\n",
       "      <td>0.09</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>quantex_at_home_id264356_2023_07_05_04_034320.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 frame  proximity_heuristic  \\\n",
       "0    quantex_at_home_id260123_2023_09_06_01_013290.jpg                  1.0   \n",
       "1    quantex_at_home_id262726_2023_03_26_01_021500.jpg            0.29,0.33   \n",
       "2    quantex_at_home_id260275_2022_04_16_01_011640.jpg                 0.37   \n",
       "3    quantex_at_home_id264351_2024_11_23_03_016520.jpg              1.0,0.6   \n",
       "4    quantex_at_home_id263284_2023_06_25_04_050240.jpg       0.59,0.45,0.51   \n",
       "5    quantex_at_home_id254922_2022_06_29_01_021030.jpg             0.3,0.16   \n",
       "6    quantex_at_home_id260275_2022_04_16_01_023830.jpg  0.57,0.44,0.22,0.19   \n",
       "7    quantex_at_home_id263284_2023_06_25_04_032620.jpg                 0.12   \n",
       "8    quantex_at_home_id262726_2023_03_24_01_032720.jpg                 0.23   \n",
       "9    quantex_at_home_id266822_2022_11_12_02_002890.jpg                 0.47   \n",
       "10   quantex_at_home_id263204_2024_12_30_01_000130.jpg                  1.0   \n",
       "11   quantex_at_home_id271611_2024_09_08_03_035360.jpg            0.89,0.37   \n",
       "12   quantex_at_home_id263207_2024_12_12_04_015340.jpg       0.47,0.52,0.47   \n",
       "13   quantex_at_home_id264683_2024_09_02_01_036800.jpg                  1.0   \n",
       "14   quantex_at_home_id262565_2022_05_08_01_008050.jpg             0.51,1.0   \n",
       "15   quantex_at_home_id262356_2023_09_10_01_004850.jpg            0.78,0.49   \n",
       "16   quantex_at_home_id263284_2023_06_25_06_029240.jpg                  NaN   \n",
       "17   quantex_at_home_id257608_2021_12_20_02_019350.jpg                 0.79   \n",
       "18   quantex_at_home_id271700_2023_06_16_01_013600.jpg                 0.59   \n",
       "19   quantex_at_home_id258239_2021_02_15_01_032340.jpg                 0.31   \n",
       "20   quantex_at_home_id260275_2022_04_16_01_027310.jpg             0.5,0.27   \n",
       "21   quantex_at_home_id267529_2024_12_23_01_006990.jpg                 0.48   \n",
       "22   quantex_at_home_id275411_2024_11_22_02_013120.jpg                 0.66   \n",
       "23   quantex_at_home_id260275_2022_04_16_01_021030.jpg                 0.19   \n",
       "24   quantex_at_home_id263284_2023_06_25_06_038560.jpg                 0.07   \n",
       "25   quantex_at_home_id263293_2022_08_29_01_032150.jpg       0.68,0.69,0.74   \n",
       "26   quantex_at_home_id264089_2023_05_14_01_030790.jpg                 0.16   \n",
       "27   quantex_at_home_id264089_2023_05_14_01_019040.jpg                  NaN   \n",
       "28   quantex_at_home_id262521_2024_12_15_02_012490.jpg                 0.46   \n",
       "30   quantex_at_home_id254922_2022_06_29_01_021380.jpg                 0.22   \n",
       "31   quantex_at_home_id260275_2022_04_16_01_027330.jpg   1.0,0.16,0.19,0.14   \n",
       "32   quantex_at_home_id263284_2023_06_25_04_025260.jpg                  0.1   \n",
       "33   quantex_at_home_id260178_2023_08_12_03_042040.jpg                  1.0   \n",
       "34   quantex_at_home_id258541_2023_03_26_01_019320.jpg                  NaN   \n",
       "35   quantex_at_home_id266822_2022_11_12_02_046200.jpg                  NaN   \n",
       "36   quantex_at_home_id264351_2024_11_23_03_010000.jpg                  NaN   \n",
       "37   quantex_at_home_id263207_2024_12_12_02_024290.jpg                 0.14   \n",
       "38   quantex_at_home_id262726_2023_04_20_01_037150.jpg             0.6,0.51   \n",
       "39   quantex_at_home_id263284_2023_06_25_06_029230.jpg                  NaN   \n",
       "40   quantex_at_home_id263293_2022_08_29_01_049840.jpg         0.77,1.0,1.0   \n",
       "41   quantex_at_home_id271700_2023_03_27_01_010730.jpg                 0.81   \n",
       "42   quantex_at_home_id271700_2023_03_27_01_008790.jpg                 0.82   \n",
       "43   quantex_at_home_id267529_2024_12_23_01_041380.jpg                  1.0   \n",
       "44   quantex_at_home_id260275_2022_04_16_01_020250.jpg             0.33,0.3   \n",
       "45   quantex_at_home_id261609_2022_04_15_02_026780.jpg            0.89,0.76   \n",
       "46   quantex_at_home_id260275_2022_04_16_01_021150.jpg            0.54,0.21   \n",
       "47   quantex_at_home_id271700_2023_03_27_01_026440.jpg                  1.0   \n",
       "48   quantex_at_home_id271611_2024_09_08_03_034960.jpg             0.34,1.0   \n",
       "49   quantex_at_home_id266606_2024_09_15_01_025050.jpg                 0.72   \n",
       "50   quantex_at_home_id254922_2022_06_29_01_031090.jpg                 0.23   \n",
       "51   quantex_at_home_id254922_2022_06_29_01_030800.jpg            0.25,0.25   \n",
       "52   quantex_at_home_id263293_2022_08_29_01_031530.jpg                 0.55   \n",
       "53   quantex_at_home_id263284_2023_06_25_06_052620.jpg                  0.1   \n",
       "54   quantex_at_home_id260275_2022_04_16_01_031820.jpg       0.74,0.23,0.21   \n",
       "55   quantex_at_home_id263190_2022_07_12_01_010860.jpg                  NaN   \n",
       "56   quantex_at_home_id257578_2021_05_12_02_008910.jpg                  1.0   \n",
       "57   quantex_at_home_id262726_2023_03_26_01_011930.jpg                 0.28   \n",
       "58   quantex_at_home_id262333_2024_12_01_03_011230.jpg                  1.0   \n",
       "59   quantex_at_home_id263293_2022_08_29_01_035870.jpg   0.72,0.73,1.0,0.67   \n",
       "60   quantex_at_home_id263986_2022_12_05_01_000190.jpg                  NaN   \n",
       "61   quantex_at_home_id262726_2023_03_26_01_021540.jpg            0.32,0.32   \n",
       "62   quantex_at_home_id263284_2023_06_25_06_043360.jpg                  0.1   \n",
       "63   quantex_at_home_id263204_2025_01_06_01_048740.jpg                  1.0   \n",
       "64   quantex_at_home_id271700_2023_06_16_01_011050.jpg                  1.0   \n",
       "65   quantex_at_home_id258704_2022_05_10_02_007070.jpg                 0.55   \n",
       "66   quantex_at_home_id254922_2022_06_29_01_020570.jpg                 0.25   \n",
       "67   quantex_at_home_id263986_2022_12_05_01_017730.jpg            0.76,0.29   \n",
       "68   quantex_at_home_id264351_2024_11_23_03_027610.jpg                  1.0   \n",
       "69   quantex_at_home_id265514_2024_09_03_05_002990.jpg                 0.54   \n",
       "70   quantex_at_home_id263284_2023_06_25_06_051080.jpg                 0.04   \n",
       "71   quantex_at_home_id262333_2024_12_01_03_011170.jpg                  1.0   \n",
       "72   quantex_at_home_id263293_2022_08_29_01_045970.jpg                 0.76   \n",
       "73   quantex_at_home_id263204_2025_01_06_01_010960.jpg                 0.93   \n",
       "74   quantex_at_home_id260176_2022_11_06_02_017870.jpg                  0.3   \n",
       "75   quantex_at_home_id264089_2023_05_14_01_005880.jpg                  1.0   \n",
       "76   quantex_at_home_id263284_2023_06_25_06_046900.jpg                 0.09   \n",
       "77   quantex_at_home_id260176_2022_11_06_02_044640.jpg            0.28,0.11   \n",
       "78   quantex_at_home_id262726_2023_03_26_01_024500.jpg                 0.34   \n",
       "79   quantex_at_home_id264356_2023_07_05_04_013650.jpg                  1.0   \n",
       "80   quantex_at_home_id260123_2023_08_31_01_016380.jpg                 0.49   \n",
       "81   quantex_at_home_id268898_2022_11_30_01_052810.jpg                  NaN   \n",
       "82   quantex_at_home_id267863_2022_08_23_01_045520.jpg                  1.0   \n",
       "83   quantex_at_home_id262758_2023_07_10_01_042650.jpg                 0.84   \n",
       "84   quantex_at_home_id258239_2021_02_15_01_043330.jpg                  NaN   \n",
       "85   quantex_at_home_id261609_2022_04_01_06_053030.jpg                  1.0   \n",
       "86   quantex_at_home_id263293_2022_08_29_01_049700.jpg        0.88,1.0,0.63   \n",
       "87   quantex_at_home_id267863_2022_08_23_01_028220.jpg                 0.74   \n",
       "88   quantex_at_home_id263207_2024_12_12_02_028120.jpg                 0.49   \n",
       "89   quantex_at_home_id260275_2022_04_16_01_016280.jpg                 0.72   \n",
       "90   quantex_at_home_id263293_2022_08_29_01_029030.jpg                 0.85   \n",
       "91   quantex_at_home_id260176_2022_11_06_02_031510.jpg                 0.21   \n",
       "92   quantex_at_home_id260275_2022_04_16_01_024520.jpg             0.94,1.0   \n",
       "93   quantex_at_home_id262208_2022_11_05_03_011680.jpg                 0.66   \n",
       "94   quantex_at_home_id262758_2023_07_10_01_010780.jpg                 0.62   \n",
       "95   quantex_at_home_id262565_2022_05_08_01_020570.jpg                  1.0   \n",
       "96   quantex_at_home_id266971_2024_09_03_01_052950.jpg             0.76,1.0   \n",
       "97   quantex_at_home_id266971_2024_09_06_04_012540.jpg                 0.85   \n",
       "98   quantex_at_home_id261609_2022_04_01_06_045610.jpg                  1.0   \n",
       "99   quantex_at_home_id260275_2022_04_16_01_024500.jpg              0.9,1.0   \n",
       "100  quantex_at_home_id257511_2021_07_06_02_030970.jpg                 0.41   \n",
       "101  quantex_at_home_id258541_2023_03_26_01_020720.jpg                  NaN   \n",
       "102  quantex_at_home_id258239_2021_02_15_01_021460.jpg                 0.21   \n",
       "103  quantex_at_home_id262356_2023_09_10_01_037340.jpg                 0.75   \n",
       "104  quantex_at_home_id260176_2022_11_06_02_016160.jpg                  1.0   \n",
       "105  quantex_at_home_id264683_2024_09_02_01_036780.jpg                  1.0   \n",
       "106  quantex_at_home_id260176_2022_11_06_02_016050.jpg                  NaN   \n",
       "107  quantex_at_home_id260275_2022_04_16_01_017740.jpg              1.0,1.0   \n",
       "108  quantex_at_home_id261609_2022_04_01_06_008910.jpg                  1.0   \n",
       "109  quantex_at_home_id263207_2024_12_12_02_018110.jpg                  NaN   \n",
       "110  quantex_at_home_id260176_2022_11_06_02_048820.jpg                  0.2   \n",
       "111  quantex_at_home_id262726_2023_03_26_01_016300.jpg                  NaN   \n",
       "112  quantex_at_home_id262208_2022_11_05_03_034900.jpg         0.74,1.0,1.0   \n",
       "113  quantex_at_home_id261609_2022_04_01_06_010120.jpg                  1.0   \n",
       "114  quantex_at_home_id262758_2023_07_10_01_025660.jpg                  0.9   \n",
       "115  quantex_at_home_id264089_2023_05_14_01_015770.jpg                 0.25   \n",
       "116  quantex_at_home_id260176_2022_11_06_02_015610.jpg                  1.0   \n",
       "117  quantex_at_home_id262758_2023_07_10_01_042090.jpg                 0.83   \n",
       "118  quantex_at_home_id263207_2024_12_12_04_006860.jpg             1.0,0.46   \n",
       "119  quantex_at_home_id257609_2022_11_09_01_008380.jpg                  NaN   \n",
       "120  quantex_at_home_id258541_2023_03_26_01_020280.jpg                  NaN   \n",
       "121  quantex_at_home_id263204_2024_12_30_01_041000.jpg                  1.0   \n",
       "122  quantex_at_home_id261609_2022_04_01_01_047890.jpg                  NaN   \n",
       "123  quantex_at_home_id254922_2022_06_29_02_008350.jpg             1.0,0.42   \n",
       "124  quantex_at_home_id262179_2023_01_22_03_049700.jpg                 0.57   \n",
       "125  quantex_at_home_id261609_2022_04_15_02_006650.jpg                  NaN   \n",
       "126  quantex_at_home_id264089_2023_05_21_01_004690.jpg                 0.97   \n",
       "127  quantex_at_home_id257511_2021_07_06_02_037420.jpg             1.0,0.52   \n",
       "128  quantex_at_home_id264089_2023_05_14_01_042700.jpg                  0.9   \n",
       "129  quantex_at_home_id257511_2021_07_06_02_031810.jpg                  1.0   \n",
       "130  quantex_at_home_id260176_2022_11_06_02_031520.jpg                 0.25   \n",
       "131  quantex_at_home_id263207_2024_12_12_02_017320.jpg            0.43,0.12   \n",
       "132  quantex_at_home_id266971_2024_09_03_01_044080.jpg                 0.69   \n",
       "133  quantex_at_home_id257608_2021_12_20_01_049380.jpg            0.53,0.38   \n",
       "134  quantex_at_home_id273855_2023_06_08_01_051980.jpg                  1.0   \n",
       "135  quantex_at_home_id258541_2023_03_26_01_013570.jpg                  NaN   \n",
       "136  quantex_at_home_id263207_2024_12_12_02_017310.jpg            0.43,0.11   \n",
       "137  quantex_at_home_id262179_2023_01_22_03_049740.jpg                  NaN   \n",
       "138  quantex_at_home_id268898_2022_11_30_01_007980.jpg                 0.66   \n",
       "139  quantex_at_home_id263293_2022_08_29_01_044470.jpg                 0.93   \n",
       "140  quantex_at_home_id262758_2023_07_10_01_027450.jpg                 0.78   \n",
       "141  quantex_at_home_id262565_2022_05_08_01_053850.jpg                 0.59   \n",
       "142  quantex_at_home_id260176_2022_11_06_02_031500.jpg                  NaN   \n",
       "143  quantex_at_home_id258239_2021_02_15_01_040920.jpg                  NaN   \n",
       "144  quantex_at_home_id263207_2024_12_12_04_007130.jpg    0.45,1.0,1.0,0.38   \n",
       "145  quantex_at_home_id258239_2021_02_15_01_005870.jpg                  1.0   \n",
       "146  quantex_at_home_id266971_2024_09_03_01_021280.jpg                  1.0   \n",
       "147  quantex_at_home_id261609_2022_04_01_06_046500.jpg                  1.0   \n",
       "148  quantex_at_home_id260176_2022_11_06_02_029380.jpg                 0.17   \n",
       "149  quantex_at_home_id258239_2021_02_15_01_038750.jpg             0.43,1.0   \n",
       "150  quantex_at_home_id257609_2022_11_09_01_018600.jpg                 0.67   \n",
       "151  quantex_at_home_id266971_2024_09_03_01_037910.jpg              1.0,1.0   \n",
       "152  quantex_at_home_id262758_2023_07_10_01_042810.jpg                 0.82   \n",
       "153  quantex_at_home_id263293_2022_08_29_01_029450.jpg                 0.88   \n",
       "154  quantex_at_home_id268898_2022_11_30_01_043160.jpg                  1.0   \n",
       "155  quantex_at_home_id262726_2023_04_20_01_037740.jpg                 0.57   \n",
       "156  quantex_at_home_id260120_2023_09_04_02_001440.jpg                 0.71   \n",
       "157  quantex_at_home_id263293_2022_08_29_01_018450.jpg   0.66,1.0,0.73,0.82   \n",
       "158  quantex_at_home_id262758_2023_07_10_01_033980.jpg                 0.68   \n",
       "159  quantex_at_home_id260772_2022_10_02_01_043450.jpg                 0.56   \n",
       "160  quantex_at_home_id263293_2022_08_29_01_043620.jpg             0.9,0.82   \n",
       "161  quantex_at_home_id273855_2023_06_04_02_016450.jpg                 0.54   \n",
       "162  quantex_at_home_id260123_2023_08_31_01_018910.jpg                 0.64   \n",
       "163  quantex_at_home_id267863_2022_08_23_01_017830.jpg                  0.9   \n",
       "164  quantex_at_home_id260176_2022_11_06_02_010930.jpg                  NaN   \n",
       "165  quantex_at_home_id261609_2022_04_15_02_011580.jpg             0.74,0.7   \n",
       "166  quantex_at_home_id263293_2022_08_29_01_015430.jpg             0.6,0.63   \n",
       "167  quantex_at_home_id263207_2024_12_12_02_007910.jpg                  1.0   \n",
       "168  quantex_at_home_id262758_2023_07_10_01_021940.jpg                 0.77   \n",
       "169  quantex_at_home_id263293_2022_08_29_01_029760.jpg                 0.97   \n",
       "170  quantex_at_home_id263284_2023_06_25_06_002850.jpg                 0.09   \n",
       "171  quantex_at_home_id264356_2023_07_05_04_034320.jpg                  NaN   \n",
       "\n",
       "    age_group  \n",
       "0       adult  \n",
       "1       adult  \n",
       "2       adult  \n",
       "3       adult  \n",
       "4       adult  \n",
       "5       adult  \n",
       "6       adult  \n",
       "7       adult  \n",
       "8       adult  \n",
       "9       adult  \n",
       "10      adult  \n",
       "11      adult  \n",
       "12      adult  \n",
       "13      adult  \n",
       "14      adult  \n",
       "15      adult  \n",
       "16      adult  \n",
       "17      adult  \n",
       "18      adult  \n",
       "19      adult  \n",
       "20      adult  \n",
       "21      adult  \n",
       "22      adult  \n",
       "23      adult  \n",
       "24      adult  \n",
       "25      adult  \n",
       "26      adult  \n",
       "27      adult  \n",
       "28      adult  \n",
       "30      adult  \n",
       "31      adult  \n",
       "32      adult  \n",
       "33      adult  \n",
       "34      adult  \n",
       "35      adult  \n",
       "36      adult  \n",
       "37      adult  \n",
       "38      adult  \n",
       "39      adult  \n",
       "40      adult  \n",
       "41      adult  \n",
       "42      adult  \n",
       "43      adult  \n",
       "44      adult  \n",
       "45      adult  \n",
       "46      adult  \n",
       "47      adult  \n",
       "48      adult  \n",
       "49      adult  \n",
       "50      adult  \n",
       "51      adult  \n",
       "52      adult  \n",
       "53      adult  \n",
       "54      adult  \n",
       "55      adult  \n",
       "56      adult  \n",
       "57      adult  \n",
       "58      adult  \n",
       "59      adult  \n",
       "60      adult  \n",
       "61      adult  \n",
       "62      adult  \n",
       "63      adult  \n",
       "64      adult  \n",
       "65      adult  \n",
       "66      adult  \n",
       "67      adult  \n",
       "68      adult  \n",
       "69      adult  \n",
       "70      adult  \n",
       "71      adult  \n",
       "72      adult  \n",
       "73      adult  \n",
       "74      adult  \n",
       "75      adult  \n",
       "76      adult  \n",
       "77      adult  \n",
       "78      child  \n",
       "79      child  \n",
       "80      child  \n",
       "81      child  \n",
       "82      child  \n",
       "83      child  \n",
       "84      child  \n",
       "85      child  \n",
       "86      child  \n",
       "87      child  \n",
       "88      child  \n",
       "89      child  \n",
       "90      child  \n",
       "91      child  \n",
       "92      child  \n",
       "93      child  \n",
       "94      child  \n",
       "95      child  \n",
       "96      child  \n",
       "97      child  \n",
       "98      child  \n",
       "99      child  \n",
       "100     child  \n",
       "101     child  \n",
       "102     child  \n",
       "103     child  \n",
       "104     child  \n",
       "105     child  \n",
       "106     child  \n",
       "107     child  \n",
       "108     child  \n",
       "109     child  \n",
       "110     child  \n",
       "111     child  \n",
       "112     child  \n",
       "113     child  \n",
       "114     child  \n",
       "115     child  \n",
       "116     child  \n",
       "117     child  \n",
       "118     child  \n",
       "119     child  \n",
       "120     child  \n",
       "121     child  \n",
       "122     child  \n",
       "123     child  \n",
       "124     child  \n",
       "125     child  \n",
       "126     child  \n",
       "127     child  \n",
       "128     child  \n",
       "129     child  \n",
       "130     child  \n",
       "131     child  \n",
       "132     child  \n",
       "133     child  \n",
       "134     child  \n",
       "135     child  \n",
       "136     child  \n",
       "137     child  \n",
       "138     child  \n",
       "139     child  \n",
       "140     child  \n",
       "141     child  \n",
       "142     child  \n",
       "143     child  \n",
       "144     child  \n",
       "145     child  \n",
       "146     child  \n",
       "147     child  \n",
       "148     child  \n",
       "149     child  \n",
       "150     child  \n",
       "151     child  \n",
       "152     child  \n",
       "153     child  \n",
       "154     adult  \n",
       "155     adult  \n",
       "156     adult  \n",
       "157     adult  \n",
       "158     adult  \n",
       "159     adult  \n",
       "160     adult  \n",
       "161     adult  \n",
       "162     adult  \n",
       "163     adult  \n",
       "164     adult  \n",
       "165     adult  \n",
       "166     adult  \n",
       "167     adult  \n",
       "168     adult  \n",
       "169     adult  \n",
       "170     adult  \n",
       "171     adult  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 72.6ms\n",
      "Speed: 10.0ms preprocess, 72.6ms inference, 192.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 16.5ms\n",
      "Speed: 27.9ms preprocess, 16.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/171 [00:02<06:38,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 faces, 17.7ms\n",
      "Speed: 3.6ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.84, child_face 0.16, 19.2ms\n",
      "Speed: 24.7ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.98, child_face 0.02, 19.2ms\n",
      "Speed: 14.4ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/171 [00:02<03:02,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 13.3ms\n",
      "Speed: 2.3ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.73, child_face 0.27, 19.2ms\n",
      "Speed: 13.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/171 [00:02<01:47,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 16.1ms\n",
      "Speed: 4.9ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.59, adult_face 0.41, 19.2ms\n",
      "Speed: 16.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 13.2ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 4/171 [00:02<01:16,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 3 faces, 1 child body parts, 11.1ms\n",
      "Speed: 1.8ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.69, child_face 0.31, 19.1ms\n",
      "Speed: 12.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.82, child_face 0.18, 19.1ms\n",
      "Speed: 13.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.91, child_face 0.09, 19.1ms\n",
      "Speed: 12.1ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 5/171 [00:03<01:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 13.5ms\n",
      "Speed: 2.3ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.90, child_face 0.10, 19.2ms\n",
      "Speed: 13.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.50, adult_face 0.50, 19.2ms\n",
      "Speed: 12.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 6/171 [00:03<00:49,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 4 faces, 13.3ms\n",
      "Speed: 2.3ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 12.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 12.3ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.83, child_face 0.17, 19.2ms\n",
      "Speed: 12.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.73, child_face 0.27, 19.2ms\n",
      "Speed: 12.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 7/171 [00:03<00:44,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 14.1ms\n",
      "Speed: 1.8ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.76, child_face 0.24, 19.2ms\n",
      "Speed: 12.3ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 8/171 [00:03<00:37,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.73, adult_face 0.27, 19.2ms\n",
      "Speed: 11.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 9/171 [00:03<00:31,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 13.4ms\n",
      "Speed: 2.3ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.71, child_face 0.29, 19.2ms\n",
      "Speed: 12.7ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 10/171 [00:03<00:28,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 13.4ms\n",
      "Speed: 2.3ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.97, child_face 0.03, 19.2ms\n",
      "Speed: 14.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 11/171 [00:03<00:25,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.1ms\n",
      "Speed: 14.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.99, child_face 0.01, 19.1ms\n",
      "Speed: 12.9ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 12/171 [00:04<00:26,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 3 faces, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.57, adult_face 0.43, 19.2ms\n",
      "Speed: 13.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.96, child_face 0.04, 19.2ms\n",
      "Speed: 13.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.74, child_face 0.26, 19.1ms\n",
      "Speed: 12.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 13/171 [00:04<00:26,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 12.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 14/171 [00:04<00:24,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 2 faces, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 12.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 14.0ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 15/171 [00:04<00:24,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 13.5ms\n",
      "Speed: 2.3ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.1ms\n",
      "Speed: 14.4ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.98, child_face 0.02, 19.2ms\n",
      "Speed: 13.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 16/171 [00:04<00:25,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 17/171 [00:04<00:22,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 14.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 18/171 [00:04<00:20,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.97, child_face 0.03, 19.1ms\n",
      "Speed: 12.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 19/171 [00:05<00:20,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.89, child_face 0.11, 19.1ms\n",
      "Speed: 14.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 20/171 [00:05<00:19,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 11.3ms\n",
      "Speed: 2.2ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.66, child_face 0.34, 19.1ms\n",
      "Speed: 15.0ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.66, child_face 0.34, 19.2ms\n",
      "Speed: 12.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 21/171 [00:05<00:18,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.1ms\n",
      "Speed: 13.1ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 22/171 [00:05<00:18,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 13.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 23/171 [00:05<00:17,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.57, child_face 0.43, 19.1ms\n",
      "Speed: 11.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 24/171 [00:05<00:17,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 13.7ms\n",
      "Speed: 1.8ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.70, adult_face 0.30, 19.2ms\n",
      "Speed: 13.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 25/171 [00:05<00:16,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 3 faces, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 14.5ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 13.2ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 13.4ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 26/171 [00:05<00:20,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 12.4ms\n",
      "Speed: 2.3ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.61, child_face 0.39, 19.2ms\n",
      "Speed: 11.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 27/171 [00:06<00:19,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 28/171 [00:06<00:18,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.3ms\n",
      "Speed: 3.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.51, child_face 0.49, 19.2ms\n",
      "Speed: 13.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 29/171 [00:07<01:11,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.58, adult_face 0.42, 19.1ms\n",
      "Speed: 14.5ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 30/171 [00:07<00:54,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 4 faces, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.77, adult_face 0.23, 19.2ms\n",
      "Speed: 15.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.59, adult_face 0.41, 19.2ms\n",
      "Speed: 14.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.73, child_face 0.27, 19.2ms\n",
      "Speed: 12.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.56, adult_face 0.44, 19.2ms\n",
      "Speed: 16.4ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 31/171 [00:07<00:46,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 12.0ms\n",
      "Speed: 1.9ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.58, adult_face 0.42, 18.0ms\n",
      "Speed: 12.4ms preprocess, 18.0ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 32/171 [00:08<00:36,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.6ms\n",
      "Speed: 15.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 33/171 [00:08<00:31,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 child body parts, 13.0ms\n",
      "Speed: 2.7ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 6.1ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 35/171 [00:08<00:21,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 child body parts, 12.8ms\n",
      "Speed: 2.5ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 36/171 [00:08<00:19,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 12.4ms\n",
      "Speed: 2.1ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.92, child_face 0.08, 17.6ms\n",
      "Speed: 13.1ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 37/171 [00:08<00:18,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.5ms\n",
      "Speed: 13.6ms preprocess, 17.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.82, adult_face 0.18, 17.6ms\n",
      "Speed: 12.3ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 38/171 [00:08<00:19,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.56, adult_face 0.44, 17.6ms\n",
      "Speed: 14.0ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.80, adult_face 0.20, 17.6ms\n",
      "Speed: 17.0ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.6ms\n",
      "Speed: 12.2ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 40/171 [00:08<00:17,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.77, child_face 0.23, 17.6ms\n",
      "Speed: 15.4ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 41/171 [00:09<00:17,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.6ms\n",
      "Speed: 14.4ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 42/171 [00:09<00:16,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.6ms\n",
      "Speed: 13.1ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 43/171 [00:09<00:15,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.96, child_face 0.04, 17.6ms\n",
      "Speed: 11.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.73, adult_face 0.27, 17.6ms\n",
      "Speed: 12.2ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 44/171 [00:09<00:16,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.59, adult_face 0.41, 17.6ms\n",
      "Speed: 15.3ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 13.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 45/171 [00:09<00:17,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 12.1ms\n",
      "Speed: 2.2ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.6ms\n",
      "Speed: 12.2ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.95, child_face 0.05, 17.6ms\n",
      "Speed: 11.9ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 46/171 [00:09<00:16,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.56, adult_face 0.44, 17.6ms\n",
      "Speed: 12.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.87, child_face 0.13, 17.2ms\n",
      "Speed: 12.0ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.92, adult_face 0.08, 17.2ms\n",
      "Speed: 14.1ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 48/171 [00:09<00:16,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.99, child_face 0.01, 17.3ms\n",
      "Speed: 13.1ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 49/171 [00:10<00:16,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.79, child_face 0.21, 17.2ms\n",
      "Speed: 11.7ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 50/171 [00:10<00:16,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 2 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.89, child_face 0.11, 17.3ms\n",
      "Speed: 12.1ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.80, child_face 0.20, 17.3ms\n",
      "Speed: 11.8ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 51/171 [00:10<00:15,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.80, child_face 0.20, 17.2ms\n",
      "Speed: 13.5ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 52/171 [00:10<00:15,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 1.7ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.54, child_face 0.46, 17.3ms\n",
      "Speed: 12.6ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 faces, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.3ms\n",
      "Speed: 14.0ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.69, child_face 0.31, 17.2ms\n",
      "Speed: 12.3ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.52, adult_face 0.48, 17.2ms\n",
      "Speed: 12.5ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 54/171 [00:10<00:14,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.2ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.57, adult_face 0.43, 17.2ms\n",
      "Speed: 19.5ms preprocess, 17.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 56/171 [00:10<00:13,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.93, child_face 0.07, 17.3ms\n",
      "Speed: 13.6ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 57/171 [00:11<00:13,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.71, child_face 0.29, 17.3ms\n",
      "Speed: 13.3ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 58/171 [00:11<00:13,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 4 faces, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.94, adult_face 0.06, 17.3ms\n",
      "Speed: 13.0ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.3ms\n",
      "Speed: 14.1ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.3ms\n",
      "Speed: 16.6ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.95, adult_face 0.05, 17.3ms\n",
      "Speed: 14.6ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 59/171 [00:11<00:15,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 faces, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.96, child_face 0.04, 18.8ms\n",
      "Speed: 12.9ms preprocess, 18.8ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.95, child_face 0.05, 19.2ms\n",
      "Speed: 12.4ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 61/171 [00:11<00:13,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 1.8ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.84, child_face 0.16, 18.7ms\n",
      "Speed: 11.9ms preprocess, 18.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 62/171 [00:11<00:13,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.9ms\n",
      "Speed: 2.3ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.3ms\n",
      "Speed: 18.0ms preprocess, 19.3ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 63/171 [00:11<00:13,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.58, child_face 0.42, 19.4ms\n",
      "Speed: 12.7ms preprocess, 19.4ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 64/171 [00:11<00:12,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 13.2ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 65/171 [00:12<00:12,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.74, child_face 0.26, 19.2ms\n",
      "Speed: 12.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 66/171 [00:12<00:12,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.58, adult_face 0.42, 19.2ms\n",
      "Speed: 14.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.75, adult_face 0.25, 19.2ms\n",
      "Speed: 12.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 67/171 [00:12<00:13,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.73, child_face 0.27, 19.2ms\n",
      "Speed: 13.3ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 68/171 [00:12<00:12,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.80, child_face 0.20, 19.1ms\n",
      "Speed: 13.5ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 69/171 [00:12<00:12,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.58, adult_face 0.42, 19.2ms\n",
      "Speed: 12.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 70/171 [00:12<00:11,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.92, child_face 0.08, 19.2ms\n",
      "Speed: 13.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 13.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 72/171 [00:12<00:11,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 18.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 73/171 [00:13<00:11,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.83, child_face 0.17, 19.2ms\n",
      "Speed: 12.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 face, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.99, child_face 0.01, 19.2ms\n",
      "Speed: 20.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 75/171 [00:13<00:10,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 2 child body partss, 11.1ms\n",
      "Speed: 1.8ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.96, child_face 0.04, 19.2ms\n",
      "Speed: 11.7ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 76/171 [00:13<00:10,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 13.6ms\n",
      "Speed: 1.7ms preprocess, 13.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.79, adult_face 0.21, 19.2ms\n",
      "Speed: 13.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.68, child_face 0.32, 19.2ms\n",
      "Speed: 12.3ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 77/171 [00:13<00:10,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 2 child body partss, 11.2ms\n",
      "Speed: 2.2ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 19.2ms\n",
      "Speed: 12.3ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 78/171 [00:13<00:10,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.98, adult_face 0.02, 19.2ms\n",
      "Speed: 14.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 79/171 [00:13<00:11,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.75, adult_face 0.25, 19.1ms\n",
      "Speed: 11.9ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 80/171 [00:13<00:10,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 child body parts, 16.8ms\n",
      "Speed: 2.3ms preprocess, 16.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 81/171 [00:13<00:10,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.96, child_face 0.04, 19.1ms\n",
      "Speed: 14.1ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 82/171 [00:14<00:10,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 15.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 83/171 [00:14<00:10,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.66, adult_face 0.34, 19.1ms\n",
      "Speed: 29.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 85/171 [00:14<00:09,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 3 faces, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.97, adult_face 0.03, 19.2ms\n",
      "Speed: 15.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 17.2ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.1ms\n",
      "Speed: 12.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 86/171 [00:14<00:10,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 17.6ms\n",
      "Speed: 13.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 87/171 [00:14<00:10,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 1.9ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.63, child_face 0.37, 17.6ms\n",
      "Speed: 12.7ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 face, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 14.5ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 89/171 [00:14<00:09,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.5ms\n",
      "Speed: 15.0ms preprocess, 17.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 1.8ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 12.1ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 91/171 [00:15<00:08,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 15.0ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.6ms\n",
      "Speed: 13.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 92/171 [00:15<00:08,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 17.6ms\n",
      "Speed: 13.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 93/171 [00:15<00:08,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.60, adult_face 0.40, 17.6ms\n",
      "Speed: 12.3ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 94/171 [00:15<00:08,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 14.3ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 95/171 [00:15<00:08,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 14.1ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.57, child_face 0.43, 17.6ms\n",
      "Speed: 18.3ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 96/171 [00:15<00:09,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.91, child_face 0.09, 17.6ms\n",
      "Speed: 14.1ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 97/171 [00:15<00:09,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 16.1ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 faces, 10.8ms\n",
      "Speed: 2.2ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 15.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.97, child_face 0.03, 17.6ms\n",
      "Speed: 14.9ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 99/171 [00:16<00:08,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 1.8ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 12.6ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 100/171 [00:16<00:08,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 child body parts, 14.3ms\n",
      "Speed: 2.3ms preprocess, 14.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.6ms\n",
      "Speed: 12.2ms preprocess, 17.6ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 102/171 [00:16<00:07,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 1 face, 11.1ms\n",
      "Speed: 3.0ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 17.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 103/171 [00:16<00:08,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 1.7ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.78, adult_face 0.22, 19.2ms\n",
      "Speed: 15.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 104/171 [00:16<00:07,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.2ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.71, child_face 0.29, 19.1ms\n",
      "Speed: 16.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 1.9ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 106/171 [00:16<00:06, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.98, adult_face 0.02, 19.1ms\n",
      "Speed: 18.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.69, adult_face 0.31, 19.2ms\n",
      "Speed: 14.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.85, adult_face 0.15, 19.2ms\n",
      "Speed: 15.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 108/171 [00:16<00:06,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.94, adult_face 0.06, 19.1ms\n",
      "Speed: 11.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 110/171 [00:17<00:05, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3 faces, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.91, adult_face 0.09, 19.2ms\n",
      "Speed: 15.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.78, adult_face 0.22, 19.1ms\n",
      "Speed: 13.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.50, child_face 0.50, 19.2ms\n",
      "Speed: 13.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 112/171 [00:17<00:06,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 14.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 113/171 [00:17<00:06,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.96, adult_face 0.04, 19.1ms\n",
      "Speed: 14.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.67, adult_face 0.33, 19.1ms\n",
      "Speed: 12.3ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 115/171 [00:17<00:05,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 12.9ms\n",
      "Speed: 1.8ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 18.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 2 child body partss, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 14.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 117/171 [00:17<00:05,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.86, adult_face 0.14, 19.1ms\n",
      "Speed: 13.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.73, child_face 0.27, 19.1ms\n",
      "Speed: 13.0ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 118/171 [00:18<00:05,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 119/171 [00:18<00:05,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.86, child_face 0.14, 19.2ms\n",
      "Speed: 34.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 121/171 [00:18<00:05,  9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 child body parts, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.55, adult_face 0.45, 19.1ms\n",
      "Speed: 20.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.69, child_face 0.31, 19.1ms\n",
      "Speed: 13.1ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 123/171 [00:18<00:05,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 19.2ms\n",
      "Speed: 15.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 124/171 [00:18<00:05,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 14.2ms\n",
      "Speed: 3.0ms preprocess, 14.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 125/171 [00:18<00:05,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 2 child body partss, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 16.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 126/171 [00:18<00:05,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.79, child_face 0.21, 19.2ms\n",
      "Speed: 12.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 13.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 127/171 [00:19<00:05,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 17.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 128/171 [00:19<00:05,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.8ms\n",
      "Speed: 1.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.91, adult_face 0.09, 19.2ms\n",
      "Speed: 14.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 129/171 [00:19<00:05,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 11.7ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.1ms\n",
      "Speed: 1.7ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.54, child_face 0.46, 19.2ms\n",
      "Speed: 13.4ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.84, child_face 0.16, 19.2ms\n",
      "Speed: 12.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 131/171 [00:19<00:04,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 13.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 132/171 [00:19<00:04,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 2 child body partss, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.54, child_face 0.46, 19.2ms\n",
      "Speed: 12.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.58, adult_face 0.42, 19.2ms\n",
      "Speed: 11.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 133/171 [00:19<00:05,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.83, adult_face 0.17, 19.2ms\n",
      "Speed: 22.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 134/171 [00:19<00:04,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 child body parts, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 faces, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.53, child_face 0.47, 19.2ms\n",
      "Speed: 12.2ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.82, child_face 0.18, 19.1ms\n",
      "Speed: 12.0ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 136/171 [00:20<00:04,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 14.2ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 138/171 [00:20<00:03,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 16.0ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 139/171 [00:20<00:03,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 12.4ms\n",
      "Speed: 2.2ms preprocess, 12.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.97, adult_face 0.03, 19.2ms\n",
      "Speed: 15.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 140/171 [00:20<00:03,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 2 child body partss, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 19.1ms\n",
      "Speed: 13.9ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 141/171 [00:20<00:03,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 child body parts, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 143/171 [00:20<00:02, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 4 faces, 2 child body partss, 11.1ms\n",
      "Speed: 1.7ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.85, adult_face 0.15, 19.2ms\n",
      "Speed: 12.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 19.2ms\n",
      "Speed: 13.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.91, adult_face 0.09, 19.1ms\n",
      "Speed: 15.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.91, child_face 0.09, 19.1ms\n",
      "Speed: 16.5ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 face, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.1ms\n",
      "Speed: 13.9ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 145/171 [00:21<00:03,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 face, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.69, child_face 0.31, 19.1ms\n",
      "Speed: 14.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.98, adult_face 0.02, 19.2ms\n",
      "Speed: 17.5ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 147/171 [00:21<00:02,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.86, adult_face 0.14, 19.1ms\n",
      "Speed: 12.5ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.73, child_face 0.27, 19.1ms\n",
      "Speed: 12.5ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.58, child_face 0.42, 19.1ms\n",
      "Speed: 12.0ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 149/171 [00:21<00:02,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.81, child_face 0.19, 19.1ms\n",
      "Speed: 13.2ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 150/171 [00:21<00:02,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 2 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.99, child_face 0.01, 19.1ms\n",
      "Speed: 12.6ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.70, adult_face 0.30, 19.2ms\n",
      "Speed: 23.2ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 151/171 [00:21<00:02,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 15.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 19.2ms\n",
      "Speed: 15.8ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 153/171 [00:22<00:02,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.60, adult_face 0.40, 19.2ms\n",
      "Speed: 33.9ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 154/171 [00:22<00:02,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 19.2ms\n",
      "Speed: 13.0ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 155/171 [00:22<00:01,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 face, 1 child body parts, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.99, child_face 0.01, 19.2ms\n",
      "Speed: 12.6ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 156/171 [00:22<00:01,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 4 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 19.2ms\n",
      "Speed: 13.1ms preprocess, 19.2ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.89, adult_face 0.11, 19.1ms\n",
      "Speed: 14.4ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 0.71, adult_face 0.29, 19.1ms\n",
      "Speed: 13.3ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.96, child_face 0.04, 19.1ms\n",
      "Speed: 14.3ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 157/171 [00:22<00:02,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.66, adult_face 0.34, 19.1ms\n",
      "Speed: 12.8ms preprocess, 19.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 158/171 [00:22<00:01,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 12.8ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 159/171 [00:22<00:01,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.3ms\n",
      "Speed: 2.2ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.96, adult_face 0.04, 17.7ms\n",
      "Speed: 14.2ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 14.9ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 160/171 [00:23<00:01,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.99, adult_face 0.01, 17.7ms\n",
      "Speed: 12.9ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 161/171 [00:23<00:01,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 13.6ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 13.7ms\n",
      "Speed: 2.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.91, adult_face 0.09, 17.7ms\n",
      "Speed: 15.0ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 163/171 [00:23<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 1.8ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 faces, 2 child body partss, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 13.2ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 12.8ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 165/171 [00:23<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 2 faces, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 1.00, child_face 0.00, 17.7ms\n",
      "Speed: 12.7ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 13.2ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 166/171 [00:23<00:00,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.87, adult_face 0.13, 17.7ms\n",
      "Speed: 14.1ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 face, 1 child body parts, 12.5ms\n",
      "Speed: 2.3ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 0.78, adult_face 0.22, 17.7ms\n",
      "Speed: 13.8ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 168/171 [00:23<00:00,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 face, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 child_face 1.00, adult_face 0.00, 17.7ms\n",
      "Speed: 15.9ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 face, 11.3ms\n",
      "Speed: 1.8ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 adult_face 0.87, child_face 0.13, 17.7ms\n",
      "Speed: 11.6ms preprocess, 17.7ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 170/171 [00:24<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 10.7ms\n",
      "Speed: 2.2ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 171/171 [00:24<00:00,  7.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from detection_pipeline.proximity_utils import ProximityCalculator\n",
    "from ultralytics import YOLO\n",
    "from constants import DetectionPaths, ClassificationPaths\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "def process_image(image_path, detector, age_classifier, prox_calculator):\n",
    "    \"\"\"Process a single image and return proximity values\"\"\"\n",
    "    full_path = os.path.join(\"/home/nele_pauline_suffo/ProcessedData/quantex_videos_processed\", \n",
    "                           os.path.basename(image_path).rsplit('_', 1)[0], \n",
    "                           image_path)\n",
    "    \n",
    "    img = cv2.imread(full_path)\n",
    "    if img is None:\n",
    "        logging.warning(f\"Could not load image at {full_path}\")\n",
    "        return None\n",
    "    \n",
    "    detections = detector(img)[0]\n",
    "    proximity_values = []\n",
    "    \n",
    "    for box in detections.boxes:\n",
    "        if int(box.cls) != 1:  # Skip non-face classes\n",
    "            continue\n",
    "            \n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        face_roi = img[y1:y2, x1:x2]\n",
    "        \n",
    "        age_result = age_classifier(face_roi)[0]\n",
    "        is_child = int(age_result.probs.top1) == 1\n",
    "        \n",
    "        proximity = prox_calculator.calculate((x1, y1, x2, y2), is_child)\n",
    "        proximity_values.append(str(round(proximity, 2)))\n",
    "    \n",
    "    return \",\".join(proximity_values) if proximity_values else None\n",
    "\n",
    "def update_dataframe_with_proximity(df):\n",
    "    \"\"\"Process all images in DataFrame and update proximity_heuristic column\"\"\"\n",
    "    # Initialize models and calculator once\n",
    "    detector = YOLO(DetectionPaths.person_face_trained_weights_path)\n",
    "    age_classifier = YOLO(ClassificationPaths.face_trained_weights_path)\n",
    "    prox_calculator = ProximityCalculator()\n",
    "    \n",
    "    # Process each row with progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_path = row['frame']\n",
    "        proximity_values = process_image(image_path, detector, age_classifier, prox_calculator)\n",
    "        \n",
    "        if proximity_values is not None:\n",
    "            df.at[idx, 'proximity_heuristic'] = proximity_values\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Process and update DataFrame\n",
    "    updated_df = update_dataframe_with_proximity(proximity_df_without_heuristic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>proximity_true</th>\n",
       "      <th>proximity_heuristic</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quantex_at_home_id260123_2023_09_06_01_013290.jpg</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantex_at_home_id262726_2023_03_26_01_021500.jpg</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.29,0.33</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantex_at_home_id260275_2022_04_16_01_011640.jpg</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.37</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quantex_at_home_id264351_2024_11_23_03_016520.jpg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.0,0.6</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_04_050240.jpg</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.59,0.45,0.51</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>quantex_at_home_id263207_2024_12_12_02_007910.jpg</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>quantex_at_home_id262758_2023_07_10_01_021940.jpg</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>quantex_at_home_id263293_2022_08_29_01_029760.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>quantex_at_home_id263284_2023_06_25_06_002850.jpg</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>quantex_at_home_id264356_2023_07_05_04_034320.jpg</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 frame  proximity_true  \\\n",
       "0    quantex_at_home_id260123_2023_09_06_01_013290.jpg            0.87   \n",
       "1    quantex_at_home_id262726_2023_03_26_01_021500.jpg            0.81   \n",
       "2    quantex_at_home_id260275_2022_04_16_01_011640.jpg            0.21   \n",
       "3    quantex_at_home_id264351_2024_11_23_03_016520.jpg            0.73   \n",
       "4    quantex_at_home_id263284_2023_06_25_04_050240.jpg            0.91   \n",
       "..                                                 ...             ...   \n",
       "167  quantex_at_home_id263207_2024_12_12_02_007910.jpg            0.61   \n",
       "168  quantex_at_home_id262758_2023_07_10_01_021940.jpg            0.89   \n",
       "169  quantex_at_home_id263293_2022_08_29_01_029760.jpg            1.00   \n",
       "170  quantex_at_home_id263284_2023_06_25_06_002850.jpg            0.16   \n",
       "171  quantex_at_home_id264356_2023_07_05_04_034320.jpg            0.79   \n",
       "\n",
       "    proximity_heuristic age_group  \n",
       "0                   1.0     adult  \n",
       "1             0.29,0.33     adult  \n",
       "2                  0.37     adult  \n",
       "3               1.0,0.6     adult  \n",
       "4        0.59,0.45,0.51     adult  \n",
       "..                  ...       ...  \n",
       "167                 1.0     adult  \n",
       "168                0.77     adult  \n",
       "169                0.97     adult  \n",
       "170                0.09     adult  \n",
       "171                 NaN     adult  \n",
       "\n",
       "[171 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
