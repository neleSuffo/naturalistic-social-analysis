{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VTC Evaluation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.detection import DetectionPrecisionRecallFMeasure, DetectionErrorRate\n",
    "from typing import List\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from src.constants import VTCPaths\n",
    "\n",
    "def plot_annotations_vs_predictions(audio_file_name: str, duration_off: float = 0.0):\n",
    "    \"\"\"\n",
    "    This function plots the annotations and predictions for a given audio file.\n",
    "    The annotations are plotted in green and the predictions are plotted in blue.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_file_name : str\n",
    "        The name of the audio file to plot the annotations and predictions for.\n",
    "    duration_off : float\n",
    "        The duration an utterance has to be off to be considered actually off by the model.\n",
    "    \"\"\"\n",
    "    gt_df = pd.read_pickle(VTCPaths.childlens_gt_df_file_path)\n",
    "    if duration_off == 0.1:\n",
    "        predictions_df = pd.read_pickle(VTCPaths.childlens_df_file_path_01)\n",
    "    elif duration_off == 2.0:\n",
    "        predictions_df = pd.read_pickle(VTCPaths.childlens_df_file_path_20)\n",
    "\n",
    "    # Strip the audio_file_name by removing the '_16kHz' extension and everything afterwards\n",
    "    predictions_df['audio_file_name'] = predictions_df['audio_file_name'].str.split('_').str[0]\n",
    "\n",
    "    example_annotation = gt_df[gt_df['audio_file_name'] == audio_file_name]\n",
    "    example_prediction = predictions_df[predictions_df['audio_file_name'] == audio_file_name]\n",
    "\n",
    "    # Set up the figure and axis\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ground_truth_fem = example_annotation[example_annotation['Voice_type'] == 'FEM']\n",
    "    ground_truth_mal = example_annotation[example_annotation['Voice_type'] == 'MAL']\n",
    "    ground_truth_kchi = example_annotation[example_annotation['Voice_type'] == 'KCHI']\n",
    "    ground_truth_chi = example_annotation[example_annotation['Voice_type'] == 'CHI']\n",
    "    ground_truth_speech = example_annotation[example_annotation['Voice_type'] == 'SPEECH']\n",
    "    predictions_fem = example_prediction[example_prediction['Voice_type'] == 'FEM']\n",
    "    predictions_mal = example_prediction[example_prediction['Voice_type'] == 'MAL']\n",
    "    predictions_kchi = example_prediction[example_prediction['Voice_type'] == 'KCHI']\n",
    "    predictions_chi = example_prediction[example_prediction['Voice_type'] == 'CHI']\n",
    "    predictions_speech = example_prediction[example_prediction['Voice_type'] == 'SPEECH']\n",
    "\n",
    "    # Define y-positions for the two bars\n",
    "    y_ground_truth_kchi = 9\n",
    "    y_predictions_kchi = 8\n",
    "\n",
    "    y_ground_truth_fem = 7\n",
    "    y_predictions_fem = 6\n",
    "\n",
    "    y_ground_truth_chi= 5\n",
    "    y_predictions_chi = 4 \n",
    "\n",
    "    y_ground_truth_mal = 3\n",
    "    y_predictions_mal = 2\n",
    "\n",
    "    y_ground_truth_speech = 1\n",
    "    y_predictions_speech = 0\n",
    "\n",
    "    # Plot predictions, excluding SPEECH\n",
    "    for idx, row in predictions_fem.iterrows():\n",
    "        ax.barh(y_predictions_fem, row['Utterance_Duration'], left=row['Utterance_Start'], color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in predictions_mal.iterrows():\n",
    "        ax.barh(y_predictions_mal, row['Utterance_Duration'], left=row['Utterance_Start'], color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in predictions_kchi.iterrows():\n",
    "        ax.barh(y_predictions_kchi, row['Utterance_Duration'], left=row['Utterance_Start'], color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in predictions_chi.iterrows():\n",
    "        ax.barh(y_predictions_chi, row['Utterance_Duration'], left=row['Utterance_Start'], color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in predictions_speech.iterrows():\n",
    "        ax.barh(y_predictions_speech, row['Utterance_Duration'], left=row['Utterance_Start'], color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in ground_truth_fem.iterrows():\n",
    "        ax.barh(y_ground_truth_fem, row['Utterance_Duration'], left=row['Utterance_Start'], color='green', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in ground_truth_mal.iterrows():\n",
    "        ax.barh(y_ground_truth_mal, row['Utterance_Duration'], left=row['Utterance_Start'], color='green', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in ground_truth_kchi.iterrows():\n",
    "        ax.barh(y_ground_truth_kchi, row['Utterance_Duration'], left=row['Utterance_Start'], color='green', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in ground_truth_chi.iterrows():\n",
    "        ax.barh(y_ground_truth_chi, row['Utterance_Duration'], left=row['Utterance_Start'], color='green', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for idx, row in ground_truth_speech.iterrows():\n",
    "        ax.barh(y_ground_truth_speech, row['Utterance_Duration'], left=row['Utterance_Start'], color='green', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # Adjust y-axis ticks and labels\n",
    "    ax.set_yticks([y_ground_truth_speech, y_predictions_speech, y_ground_truth_fem, y_predictions_fem, y_ground_truth_mal, y_predictions_mal, y_ground_truth_kchi, y_predictions_kchi, y_ground_truth_chi, y_predictions_chi])\n",
    "    ax.set_yticklabels([\"GT SPEECH\", \"Pred SPEECH\", \"GT FEM\", \"Pred FEM\", \"GT MAL\", \"Pred MAL\", \"GT KCHI\", \"Pred KCHI\", \"GT CHI\", \"Pred CHI\"])\n",
    "\n",
    "    # Add labels and titles\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Category\")\n",
    "    ax.set_title(\"Annotations vs Predictions\")\n",
    "    ax.set_xlim(0, 250)\n",
    "    \n",
    "def combine_pickles(folder_path: str, output_file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Combines all pickle files in a folder into a single DataFrame and saves it to a new pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the pickle files.\n",
    "    - output_path (str): Path to save the combined DataFrame as a pickle file.\n",
    "    \"\"\"\n",
    "    # List to store individual DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pkl'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                df = pickle.load(f)\n",
    "                dataframes.append(df)\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    output_path = os.path.join(folder_path, output_file_name)\n",
    "    combined_df.to_pickle(output_path)\n",
    "    \n",
    "def rttm_to_dataframe(rttm_file: Path, output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function reads the voice_type_classifier\n",
    "    output rttm file and returns its content as a pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rttm_file : path\n",
    "        the path to the RTTM file\n",
    "    output_path: path\n",
    "        the path to the output pkl file\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info(f\"Reading RTTM file from: {rttm_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            rttm_file,\n",
    "            sep=\" \",\n",
    "            names=[\n",
    "                \"Speaker\",\n",
    "                \"audio_file_name\",\n",
    "                \"audio_file_id\",\n",
    "                \"Utterance_Start\",\n",
    "                \"Utterance_Duration\",\n",
    "                \"NA_1\",\n",
    "                \"NA_2\",\n",
    "                \"Voice_type\",\n",
    "                \"NA_3\",\n",
    "                \"NA_4\",\n",
    "            ],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read RTTM file: {e}\")\n",
    "        raise\n",
    "    \n",
    "    logging.info(\"Successfully read RTTM file. Processing data...\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[\"Speaker\", \"audio_file_id\", \"NA_1\", \"NA_2\", \"NA_3\", \"NA_4\"])  # noqa: E501\n",
    "    df[\"Utterance_End\"] = df[\"Utterance_Start\"] + df[\"Utterance_Duration\"]\n",
    "    \n",
    "    logging.info(\"Data processing complete. Returning DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        df.to_pickle(output_path)\n",
    "        logging.info(f\"DataFrame successfully saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save DataFrame to file: {e}\")\n",
    "        raise\n",
    "\n",
    "def dataframe_to_annotation(df, label_column=\"Voice_type\"):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a pyannote.core.Annotation object.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'Utterance_Start', 'Utterance_End', and a label column.\n",
    "    - label_column (str): Column name for the labels (default: 'Voice_type').\n",
    "\n",
    "    Returns:\n",
    "    - Annotation: pyannote.core.Annotation object.\n",
    "    \"\"\"\n",
    "    annotation = Annotation()\n",
    "    for _, row in df.iterrows():\n",
    "        start = float(row[\"Utterance_Start\"])\n",
    "        end = float(row[\"Utterance_End\"])\n",
    "        label = row[label_column]\n",
    "        annotation[Segment(start, end)] = label\n",
    "    return annotation\n",
    "\n",
    "def compute_metrics(hypothesis_file_name: str,\n",
    "                    reference_df_path: Path=VTCPaths.childlens_gt_df_file_path,\n",
    "    ):\n",
    "    hypothesis_dir = Path('/home/nele_pauline_suffo/outputs/')\n",
    "    hypothesis_path = hypothesis_dir / hypothesis_file_name\n",
    "    # Load reference and hypothesis DataFrames\n",
    "    reference_df = pd.read_pickle(reference_df_path)\n",
    "    hypothesis_df = pd.read_pickle(hypothesis_path)\n",
    "\n",
    "    # Strip the audio_file_name by removing the '_16kHz' extension and everything afterwards\n",
    "    hypothesis_df['audio_file_name'] = hypothesis_df['audio_file_name'].str.split('_').str[0]\n",
    "    \n",
    "    # Get the list of audio files that have annotations\n",
    "    annotated_files = reference_df['audio_file_name'].unique()\n",
    "    # Filter the hypothesis DataFrame to only include these files\n",
    "    predictions = hypothesis_df[hypothesis_df['audio_file_name'].isin(annotated_files)]\n",
    "    \n",
    "    voice_types = ['KCHI', 'OCH', 'FEM', 'MAL', 'SPEECH']\n",
    "    class_metrics = {voice_type: {'precision': [], 'recall': [], 'f1_score': [], 'error_rate': []} for voice_type in voice_types}\n",
    "\n",
    "    # Loop over each video\n",
    "    for video in annotated_files:\n",
    "        # Filter DataFrames for the current video\n",
    "        reference_filtered = reference_df[reference_df['audio_file_name'] == video]\n",
    "        predictions_filtered = predictions[predictions['audio_file_name'] == video]\n",
    "\n",
    "        # Skip if there are no annotations for this video\n",
    "        if reference_filtered.empty:\n",
    "            print(f\"No annotations available for video: {video}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Now, compute metrics per class within this video\n",
    "        for voice_type in voice_types:\n",
    "            # Filter annotations for the current voice type\n",
    "            ref_class_filtered = reference_filtered[reference_filtered['Voice_type'] == voice_type]\n",
    "            hyp_class_filtered = predictions_filtered[predictions_filtered['Voice_type'] == voice_type]\n",
    "\n",
    "            # Convert to pyannote Annotations\n",
    "            reference_class_annotation = dataframe_to_annotation(ref_class_filtered)\n",
    "            hypothesis_class_annotation = dataframe_to_annotation(hyp_class_filtered)\n",
    "\n",
    "            # Initialize class-specific metrics\n",
    "            detection_f1_class = DetectionPrecisionRecallFMeasure(collar=0, skip_overlap=False)\n",
    "            error_rate_metric_class = DetectionErrorRate(collar=0, skip_overlap=False)\n",
    "\n",
    "            # Compute the detection metrics for this class\n",
    "            detection_f1_class(reference_class_annotation, hypothesis_class_annotation)\n",
    "            error_rate_class = error_rate_metric_class(reference_class_annotation, hypothesis_class_annotation)\n",
    "\n",
    "            # Retrieve precision, recall, and F1 score\n",
    "            precision_class, recall_class, f1_score_class = detection_f1_class.compute_metrics()\n",
    "\n",
    "            # Store the per-class results\n",
    "            class_metrics[voice_type]['precision'].append(precision_class)\n",
    "            class_metrics[voice_type]['recall'].append(recall_class)\n",
    "            class_metrics[voice_type]['f1_score'].append(f1_score_class)\n",
    "            class_metrics[voice_type]['error_rate'].append(error_rate_class)\n",
    "\n",
    "    print(\"\\nAveraged Metrics Per Class Over All Videos:\")\n",
    "    avg_f1_scores_per_class = []  # List to store avg F1 scores per class\n",
    "\n",
    "    for voice_type in voice_types:\n",
    "        precisions = class_metrics[voice_type]['precision']\n",
    "        recalls = class_metrics[voice_type]['recall']\n",
    "        f1_scores = class_metrics[voice_type]['f1_score']\n",
    "        error_rates = class_metrics[voice_type]['error_rate']\n",
    "\n",
    "        if precisions:\n",
    "            avg_precision = sum(precisions) / len(precisions)\n",
    "            avg_recall = sum(recalls) / len(recalls)\n",
    "            avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "            avg_error_rate = sum(error_rates) / len(error_rates)\n",
    "        else:\n",
    "            avg_precision = avg_recall = avg_f1 = avg_error_rate = 0\n",
    "\n",
    "        avg_f1_scores_per_class.append(avg_f1)\n",
    "\n",
    "        print(f\"Class '{voice_type.upper()}':\")\n",
    "        print(f\"  Precision: {avg_precision:.3f}\")\n",
    "        print(f\"  Recall: {avg_recall:.3f}\")\n",
    "        print(f\"  F1 Score: {avg_f1:.3f}\")\n",
    "        print(f\"  Error Rate: {avg_error_rate:.3f}\\n\")\n",
    "        \n",
    "    # Calculate the final F1 score over all classes\n",
    "    if avg_f1_scores_per_class:\n",
    "        final_f1_score = sum(avg_f1_scores_per_class) / len(avg_f1_scores_per_class)\n",
    "    else:\n",
    "        final_f1_score = 0\n",
    "\n",
    "    print(f\"Final F1 Score over all classes: {final_f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert Output RTTM to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rttm_file_path = \"/home/nele_pauline_suffo/outputs/vtc/childlens_audio/all.rttm\"\n",
    "rttm_ft_file_path = \"/home/nele_pauline_suffo/outputs/vtc_finetuned/childlens_audio/all.rttm\"\n",
    "rttm_cl_file_path = \"/home/nele_pauline_suffo/outputs/vtc_from_scratch/childlens_audio/all.rttm\"\n",
    "\n",
    "output_file_path = \"/home/nele_pauline_suffo/outputs/vtc/childlens_df.pkl\"\n",
    "output_ft_file_path = \"/home/nele_pauline_suffo/outputs/vtc_finetuned/childlens_df.pkl\"\n",
    "output_cl_file_path = \"/home/nele_pauline_suffo/outputs/vtc_from_scratch/childlens_df.pkl\"\n",
    "\n",
    "rttm_to_dataframe(rttm_file_path, output_file_path)\n",
    "rttm_to_dataframe(rttm_ft_file_path, output_ft_file_path)\n",
    "rttm_to_dataframe(rttm_cl_file_path, output_cl_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_metrics\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvtc/childlens_df_duration_off_01.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "compute_metrics('vtc/childlens_df_duration_off_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nele_pauline_suffo/.conda/envs/pyannote/lib/python3.8/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averaged Metrics Per Class Over All Videos:\n",
      "Class 'KCHI':\n",
      "  Precision: 0.778\n",
      "  Recall: 0.532\n",
      "  F1 Score: 0.595\n",
      "  Error Rate: 0.723\n",
      "\n",
      "Class 'OCH':\n",
      "  Precision: 1.000\n",
      "  Recall: 0.792\n",
      "  F1 Score: 0.792\n",
      "  Error Rate: 0.208\n",
      "\n",
      "Class 'FEM':\n",
      "  Precision: 0.324\n",
      "  Recall: 0.650\n",
      "  F1 Score: 0.335\n",
      "  Error Rate: 6.784\n",
      "\n",
      "Class 'MAL':\n",
      "  Precision: 0.244\n",
      "  Recall: 0.676\n",
      "  F1 Score: 0.179\n",
      "  Error Rate: 3.158\n",
      "\n",
      "Class 'SPEECH':\n",
      "  Precision: 0.779\n",
      "  Recall: 0.692\n",
      "  F1 Score: 0.688\n",
      "  Error Rate: 1.810\n",
      "\n",
      "Final F1 Score over all classes: 0.518\n"
     ]
    }
   ],
   "source": [
    "compute_metrics('quantex_share_vtc_output_03.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nele_pauline_suffo/.conda/envs/pyannote/lib/python3.8/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averaged Metrics Per Class Over All Videos:\n",
      "Class 'KCHI':\n",
      "  Precision: 0.751\n",
      "  Recall: 0.653\n",
      "  F1 Score: 0.657\n",
      "  Error Rate: 0.707\n",
      "\n",
      "Class 'OCH':\n",
      "  Precision: 1.000\n",
      "  Recall: 0.792\n",
      "  F1 Score: 0.792\n",
      "  Error Rate: 0.208\n",
      "\n",
      "Class 'FEM':\n",
      "  Precision: 0.305\n",
      "  Recall: 0.768\n",
      "  F1 Score: 0.353\n",
      "  Error Rate: 8.782\n",
      "\n",
      "Class 'MAL':\n",
      "  Precision: 0.240\n",
      "  Recall: 0.703\n",
      "  F1 Score: 0.189\n",
      "  Error Rate: 3.533\n",
      "\n",
      "Class 'SPEECH':\n",
      "  Precision: 0.751\n",
      "  Recall: 0.822\n",
      "  F1 Score: 0.742\n",
      "  Error Rate: 2.013\n",
      "\n",
      "Final F1 Score over all classes: 0.547\n"
     ]
    }
   ],
   "source": [
    "compute_metrics('quantex_share_vtc_output_20.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_annotations_vs_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_annotations_vs_predictions\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m300654\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_annotations_vs_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "plot_annotations_vs_predictions('300654', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotations_vs_predictions('284099', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of each Voice Type Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Voice_type\n",
       "FEM        280.525750\n",
       "KCHI       731.155017\n",
       "MAL        114.184067\n",
       "OCH         40.292133\n",
       "SPEECH    1465.549850\n",
       "Name: Utterance_Duration, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_df = pd.read_pickle(VTCPaths.childlens_gt_df_file_path)\n",
    "# sum utterance durations for each Voice_type in minutes (divide by 60)\n",
    "gt_df.groupby('Voice_type')['Utterance_Duration'].sum() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def convert_to_16khz(input_folder: Path, output_folder: Path) -> None:\n",
    "    \"\"\"Convert all audio files in a folder to 16kHz sampling rate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_folder : Path\n",
    "        Path to folder containing audio files\n",
    "    output_folder : Path\n",
    "        Path where converted files will be saved\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Supported audio formats\n",
    "    audio_extensions = ('.wav', '.mp3', '.m4a', '.flac')\n",
    "    \n",
    "    # Get all audio files\n",
    "    audio_files = [f for f in input_folder.iterdir() if f.suffix.lower() in audio_extensions]\n",
    "    \n",
    "    if not audio_files:\n",
    "        logging.warning(f\"No audio files found in {input_folder}\")\n",
    "        return\n",
    "        \n",
    "    for audio_file in audio_files:\n",
    "        try:\n",
    "            # Load audio file\n",
    "            audio = AudioSegment.from_file(audio_file)\n",
    "            \n",
    "            # Convert to 16kHz\n",
    "            audio_16k = audio.set_frame_rate(16000)\n",
    "            \n",
    "            # Create output path\n",
    "            output_path = output_folder / f\"{audio_file.stem}_16k{audio_file.suffix}\"\n",
    "            \n",
    "            # Export converted audio\n",
    "            audio_16k.export(output_path, format=output_path.suffix.replace('.', ''))\n",
    "            \n",
    "            logging.info(f\"Converted: {audio_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error converting {audio_file.name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_folder = Path(\"/home/nele_pauline_suffo/ProcessedData/quantex_audio\")\n",
    "    output_folder = Path(\"/home/nele_pauline_suffo/ProcessedData/quantex_audio_16khz\")\n",
    "    \n",
    "    convert_to_16khz(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
