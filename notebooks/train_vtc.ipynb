{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb0f569",
   "metadata": {},
   "source": [
    "## Generate GT RTTM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c42d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 RTTM file for train split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/train.rttm\n",
      "📝 RTTM file for dev split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/dev.rttm\n",
      "📝 RTTM file for test split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/test.rttm\n",
      "📝 Complete RTTM file for all splits saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/complete.rttm\n",
      "📝 RTTM file for train split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/train.rttm\n",
      "📝 RTTM file for dev split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/dev.rttm\n",
      "📝 RTTM file for test split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/test.rttm\n",
      "📝 Complete RTTM file for all splits saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/complete.rttm\n",
      "✅ Combined DataFrame for all splits saved to /home/nele_pauline_suffo/ProcessedData/childlens_annotations/processed/childlens_annotations_gt.pkl (48216 rows)\n",
      "✅ Combined UEM file for all videos saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens/complete.uem (161 segments)\n",
      "\n",
      "✅ Total processed files: 322\n",
      "\n",
      "🎙️ Speaker instance counts in all splits:\n",
      "  KCHI: 23190\n",
      "  CHI: 1402\n",
      "  FEM: 12386\n",
      "  MAL: 4894\n",
      "  SPEECH: 54560\n",
      "\n",
      "📊 RTTM split durations and video counts:\n",
      "  train: 146244.93 sec (80.1%), 98 videos\n",
      "  dev: 18337.89 sec (10.0%), 23 videos\n",
      "  test: 17952.87 sec (9.8%), 40 videos\n",
      "✅ .lst files created for train, development, and test splits.\n",
      "✅ .uem files created for train, development, and test splits.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Folder and output paths\n",
    "input_folder = \"/home/nele_pauline_suffo/ProcessedData/childlens_annotations\"\n",
    "output_dir = Path(\"/home/nele_pauline_suffo/ProcessedData/vtc_childlens\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "valid_action_names = {\"Child Talking\", \"Other Person Talking\", \"Overheard Speech\", \"Singing/Humming\"}\n",
    "\n",
    "all_files = []\n",
    "speaker_counts = Counter()\n",
    "files_processed = 0\n",
    "\n",
    "# Step 1: Load all JSON files and collect metadata\n",
    "json_files = glob(f\"{input_folder}/*.json\")\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "        uri = annotations['metadata']['name']\n",
    "        duration = annotations['metadata']['duration'] / 1_000_000  # microseconds to seconds\n",
    "        all_files.append({\n",
    "            \"path\": json_file,\n",
    "            \"uri\": uri,\n",
    "            \"duration\": duration\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {json_file} due to error: {e}\")\n",
    "\n",
    "# Step 2: Sort and split files by total duration\n",
    "all_files.sort(key=lambda x: x[\"duration\"], reverse=True)\n",
    "total_duration = sum(f[\"duration\"] for f in all_files)\n",
    "\n",
    "train_duration, dev_duration, test_duration = 0, 0, 0\n",
    "train_files, dev_files, test_files = [], [], []\n",
    "\n",
    "for f in all_files:\n",
    "    if train_duration < 0.8 * total_duration:\n",
    "        train_files.append(f)\n",
    "        train_duration += f[\"duration\"]\n",
    "    elif dev_duration < 0.1 * total_duration:\n",
    "        dev_files.append(f)\n",
    "        dev_duration += f[\"duration\"]\n",
    "    else:\n",
    "        test_files.append(f)\n",
    "        test_duration += f[\"duration\"]\n",
    "\n",
    "splits = {\n",
    "    \"train\": train_files,\n",
    "    \"dev\": dev_files,\n",
    "    \"test\": test_files\n",
    "}\n",
    "split_durations = {\n",
    "    \"train\": train_duration,\n",
    "    \"dev\": dev_duration,\n",
    "    \"test\": test_duration\n",
    "}\n",
    "\n",
    "# Step 3: Process each split and write RTTM\n",
    "all_df_rows = [] # Initialize list for DataFrame rows\n",
    "all_rttm_lines_combined = [] # Initialize list for all RTTM lines for the complete.rttm\n",
    "\n",
    "for split_name, files_in_split in splits.items():\n",
    "    rttm_lines_split = [] # RTTM lines for the current split\n",
    "\n",
    "    for f_info in files_in_split:\n",
    "        try:\n",
    "            with open(f_info[\"path\"], \"r\") as file_handle:\n",
    "                annotations = json.load(file_handle)\n",
    "            uri = annotations['metadata']['name']\n",
    "            files_processed += 1\n",
    "            \n",
    "            for instance in annotations.get('instances', []):\n",
    "                if instance[\"meta\"][\"type\"] != \"event\":\n",
    "                    continue\n",
    "                try:\n",
    "                    # Assuming the first parameter block contains the relevant timestamps and attributes\n",
    "                    if not instance[\"parameters\"] or not instance[\"parameters\"][0].get(\"timestamps\"):\n",
    "                        continue\n",
    "                    \n",
    "    \n",
    "                    parameter_block = instance[\"parameters\"][0]\n",
    "                    # Ensure 'timestamps' key exists and is a list\n",
    "                    if not isinstance(parameter_block.get(\"timestamps\"), list):\n",
    "                        continue\n",
    "                    \n",
    "                    # The 'start' and 'end' for the whole instance parameter block\n",
    "                    instance_start_time_us = parameter_block.get(\"start\")\n",
    "                    instance_end_time_us = parameter_block.get(\"end\")\n",
    "                    \n",
    "                    if instance_start_time_us is None or instance_end_time_us is None:\n",
    "                        pass\n",
    "                    \n",
    "                    for detail_idx, detail in enumerate(parameter_block[\"timestamps\"]):\n",
    "                        if \"attributes\" not in detail:\n",
    "                            continue\n",
    "                        \n",
    "                        action_type = next(\n",
    "                            (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                             if attr[\"groupName\"] == \"Type of Action\"),\n",
    "                            None\n",
    "                        )  \n",
    "\n",
    "                        if action_type in valid_action_names:\n",
    "                            speaker_id = \"NA\"\n",
    "\n",
    "                            if action_type in [\"Child Talking\", \"Singing/Humming\"]:\n",
    "                                speaker_id = \"KCHI\"\n",
    "                            elif action_type == \"Other Person Talking\":\n",
    "                                age_group = next(\n",
    "                                    (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                                     if attr[\"groupName\"] == \"1st Person Age Group\"),\n",
    "                                    None\n",
    "                                )\n",
    "                                gender = next(\n",
    "                                    (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                                     if attr[\"groupName\"] == \"1st Person Gender\"),\n",
    "                                    None\n",
    "                                )\n",
    "\n",
    "                                if age_group in [\"Child\", \"Infant\"]:\n",
    "                                    speaker_id = \"CHI\"\n",
    "                                elif age_group in [\"Adult\", \"Adolescent\"]:\n",
    "                                    if gender == \"Female\":\n",
    "                                        speaker_id = \"FEM\"\n",
    "                                    elif gender == \"Male\":\n",
    "                                        speaker_id = \"MAL\"\n",
    "                            elif action_type == \"Overheard Speech\":\n",
    "                                speaker_id = \"SPEECH\"\n",
    "\n",
    "                       \n",
    "                            # Correctly use segment start/end from the parameter_block\n",
    "                            # (which was referred to as 'timestamps' variable in original code)\n",
    "                            segment_start_us = parameter_block.get(\"start\")\n",
    "                            segment_end_us = parameter_block.get(\"end\")\n",
    "                            \n",
    "                            \n",
    "                            if segment_start_us is None or segment_end_us is None:\n",
    "                                print(f\"Warning: Missing start/end in parameter block for instance in {f_info['path']}. Skipping.\")\n",
    "                                continue\n",
    "                            \n",
    "                            \n",
    "                            start_sec = segment_start_us / 1_000_000\n",
    "                            end_sec = segment_end_us / 1_000_000\n",
    "                            duration_sec = end_sec - start_sec\n",
    "\n",
    "                            if duration_sec <= 0: # Ensure duration is positive\n",
    "                                print(f\"Warning: Non-positive duration {duration_sec:.3f}s for segment in {f_info['path']}. Skipping.\")\n",
    "                                continue\n",
    "                            \n",
    "                            rttm_line = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> {speaker_id} <NA> <NA>\"\n",
    "                            rttm_line_speech = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> SPEECH <NA> <NA>\"\n",
    "\n",
    "                            if speaker_id != \"NA\":\n",
    "                                rttm_lines_split.append(rttm_line)\n",
    "                                all_rttm_lines_combined.append(rttm_line) # Add to combined list\n",
    "                                rttm_lines_split.append(rttm_line_speech) # RTTM gets both lines\n",
    "                                all_rttm_lines_combined.append(rttm_line_speech) # Add to combined list\n",
    "\n",
    "                                speaker_counts[speaker_id] += 1    # Counts original speaker_id (e.g. OCH)\n",
    "                                speaker_counts[\"SPEECH\"] += 1\n",
    "\n",
    "                                # Prepare data for DataFrame (only specific voice type, OCH mapped to CHI)\n",
    "                                row_data_specific = {\n",
    "                                    \"audio_file_name\": uri,\n",
    "                                    \"Utterance_Start\": round(start_sec, 3),\n",
    "                                    \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                                    \"Voice_type\": speaker_id, # This will be KCHI, CHI, FEM, MAL\n",
    "                                    \"Utterance_End\": round(end_sec, 3)\n",
    "                                }\n",
    "                                all_df_rows.append(row_data_specific)\n",
    "                                \n",
    "                                # Add corresponding SPEECH entry for the DataFrame\n",
    "                                row_data_speech = {\n",
    "                                    \"audio_file_name\": uri,\n",
    "                                    \"Utterance_Start\": round(start_sec, 3),\n",
    "                                    \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                                    \"Voice_type\": \"SPEECH\", # Add the SPEECH category\n",
    "                                    \"Utterance_End\": round(end_sec, 3)\n",
    "                                }\n",
    "                                all_df_rows.append(row_data_speech)\n",
    "                                \n",
    "                                # only use the first instance of each segment\n",
    "                                break\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping instance in {f_info['path']} due to error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f_info['path']}: {e}\")\n",
    "\n",
    "    # Save to RTTM file for the current split\n",
    "    rttm_path_split = output_dir / f\"{split_name}.rttm\"\n",
    "    with open(rttm_path_split, \"w\") as out_f:\n",
    "        for line in rttm_lines_split:\n",
    "            out_f.write(line + \"\\n\")\n",
    "    print(f\"📝 RTTM file for {split_name} split saved to {rttm_path_split}\")\n",
    "\n",
    "# Save the complete RTTM file after processing all splits\n",
    "complete_rttm_path = output_dir / \"complete.rttm\"\n",
    "with open(complete_rttm_path, \"w\") as out_f:\n",
    "    for line in all_rttm_lines_combined:\n",
    "        out_f.write(line + \"\\n\")\n",
    "print(f\"📝 Complete RTTM file for all splits saved to {complete_rttm_path}\")\n",
    "\n",
    "# Step 3: Process each split and write RTTM\n",
    "all_df_rows = [] # Initialize list for DataFrame rows\n",
    "all_rttm_lines_combined = [] # Initialize list for all RTTM lines for the complete.rttm\n",
    "\n",
    "for split_name, files_in_split in splits.items():\n",
    "    rttm_lines_split = [] # RTTM lines for the current split\n",
    "\n",
    "    for f_info in files_in_split:\n",
    "        try:\n",
    "            with open(f_info[\"path\"], \"r\") as file_handle:\n",
    "                annotations = json.load(file_handle)\n",
    "            uri = annotations['metadata']['name']\n",
    "            files_processed += 1\n",
    "            \n",
    "            for instance in annotations.get('instances', []):\n",
    "                if instance[\"meta\"][\"type\"] != \"event\":\n",
    "                    continue\n",
    "                try:\n",
    "                    # Assuming the first parameter block contains the relevant timestamps and attributes\n",
    "                    if not instance[\"parameters\"] or not instance[\"parameters\"][0].get(\"timestamps\"):\n",
    "                        continue\n",
    "                    \n",
    "    \n",
    "                    parameter_block = instance[\"parameters\"][0]\n",
    "                    # Ensure 'timestamps' key exists and is a list\n",
    "                    if not isinstance(parameter_block.get(\"timestamps\"), list):\n",
    "                        continue\n",
    "                    \n",
    "                    # The 'start' and 'end' for the whole instance parameter block\n",
    "                    instance_start_time_us = parameter_block.get(\"start\")\n",
    "                    instance_end_time_us = parameter_block.get(\"end\")\n",
    "                    \n",
    "                    if instance_start_time_us is None or instance_end_time_us is None:\n",
    "                        pass\n",
    "                    \n",
    "                    for detail_idx, detail in enumerate(parameter_block[\"timestamps\"]):\n",
    "                        if \"attributes\" not in detail:\n",
    "                            continue\n",
    "                        \n",
    "                        action_type = next(\n",
    "                            (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                             if attr[\"groupName\"] == \"Type of Action\"),\n",
    "                            None\n",
    "                        )  \n",
    "\n",
    "                        if action_type in valid_action_names:\n",
    "                            speaker_id = \"NA\"\n",
    "\n",
    "                            if action_type in [\"Child Talking\", \"Singing/Humming\"]:\n",
    "                                speaker_id = \"KCHI\"\n",
    "                            elif action_type == \"Other Person Talking\":\n",
    "                                age_group = next(\n",
    "                                    (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                                     if attr[\"groupName\"] == \"1st Person Age Group\"),\n",
    "                                    None\n",
    "                                )\n",
    "                                gender = next(\n",
    "                                    (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                                     if attr[\"groupName\"] == \"1st Person Gender\"),\n",
    "                                    None\n",
    "                                )\n",
    "\n",
    "                                if age_group in [\"Child\", \"Infant\"]:\n",
    "                                    speaker_id = \"CHI\"\n",
    "                                elif age_group in [\"Adult\", \"Adolescent\"]:\n",
    "                                    if gender == \"Female\":\n",
    "                                        speaker_id = \"FEM\"\n",
    "                                    elif gender == \"Male\":\n",
    "                                        speaker_id = \"MAL\"\n",
    "                            elif action_type == \"Overheard Speech\":\n",
    "                                speaker_id = \"SPEECH\"\n",
    "\n",
    "                       \n",
    "                            # Correctly use segment start/end from the parameter_block\n",
    "                            # (which was referred to as 'timestamps' variable in original code)\n",
    "                            segment_start_us = parameter_block.get(\"start\")\n",
    "                            segment_end_us = parameter_block.get(\"end\")\n",
    "                            \n",
    "                            \n",
    "                            if segment_start_us is None or segment_end_us is None:\n",
    "                                print(f\"Warning: Missing start/end in parameter block for instance in {f_info['path']}. Skipping.\")\n",
    "                                continue\n",
    "                            \n",
    "                            \n",
    "                            start_sec = segment_start_us / 1_000_000\n",
    "                            end_sec = segment_end_us / 1_000_000\n",
    "                            duration_sec = end_sec - start_sec\n",
    "\n",
    "                            if duration_sec <= 0: # Ensure duration is positive\n",
    "                                print(f\"Warning: Non-positive duration {duration_sec:.3f}s for segment in {f_info['path']}. Skipping.\")\n",
    "                                continue\n",
    "                            \n",
    "                            rttm_line = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> {speaker_id} <NA> <NA>\"\n",
    "                            rttm_line_speech = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> SPEECH <NA> <NA>\"\n",
    "\n",
    "                            if speaker_id != \"NA\":\n",
    "                                rttm_lines_split.append(rttm_line)\n",
    "                                all_rttm_lines_combined.append(rttm_line) # Add to combined list\n",
    "                                rttm_lines_split.append(rttm_line_speech) # RTTM gets both lines\n",
    "                                all_rttm_lines_combined.append(rttm_line_speech) # Add to combined list\n",
    "\n",
    "                                speaker_counts[speaker_id] += 1    # Counts original speaker_id (e.g. OCH)\n",
    "                                speaker_counts[\"SPEECH\"] += 1\n",
    "\n",
    "                                # Prepare data for DataFrame (only specific voice type, OCH mapped to CHI)\n",
    "                                row_data_specific = {\n",
    "                                    \"audio_file_name\": uri,\n",
    "                                    \"Utterance_Start\": round(start_sec, 3),\n",
    "                                    \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                                    \"Voice_type\": speaker_id, # This will be KCHI, CHI, FEM, MAL\n",
    "                                    \"Utterance_End\": round(end_sec, 3)\n",
    "                                }\n",
    "                                all_df_rows.append(row_data_specific)\n",
    "                                \n",
    "                                # Add corresponding SPEECH entry for the DataFrame\n",
    "                                row_data_speech = {\n",
    "                                    \"audio_file_name\": uri,\n",
    "                                    \"Utterance_Start\": round(start_sec, 3),\n",
    "                                    \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                                    \"Voice_type\": \"SPEECH\", # Add the SPEECH category\n",
    "                                    \"Utterance_End\": round(end_sec, 3)\n",
    "                                }\n",
    "                                all_df_rows.append(row_data_speech)\n",
    "                                \n",
    "                                # only use the first instance of each segment\n",
    "                                break\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping instance in {f_info['path']} due to error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f_info['path']}: {e}\")\n",
    "\n",
    "    # Save to RTTM file for the current split\n",
    "    rttm_path_split = output_dir / f\"{split_name}.rttm\"\n",
    "    with open(rttm_path_split, \"w\") as out_f:\n",
    "        for line in rttm_lines_split:\n",
    "            out_f.write(line + \"\\n\")\n",
    "    print(f\"📝 RTTM file for {split_name} split saved to {rttm_path_split}\")\n",
    "\n",
    "# Save the complete RTTM file after processing all splits\n",
    "complete_rttm_path = output_dir / \"complete.rttm\"\n",
    "with open(complete_rttm_path, \"w\") as out_f:\n",
    "    for line in all_rttm_lines_combined:\n",
    "        out_f.write(line + \"\\n\")\n",
    "print(f\"📝 Complete RTTM file for all splits saved to {complete_rttm_path}\")\n",
    "\n",
    "# Create and save ONE COMBINED DataFrame after processing all splits\n",
    "if all_df_rows:\n",
    "    combined_df = pd.DataFrame(all_df_rows)\n",
    "    # Ensure desired column order\n",
    "    combined_df = combined_df[[\"audio_file_name\", \"Utterance_Start\", \"Utterance_Duration\", \"Voice_type\", \"Utterance_End\"]]\n",
    "    # Define the single output path for the combined pickle file\n",
    "    df_pkl_path = Path(\"/home/nele_pauline_suffo/ProcessedData/childlens_annotations/processed/childlens_annotations_gt.pkl\")\n",
    "    # Ensure the directory exists\n",
    "    df_pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    combined_df.to_pickle(df_pkl_path)\n",
    "    print(f\"✅ Combined DataFrame for all splits saved to {df_pkl_path} ({len(combined_df)} rows)\")\n",
    "else:\n",
    "    print(f\"ℹ️ No data to create combined DataFrame.\")\n",
    "\n",
    "# Step 3.5: Save a complete UEM file with all video information\n",
    "uem_lines = []\n",
    "for f in all_files:\n",
    "    uri = f[\"uri\"]\n",
    "    start_time = 0.000\n",
    "    end_time = f[\"duration\"]\n",
    "    uem_lines.append(f\"{uri} 1 {start_time:.3f} {end_time:.3f}\")\n",
    "\n",
    "uem_path = output_dir / \"complete.uem\"\n",
    "with open(uem_path, \"w\") as uem_file:\n",
    "    for line in uem_lines:\n",
    "        uem_file.write(line + \"\\n\")\n",
    "print(f\"✅ Combined UEM file for all videos saved to {uem_path} ({len(uem_lines)} segments)\")\n",
    "\n",
    "# Step 4: Summary logs\n",
    "print(f\"\\n✅ Total processed files: {files_processed}\")\n",
    "print(\"\\n🎙️ Speaker instance counts in all splits:\")\n",
    "for speaker_id in ['KCHI', 'CHI', 'FEM', 'MAL', 'SPEECH']:\n",
    "    print(f\"  {speaker_id}: {speaker_counts[speaker_id]}\")\n",
    "\n",
    "print(\"\\n📊 RTTM split durations and video counts:\")\n",
    "for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "    dur = split_durations[split_name]\n",
    "    perc = (dur / total_duration) * 100\n",
    "    count = len(splits[split_name])\n",
    "    print(f\"  {split_name}: {dur:.2f} sec ({perc:.1f}%), {count} videos\")\n",
    "\n",
    "# Generate .lst files for train, development, and test splits\n",
    "for split_name, files in splits.items():\n",
    "    lst_path = output_dir / f\"{split_name}.lst\"\n",
    "    with open(lst_path, \"w\") as lst_file:\n",
    "        for f in files:\n",
    "            lst_file.write(f\"{f['uri']}\\n\")\n",
    "\n",
    "print(\"✅ .lst files created for train, development, and test splits.\")\n",
    "\n",
    "# Generate .uem files for train, development, and test splits\n",
    "for split_name, files in splits.items():\n",
    "    uem_path = output_dir / f\"{split_name}.uem\"\n",
    "    with open(uem_path, \"w\") as uem_file:\n",
    "        for f in files:\n",
    "            try:\n",
    "                # Extract the URI and duration for each video\n",
    "                uri = f[\"uri\"]\n",
    "                start = 0  # Start time is always 0\n",
    "                end = f[\"duration\"]  # End time is the video's duration\n",
    "\n",
    "                # Write a single line for each video\n",
    "                uem_line = f\"{uri} 1 {start:.3f} {end:.3f}\"\n",
    "                uem_file.write(uem_line + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {f['path']}: {e}\")\n",
    "\n",
    "print(\"✅ .uem files created for train, development, and test splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e18f98",
   "metadata": {},
   "source": [
    "## Generate GT RTTM File for ChildLens_v2 VTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e75af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 RTTM file for train split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens_v2/train.rttm\n",
      "📝 RTTM file for dev split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens_v2/dev.rttm\n",
      "📝 RTTM file for test split saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens_v2/test.rttm\n",
      "📝 Complete RTTM file for v2 (all splits) saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens_v2/complete.rttm\n",
      "✅ Combined DataFrame for all splits saved to /home/nele_pauline_suffo/ProcessedData/childlens_annotations/processed/childlens_annotations_gt_v2.pkl (24150 rows)\n",
      "✅ Combined UEM file for all videos saved to /home/nele_pauline_suffo/ProcessedData/vtc_childlens_v2/complete.uem (161 segments)\n",
      "\n",
      "✅ Total processed files: 161\n",
      "\n",
      "🎙️ Speaker instance counts in all splits:\n",
      "  KCHI: 11595\n",
      "  SPEECH: 11595\n",
      "\n",
      "📊 RTTM split durations and video counts:\n",
      "  train: 146244.93 sec (80.1%), 98 videos\n",
      "  dev: 18337.89 sec (10.0%), 23 videos\n",
      "  test: 17952.87 sec (9.8%), 40 videos\n",
      "✅ .lst files created for train, development, and test splits.\n",
      "✅ .uem files created for train, development, and test splits.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Folder and output paths\n",
    "input_folder = \"/home/nele_pauline_suffo/ProcessedData/childlens_annotations\"\n",
    "output_dir = Path(\"/home/nele_pauline_suffo/ProcessedData/vtc_childlens_v2\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "valid_action_names = {\"Child Talking\", \"Other Person Talking\", \"Overheard Speech\", \"Singing/Humming\"}\n",
    "\n",
    "all_files = []\n",
    "speaker_counts = Counter()\n",
    "files_processed = 0\n",
    "\n",
    "# Step 1: Load all JSON files and collect metadata\n",
    "json_files = glob(f\"{input_folder}/*.json\")\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "        uri = annotations['metadata']['name']\n",
    "        duration = annotations['metadata']['duration'] / 1_000_000  # microseconds to seconds\n",
    "        all_files.append({\n",
    "            \"path\": json_file,\n",
    "            \"uri\": uri,\n",
    "            \"duration\": duration\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {json_file} due to error: {e}\")\n",
    "\n",
    "# Step 2: Sort and split files by total duration\n",
    "all_files.sort(key=lambda x: x[\"duration\"], reverse=True)\n",
    "total_duration = sum(f[\"duration\"] for f in all_files)\n",
    "\n",
    "train_duration, dev_duration, test_duration = 0, 0, 0\n",
    "train_files, dev_files, test_files = [], [], []\n",
    "\n",
    "for f in all_files:\n",
    "    if train_duration < 0.8 * total_duration:\n",
    "        train_files.append(f)\n",
    "        train_duration += f[\"duration\"]\n",
    "    elif dev_duration < 0.1 * total_duration:\n",
    "        dev_files.append(f)\n",
    "        dev_duration += f[\"duration\"]\n",
    "    else:\n",
    "        test_files.append(f)\n",
    "        test_duration += f[\"duration\"]\n",
    "\n",
    "splits = {\n",
    "    \"train\": train_files,\n",
    "    \"dev\": dev_files,\n",
    "    \"test\": test_files\n",
    "}\n",
    "split_durations = {\n",
    "    \"train\": train_duration,\n",
    "    \"dev\": dev_duration,\n",
    "    \"test\": test_duration\n",
    "}\n",
    "\n",
    "# Step 3: Process each split and write RTTM\n",
    "all_df_rows = [] # Initialize list for DataFrame rows\n",
    "all_rttm_lines_combined_v2 = [] # Initialize list for all RTTM lines for the complete_v2.rttm\n",
    "\n",
    "for split_name, files_in_split in splits.items():\n",
    "    rttm_lines = [] # RTTM lines for the current split (v2 logic)\n",
    "\n",
    "    for f_info in files_in_split:\n",
    "        try:\n",
    "            with open(f_info[\"path\"], \"r\") as file_handle:\n",
    "                annotations = json.load(file_handle)\n",
    "            uri = annotations['metadata']['name']\n",
    "            files_processed += 1\n",
    "            \n",
    "            for instance in annotations.get('instances', []):\n",
    "                if instance[\"meta\"][\"type\"] != \"event\":\n",
    "                    continue\n",
    "                try:\n",
    "                    # Assuming the first parameter block contains the relevant timestamps and attributes\n",
    "                    if not instance[\"parameters\"] or not instance[\"parameters\"][0].get(\"timestamps\"):\n",
    "                        continue\n",
    "                    \n",
    "    \n",
    "                    parameter_block = instance[\"parameters\"][0]\n",
    "                    # Ensure 'timestamps' key exists and is a list\n",
    "                    if not isinstance(parameter_block.get(\"timestamps\"), list):\n",
    "                        continue\n",
    "                    \n",
    "                    # The 'start' and 'end' for the whole instance parameter block\n",
    "                    instance_start_time_us = parameter_block.get(\"start\")\n",
    "                    instance_end_time_us = parameter_block.get(\"end\")\n",
    "                    \n",
    "                    if instance_start_time_us is None or instance_end_time_us is None:\n",
    "                        pass\n",
    "                    \n",
    "                    for detail_idx, detail in enumerate(parameter_block[\"timestamps\"]):\n",
    "                        if \"attributes\" not in detail:\n",
    "                            continue\n",
    "                        \n",
    "                        action_type = next(\n",
    "                            (attr[\"name\"] for attr in detail[\"attributes\"]\n",
    "                             if attr[\"groupName\"] == \"Type of Action\"),\n",
    "                            None\n",
    "                        )  \n",
    "\n",
    "                        if action_type in valid_action_names:\n",
    "                            speaker_id = \"NA\"\n",
    "\n",
    "                            if action_type in [\"Child Talking\", \"Singing/Humming\"]:\n",
    "                                speaker_id = \"KCHI\"\n",
    "                            elif action_type == \"Other Person Talking\":\n",
    "                                speaker_id = \"CDS\" # Child Directed Speech\n",
    "                            elif action_type == \"Overheard Speech\":\n",
    "                                speaker_id = \"OHS\"\n",
    "\n",
    "                            # Correctly use segment start/end from the parameter_block\n",
    "                            # (which was referred to as 'timestamps' variable in original code)\n",
    "                            segment_start_us = parameter_block.get(\"start\")\n",
    "                            segment_end_us = parameter_block.get(\"end\")\n",
    "                            \n",
    "                            \n",
    "                            if segment_start_us is None or segment_end_us is None:\n",
    "                                print(f\"Warning: Missing start/end in parameter block for instance in {f_info['path']}. Skipping.\")\n",
    "                                continue\n",
    "                            \n",
    "                            \n",
    "                            start_sec = segment_start_us / 1_000_000\n",
    "                            end_sec = segment_end_us / 1_000_000\n",
    "                            duration_sec = end_sec - start_sec\n",
    "\n",
    "                            if duration_sec <= 0: # Ensure duration is positive\n",
    "                                print(f\"Warning: Non-positive duration {duration_sec:.3f}s for segment in {f_info['path']}. Skipping.\")\n",
    "                                continue\n",
    "                            \n",
    "                            rttm_line = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> {speaker_id} <NA> <NA>\"\n",
    "                            rttm_line_speech = f\"SPEAKER {uri} 1 {start_sec:.3f} {duration_sec:.3f} <NA> <NA> SPEECH <NA> <NA>\"\n",
    "\n",
    "                            if speaker_id != \"NA\":\n",
    "                                rttm_lines.append(rttm_line)\n",
    "                                all_rttm_lines_combined_v2.append(rttm_line) # Add to combined list for v2\n",
    "                                if speaker_id == \"KCHI\":\n",
    "                                    rttm_lines.append(rttm_line_speech)\n",
    "                                    all_rttm_lines_combined_v2.append(rttm_line_speech) # Add to combined list for v2\n",
    "                                    speaker_counts[\"SPEECH\"] += 1\n",
    "                                speaker_counts[speaker_id] += 1\n",
    "                                \n",
    "                                # Prepare data for DataFrame (only specific voice type\n",
    "                                row_data_specific = {\n",
    "                                    \"audio_file_name\": uri,\n",
    "                                    \"Utterance_Start\": round(start_sec, 3),\n",
    "                                    \"Utterance_Duration\": round(duration_sec, 3),\n",
    "                                    \"Voice_type\": speaker_id, \n",
    "                                    \"Utterance_End\": round(end_sec, 3)\n",
    "                                }\n",
    "                                all_df_rows.append(row_data_specific)\n",
    "                                \n",
    "                                # only use the first instance of each segment\n",
    "                                break\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping instance in {f_info['path']} due to error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f_info['path']}: {e}\")\n",
    "\n",
    "    # Save to RTTM file for the current split\n",
    "    rttm_path = output_dir / f\"{split_name}.rttm\" # This is vtc_childlens_v2\n",
    "    with open(rttm_path, \"w\") as out_f:\n",
    "        for line in rttm_lines:\n",
    "            out_f.write(line + \"\\n\")\n",
    "    print(f\"📝 RTTM file for {split_name} split saved to {rttm_path}\")\n",
    "\n",
    "# Save the complete RTTM file for v2 after processing all splits\n",
    "complete_rttm_path_v2 = output_dir / \"complete.rttm\" # This is vtc_childlens_v2\n",
    "with open(complete_rttm_path_v2, \"w\") as out_f:\n",
    "    for line in all_rttm_lines_combined_v2:\n",
    "        out_f.write(line + \"\\n\")\n",
    "print(f\"📝 Complete RTTM file for v2 (all splits) saved to {complete_rttm_path_v2}\")\n",
    "\n",
    "# Create and save ONE COMBINED DataFrame after processing all splits\n",
    "if all_df_rows:\n",
    "    combined_df = pd.DataFrame(all_df_rows)\n",
    "    # Ensure desired column order\n",
    "    combined_df = combined_df[[\"audio_file_name\", \"Utterance_Start\", \"Utterance_Duration\", \"Voice_type\", \"Utterance_End\"]]\n",
    "    # Define the single output path for the combined pickle file\n",
    "    df_pkl_path = Path(\"/home/nele_pauline_suffo/ProcessedData/childlens_annotations/processed/childlens_annotations_gt_v2.pkl\")\n",
    "    # Ensure the directory exists\n",
    "    df_pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    combined_df.to_pickle(df_pkl_path)\n",
    "    print(f\"✅ Combined DataFrame for all splits saved to {df_pkl_path} ({len(combined_df)} rows)\")\n",
    "else:\n",
    "    print(f\"ℹ️ No data to create combined DataFrame.\")\n",
    "\n",
    "# Step 3.5: Save a complete UEM file with all video information\n",
    "uem_lines = []\n",
    "for f in all_files:\n",
    "    uri = f[\"uri\"]\n",
    "    start_time = 0.000\n",
    "    end_time = f[\"duration\"]\n",
    "    uem_lines.append(f\"{uri} 1 {start_time:.3f} {end_time:.3f}\")\n",
    "\n",
    "uem_path = output_dir / \"complete.uem\"\n",
    "with open(uem_path, \"w\") as uem_file:\n",
    "    for line in uem_lines:\n",
    "        uem_file.write(line + \"\\n\")\n",
    "print(f\"✅ Combined UEM file for all videos saved to {uem_path} ({len(uem_lines)} segments)\")\n",
    "\n",
    "# Step 4: Summary logs\n",
    "print(f\"\\n✅ Total processed files: {files_processed}\")\n",
    "print(\"\\n🎙️ Speaker instance counts in all splits:\")\n",
    "for speaker_id in ['KCHI', 'SPEECH']:\n",
    "    print(f\"  {speaker_id}: {speaker_counts[speaker_id]}\")\n",
    "\n",
    "print(\"\\n📊 RTTM split durations and video counts:\")\n",
    "for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "    dur = split_durations[split_name]\n",
    "    perc = (dur / total_duration) * 100\n",
    "    count = len(splits[split_name])\n",
    "    print(f\"  {split_name}: {dur:.2f} sec ({perc:.1f}%), {count} videos\")\n",
    "\n",
    "# Generate .lst files for train, development, and test splits\n",
    "for split_name, files in splits.items():\n",
    "    lst_path = output_dir / f\"{split_name}.lst\"\n",
    "    with open(lst_path, \"w\") as lst_file:\n",
    "        for f in files:\n",
    "            lst_file.write(f\"{f['uri']}\\n\")\n",
    "\n",
    "print(\"✅ .lst files created for train, development, and test splits.\")\n",
    "\n",
    "# Generate .uem files for train, development, and test splits\n",
    "for split_name, files in splits.items():\n",
    "    uem_path = output_dir / f\"{split_name}.uem\"\n",
    "    with open(uem_path, \"w\") as uem_file:\n",
    "        for f in files:\n",
    "            try:\n",
    "                # Extract the URI and duration for each video\n",
    "                uri = f[\"uri\"]\n",
    "                start = 0  # Start time is always 0\n",
    "                end = f[\"duration\"]  # End time is the video's duration\n",
    "\n",
    "                # Write a single line for each video\n",
    "                uem_line = f\"{uri} 1 {start:.3f} {end:.3f}\"\n",
    "                uem_file.write(uem_line + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {f['path']}: {e}\")\n",
    "\n",
    "print(\"✅ .uem files created for train, development, and test splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b1a56",
   "metadata": {},
   "source": [
    "## Create rttm files per video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3764ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Individual RTTM files created in /home/nele_pauline_suffo/ProcessedData/vtc_childlens/rttm_per_video.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Create a new folder to save individual RTTM files\n",
    "individual_rttm_dir = Path(\"/home/nele_pauline_suffo/ProcessedData/vtc_childlens/rttm_per_video\")\n",
    "individual_rttm_dir.mkdir(exist_ok=True)\n",
    "output_dir = Path(\"/home/nele_pauline_suffo/ProcessedData/vtc_childlens\")\n",
    "\n",
    "# Process each split's RTTM file\n",
    "for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "    rttm_path = output_dir / f\"{split_name}.rttm\"\n",
    "    try:\n",
    "        with open(rttm_path, \"r\") as rttm_file:\n",
    "            lines = rttm_file.readlines()\n",
    "\n",
    "        # Group lines by video ID (URI)\n",
    "        video_rttm_data = {}\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            uri = parts[1]  # Video ID\n",
    "            if uri not in video_rttm_data:\n",
    "                video_rttm_data[uri] = []\n",
    "            video_rttm_data[uri].append(line)\n",
    "\n",
    "        # Write each video's RTTM data to a separate file\n",
    "        for uri, rttm_lines in video_rttm_data.items():\n",
    "            video_rttm_path = individual_rttm_dir / f\"{uri}.rttm\"\n",
    "            with open(video_rttm_path, \"w\") as video_rttm_file:\n",
    "                video_rttm_file.writelines(rttm_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing RTTM file {rttm_path}: {e}\")\n",
    "\n",
    "print(f\"✅ Individual RTTM files created in {individual_rttm_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c82e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyannote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
